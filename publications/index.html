<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Full list of publications &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">


      

      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item active" href="/publications/">Publications</a>
      <a class="sidebar-nav-item" href="/researchprojects/">Research Projects</a>
      <a class="sidebar-nav-item" href="https://scholar.google.com/citations?user=47YTUrEAAAAJ&hl=en&oi=ao">Google Scholar</a>
      <a class="sidebar-nav-item" href="https://www.youtube.com/channel/UCSv1_NZITsPl-abaCWtRrJg">YouTube</a>

      <span class="sidebar-nav-item">Jekyll Hyde, Currently v2.1.0</span>
    </nav>

    <p>&copy; 2021. All rights reserved.</p> 
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Full list of publications</h1>
  <h2 id="pre-prints">Pre-prints</h2>

<ol class="bibliography"><li><div class="text-justify"><span id="yi2016solving">Yi, Xinyang, Constantine Caramanis, and Sujay Sanghavi. “Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization.” <i>ArXiv Preprint ArXiv:1608.05749</i>, 2016.</span></div>
<button class="button0" onclick="toggleBibtexyi2016solving()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2016solving()">Abstract</button>



    <a href="https://arxiv.org/pdf/1608.05749.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2016solving" style="display: none;">
<pre>@article{yi2016solving,
  title = {Solving a mixture of many random linear equations by tensor decomposition and alternating minimization},
  author = {Yi, Xinyang and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {arXiv preprint arXiv:1608.05749},
  year = {2016},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/1608.05749.pdf}
}
</pre>
</div>

<div id="ayi2016solving" style="display: none;">
<pre>We consider the problem of solving mixed random linear equations with k components. This is the noiseless setting of mixed linear regression. The goal is to estimate multiple linear models from mixed samples in the case where the labels (which sample corresponds to which model) are not observed. We give a tractable algorithm for the mixed linear equation problem, and show that under some technical conditions, our algorithm is guaranteed to solve the problem exactly with sample complexity linear in the dimension, and polynomial in k, the number of components. Previous approaches have required either exponential dependence on k, or super-linear dependence on the dimension. The proposed algorithm is a combination of tensor decomposition and alternating minimization. Our analysis involves proving that the initialization provided by the tensor method allows alternating minimization, which is equivalent to EM in our setting, to converge to the global optimum at a linear rate.</pre>
</div>


<script>
function toggleAbstractCCyi2016solving(parameter) {
    var x= document.getElementById('ayi2016solving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2016solving(parameter) {
    var x= document.getElementById('Byi2016solving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2016solving(parameter) {
    var x= document.getElementById('ayi2016solving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>

<h2 id="conference-papers">Conference Papers</h2>

<ol class="bibliography"><li><div class="text-justify"><span id="kwon2020minimax">Kwon, Jeongyeol, Nhat Ho, and Constantine Caramanis. “On the Minimax Optimality of the Em Algorithm for Learning Two-Component Mixed Linear Regression.” In <i>International Conference on Artificial Intelligence and Statistics</i>. PMLR, 2021.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020minimax()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020minimax()">Abstract</button>



    <a href="arXiv preprint arXiv:2006.02601"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020minimax" style="display: none;">
<pre>@inproceedings{kwon2020minimax,
  title = {On the minimax optimality of the em algorithm for learning two-component mixed linear regression},
  author = {Kwon, Jeongyeol and Ho, Nhat and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {arXiv preprint arXiv:2006.02601}
}
</pre>
</div>

<div id="akwon2020minimax" style="display: none;">
<pre>We study the convergence rates of the EM algorithm for learning two-component mixed linear regression under all regimes of signal-to-noise ratio (SNR). We resolve a long-standing question that many recent results have attempted to tackle: we completely characterize the convergence behavior of EM, and show that the EM algorithm achieves minimax optimal sample complexity under all SNR regimes. In particular, when the SNR is sufficiently large, the EM updates converge to the true parameter θ^* at the standard parametric convergence rate \calo((d/n)^1/2) after \calo(\log(n/d)) iterations. In the regime where the SNR is above \calo((d/n)^1/4) and below some constant, the EM iterates converge to a \calo(\rm SNR^-1 (d/n)^1/2) neighborhood of the true parameter, when the number of iterations is of the order \calo(\rm SNR^-2 \log(n/d)). In the low SNR regime where the SNR is below \calo((d/n)^1/4), we show that EM converges to a \calo((d/n)^1/4) neighborhood of the true parameters, after \calo((n/d)^1/2) iterations. Notably, these results are achieved under mild conditions of either random initialization or an efficiently computable local initialization. By providing tight convergence guarantees of the EM algorithm in middle-to-low SNR regimes, we fill the remaining gap in the literature, and significantly, reveal that in low SNR, EM changes rate, matching the n^-1/4 rate of the MLE, a behavior that previous work had been unable to show. </pre>
</div>


<script>
function toggleAbstractCCkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020minimax(parameter) {
    var x= document.getElementById('Bkwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="basu2020contextual">Basu, Soumya, Orestis Papadigenopoulos, Constantine Caramanis, and Sanjay Shakkottai. “Contextual Blocking Bandits.” In <i>International Conference on Artificial Intelligence and Statistics</i>. PMLR, 2021.</span></div>
<button class="button0" onclick="toggleBibtexbasu2020contextual()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractbasu2020contextual()">Abstract</button>



    <a href="arXiv preprint arXiv:2006.02601"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bbasu2020contextual" style="display: none;">
<pre>@inproceedings{basu2020contextual,
  title = {Contextual Blocking Bandits},
  author = {Basu, Soumya and Papadigenopoulos, Orestis and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {arXiv preprint arXiv:2006.02601}
}
</pre>
</div>

<div id="abasu2020contextual" style="display: none;">
<pre>We study a novel variant of the multi-armed bandit problem, where at each time step, the player observes an independently sampled context that determines the arms’ mean rewards. However, playing an arm blocks it (across all contexts) for a fixed number of future time steps. The above contextual setting captures important scenarios such as recommendation systems or ad placement with diverse users.
This problem has been recently studied \citepDSSX18 in the full-information setting (i.e., assuming knowledge of the mean context-dependent arm rewards), where competitive ratio bounds have been derived.
We focus on the bandit setting, where these means are initially unknown; we propose a UCB-based variant of the full-information algorithm that guarantees a \mathcalO(\log T)-regret w.r.t. an α-optimal strategy in T time steps, matching the Ω(\log(T)) regret lower bound in this setting. Due to the time correlations caused by blocking, existing techniques for upper bounding regret fail. For proving our regret bounds, we introduce the novel concepts of delayed exploitation and opportunistic sub-sampling and combine them with ideas from combinatorial bandits and non-stationary Markov chains coupling.</pre>
</div>


<script>
function toggleAbstractCCbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexbasu2020contextual(parameter) {
    var x= document.getElementById('Bbasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="jalal2020robust">Jalal, Ajil, Liu Liu, Alexandros G Dimakis, and Constantine Caramanis. “Robust Compressed Sensing of Generative Models.” In <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2020.</span></div>
<button class="button0" onclick="toggleBibtexjalal2020robust()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractjalal2020robust()">Abstract</button>



    <a href="https://arxiv.org/pdf/2006.09461.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 

    <a href="https://github.com/ajiljalal/csgm-robust-neurips"><input class="button3" type="button" value="code" /></a>


<div id="Bjalal2020robust" style="display: none;">
<pre>@inproceedings{jalal2020robust,
  title = {Robust compressed sensing of generative models},
  author = {Jalal, Ajil and Liu, Liu and Dimakis, Alexandros G and Caramanis, Constantine},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages = {},
  year = {2020},
  organization = {},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2006.09461.pdf},
  code = {https://github.com/ajiljalal/csgm-robust-neurips}
}
</pre>
</div>

<div id="ajalal2020robust" style="display: none;">
<pre>The goal of compressed sensing is to estimate a high dimensional vector from an underdetermined system of noisy linear equations. In analogy to classical compressed sensing, here we assume a generative model as a prior, that is, we assume the  vector is represented by a deep generative model G: \R^k →\R^n. Classical recovery approaches such as empirical risk minimization (ERM) are guaranteed to succeed when the  measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median of Means (MOM). Our algorithm guarantees recovery for heavy tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm enjoys the same sample complexity guarantees as ERM under sub-Gaussian assumptions. Our experiments validate both aspects of our claims: other algorithms are indeed fragile and fail under heavy tailed and/or corrupted data, while our approach exhibits the predicted robustness.</pre>
</div>


<script>
function toggleAbstractCCjalal2020robust(parameter) {
    var x= document.getElementById('ajalal2020robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexjalal2020robust(parameter) {
    var x= document.getElementById('Bjalal2020robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractjalal2020robust(parameter) {
    var x= document.getElementById('ajalal2020robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="tziotis2020second">Tziotis, Isidoros, Constantine Caramanis, and Aryan Mokhtari. “Second Order Optimality in Decentralized Non-Convex Optimization via Perturbed Gradient Tracking.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i> 33 (2020).</span></div>
<button class="button0" onclick="toggleBibtextziotis2020second()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracttziotis2020second()">Abstract</button>



    <a href="https://proceedings.neurips.cc/paper/2020/file/f1ea154c843f7cf3677db7ce922a2d17-Paper.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Btziotis2020second" style="display: none;">
<pre>@article{tziotis2020second,
  title = {Second Order Optimality in Decentralized Non-Convex Optimization via Perturbed Gradient Tracking},
  author = {Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://proceedings.neurips.cc/paper/2020/file/f1ea154c843f7cf3677db7ce922a2d17-Paper.pdf}
}
</pre>
</div>

<div id="atziotis2020second" style="display: none;">
<pre>In this paper we study the problem of escaping from saddle points and achieving second-order optimality in a decentralized setting where a group of agents collaborate to minimize their aggregate objective function. We provide a non-asymptotic (finite-time) analysis and show that by following the idea of perturbed gradient descent, it is possible to converge to a second-order stationary point in a number of iterations which depends linearly on dimension and polynomially on the accuracy of second-order stationary point. Doing this in a communication-efficient manner requires overcoming several challenges, from identifying (first order) stationary points in a distributed manner, to adapting the perturbed gradient framework without prohibitive communication complexity. Our proposed Perturbed Decentralized Gradient Tracking (PDGT) method consists of two major stages: (i) a gradient based step to find a first-order stationary point and (ii) a perturbed gradient descent step to escape from a first-order stationary point, if it is a saddle point with sufficient curvature. As a side benefit of our result, in the case that all saddle points are non-degenerate (strict), the proposed PDGT method finds a local minimum of the considered decentralized optimization problem in a finite number of iterations.</pre>
</div>


<script>
function toggleAbstractCCtziotis2020second(parameter) {
    var x= document.getElementById('atziotis2020second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtextziotis2020second(parameter) {
    var x= document.getElementById('Btziotis2020second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracttziotis2020second(parameter) {
    var x= document.getElementById('atziotis2020second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="faw2020mix">Faw, Matthew, Rajat Sen, Karthikeyan Shanmugam, Constantine Caramanis, and Sanjay Shakkottai. “Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i> 33 (2020).</span></div>
<button class="button0" onclick="toggleBibtexfaw2020mix()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractfaw2020mix()">Abstract</button>



    <a href="https://arxiv.org/pdf/1907.10154.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bfaw2020mix" style="display: none;">
<pre>@article{faw2020mix,
  title = {Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions},
  author = {Faw, Matthew and Sen, Rajat and Shanmugam, Karthikeyan and Caramanis, Constantine and Shakkottai, Sanjay},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1907.10154.pdf}
}
</pre>
</div>

<div id="afaw2020mix" style="display: none;">
<pre>We consider a covariate shift problem where one has access to several different training datasets for the same learning problem and a small validation set which possibly differs from all the individual training distributions. This covariate shift is caused, in part, due to unobserved features in the datasets. The objective, then, is to find the best mixture distribution over the training datasets (with only observed features) such that training a learning algorithm using this mixture has the best validation performance. Our proposed algorithm, MixNMatch, combines stochastic gradient descent (SGD) with optimistic tree search and model re-use (evolving partially trained models with samples from different mixture distributions) over the space of mixtures, for this task. We prove simple regret guarantees for our algorithm with respect to recovering the optimal mixture, given a total budget of SGD evaluations. Finally, we validate our algorithm on two real-world datasets.</pre>
</div>


<script>
function toggleAbstractCCfaw2020mix(parameter) {
    var x= document.getElementById('afaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexfaw2020mix(parameter) {
    var x= document.getElementById('Bfaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractfaw2020mix(parameter) {
    var x= document.getElementById('afaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2019disentangling">Hoffmann, Jessica, Soumya Basu, Surbhi Goel, and Constantine Caramanis. “Disentangling Mixtures of Epidemics on Graphs.” <i>International Conference on Machine Learning (ICML)</i>, 2020.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2019disentangling()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2019disentangling()">Abstract</button>



    <a href="https://arxiv.org/pdf/1906.06057.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2019disentangling" style="display: none;">
<pre>@article{hoffmann2019disentangling,
  title = {Disentangling Mixtures of Epidemics on Graphs},
  author = {Hoffmann, Jessica and Basu, Soumya and Goel, Surbhi and Caramanis, Constantine},
  journal = {International Conference on Machine Learning (ICML)},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1906.06057.pdf}
}
</pre>
</div>

<div id="ahoffmann2019disentangling" style="display: none;">
<pre>We consider the problem of learning the weighted edges of a balanced mixture of two undirected graphs from epidemic cascades. While mixture models are popular modeling tools, algorithmic development with rigorous guarantees has lagged. Graph mixtures are apparently no exception: until now, very little is known about whether this problem is solvable. To the best of our knowledge, we establish the first necessary and sufficient conditions for this problem to be solvable in polynomial time on edge-separated graphs. When the conditions are met, i.e., when the graphs are connected with at least three edges, we give an efficient algorithm for learning the weights of both graphs with optimal sample complexity (up to log factors). We give complimentary results and provide sample-optimal (up to log factors) algorithms for mixtures of directed graphs of out-degree at least three, for mixture of undirected graphs of unbalanced and/or unknown priors.</pre>
</div>


<script>
function toggleAbstractCChoffmann2019disentangling(parameter) {
    var x= document.getElementById('ahoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2019disentangling(parameter) {
    var x= document.getElementById('Bhoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2019disentangling(parameter) {
    var x= document.getElementById('ahoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2020algorithm">Kwon, Jeongyeol, and Constantine Caramanis. “EM Algorithm Is Sample-Optimal for Learning Mixtures of Well-Separated Gaussians.” <i>The Conference on Learning Theory (COLT)</i>, 2020.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020algorithm()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020algorithm()">Abstract</button>



    <a href="https://arxiv.org/pdf/1906.06057.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020algorithm" style="display: none;">
<pre>@article{kwon2020algorithm,
  title = {EM algorithm is sample-optimal for learning mixtures of well-separated gaussians},
  author = {Kwon, Jeongyeol and Caramanis, Constantine},
  journal = {The Conference on Learning Theory (COLT)},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1906.06057.pdf}
}
</pre>
</div>

<div id="akwon2020algorithm" style="display: none;">
<pre></pre>
</div>


<script>
function toggleAbstractCCkwon2020algorithm(parameter) {
    var x= document.getElementById('akwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020algorithm(parameter) {
    var x= document.getElementById('Bkwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020algorithm(parameter) {
    var x= document.getElementById('akwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kocaoglu2020applications">Kocaoglu, Murat, Sanjay Shakkottai, Alexandros G Dimakis, Constantine Caramanis, and Sriram Vishwanath. “Applications of Common Entropy for Causal Inference.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i> 33 (2020).</span></div>
<button class="button0" onclick="toggleBibtexkocaoglu2020applications()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkocaoglu2020applications()">Abstract</button>



    <a href="https://arxiv.org/pdf/1807.10399.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkocaoglu2020applications" style="display: none;">
<pre>@article{kocaoglu2020applications,
  title = {Applications of Common Entropy for Causal Inference},
  author = {Kocaoglu, Murat and Shakkottai, Sanjay and Dimakis, Alexandros G and Caramanis, Constantine and Vishwanath, Sriram},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1807.10399.pdf}
}
</pre>
</div>

<div id="akocaoglu2020applications" style="display: none;">
<pre>We study the problem of discovering the simplest latent variable that can make two observed discrete variables conditionally independent. The minimum entropy required for such a latent is known as common entropy in information theory. We extend this notion to Renyi common entropy by minimizing the Renyi entropy of the latent variable. To efficiently compute common entropy, we propose an iterative algorithm that can be used to discover the trade-off between the entropy of the latent variable and the conditional mutual information of the observed variables. We show two applications of common entropy in causal inference: First, under the assumption that there are no low-entropy mediators, it can be used to distinguish causation from spurious correlation among almost all joint distributions on simple causal graphs with two observed variables. Second, common entropy can be used to improve constraint-based methods such as PC or FCI algorithms in the small-sample regime, where these methods are known to struggle. We propose a modification to these constraint-based methods to assess if a separating set found by these algorithms is valid using common entropy. We finally evaluate our algorithms on synthetic and real data to establish their performance.</pre>
</div>


<script>
function toggleAbstractCCkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkocaoglu2020applications(parameter) {
    var x= document.getElementById('Bkocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="zhuo2020communication">Zhuo, Jiacheng, Qi Lei, Alex Dimakis, and Constantine Caramanis. “Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-Norm Balls.” In <i>International Conference on Artificial Intelligence and Statistics</i>, 1464–74. PMLR, 2020.</span></div>
<button class="button0" onclick="toggleBibtexzhuo2020communication()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuo2020communication()">Abstract</button>



    <a href="https://arxiv.org/pdf/1910.07703.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bzhuo2020communication" style="display: none;">
<pre>@inproceedings{zhuo2020communication,
  title = {Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls},
  author = {Zhuo, Jiacheng and Lei, Qi and Dimakis, Alex and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {1464--1474},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1910.07703.pdf}
}
</pre>
</div>

<div id="azhuo2020communication" style="display: none;">
<pre>Large-scale machine learning training suffers from two prior challenges, specifically for nuclear-norm constrained problems with distributed systems: the synchronization slowdown due to the straggling workers, and high communication costs. In this work, we propose an asynchronous Stochastic Frank Wolfe (SFW-asyn) method, which, for the first time, solves the two problems simultaneously, while successfully maintaining the same convergence rate as the vanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon EC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number of machines compared to the vanilla SFW.</pre>
</div>


<script>
function toggleAbstractCCzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuo2020communication(parameter) {
    var x= document.getElementById('Bzhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="liu2020high">Liu, Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. “High Dimensional Robust Sparse Regression.” In <i>International Conference on Artificial Intelligence and Statistics</i>, 411–21. PMLR, 2020.</span></div>
<button class="button0" onclick="toggleBibtexliu2020high()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractliu2020high()">Abstract</button>



    <a href="https://arxiv.org/pdf/1805.11643.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bliu2020high" style="display: none;">
<pre>@inproceedings{liu2020high,
  title = {High dimensional robust sparse regression},
  author = {Liu, Liu and Shen, Yanyao and Li, Tianyang and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {411--421},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1805.11643.pdf}
}
</pre>
</div>

<div id="aliu2020high" style="display: none;">
<pre>We provide a novel – and to the best of our knowledge, the first – algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity, in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators: when the covariance matrix in sparse regression is identity, our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm which consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance. Also, it is orderwise more efficient computationally than the ellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.</pre>
</div>


<script>
function toggleAbstractCCliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexliu2020high(parameter) {
    var x= document.getElementById('Bliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2020converges">Kwon, Jeongyeol, and Constantine Caramanis. “EM Converges for a Mixture of Many Linear Regressions.” In <i>International Conference on Artificial Intelligence and Statistics</i>, 1727–36. PMLR, 2020.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020converges()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020converges()">Abstract</button>



    <a href="https://arxiv.org/pdf/1905.12106.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020converges" style="display: none;">
<pre>@inproceedings{kwon2020converges,
  title = {EM converges for a mixture of many linear regressions},
  author = {Kwon, Jeongyeol and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {1727--1736},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1905.12106.pdf}
}
</pre>
</div>

<div id="akwon2020converges" style="display: none;">
<pre>We study the convergence of the Expectation-Maximization (EM) algorithm for mixtures of linear regressions with an arbitrary number k of components. We show that as long as signal-to-noise ratio (SNR) is \tildeΩ(k), well-initialized EM converges to the true regression parameters. Previous results for k ≥3 have only established local convergence for the noiseless setting, i.e., where SNR is infinitely large. Our results enlarge the scope to the environment with noises, and notably, we establish a statistical error rate that is independent of the norm (or pairwise distance) of the regression parameters. In particular, our results imply exact recovery as σ→0, in contrast to most previous local convergence results for EM, where the statistical error scaled with the norm of parameters. Standard moment-method approaches may be applied to guarantee we are in the region where our local convergence guarantees apply.</pre>
</div>


<script>
function toggleAbstractCCkwon2020converges(parameter) {
    var x= document.getElementById('akwon2020converges');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020converges(parameter) {
    var x= document.getElementById('Bkwon2020converges');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020converges(parameter) {
    var x= document.getElementById('akwon2020converges');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="lei2019primal">Lei, Qi, Jiacheng Zhuo, Constantine Caramanis, Inderjit S Dhillon, and Alexandros G Dimakis. “Primal-Dual Block Generalized Frank-Wolfe.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i> 32 (2019): 13866–75.</span></div>
<button class="button0" onclick="toggleBibtexlei2019primal()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractlei2019primal()">Abstract</button>



    <a href="https://arxiv.org/pdf/1906.02436.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Blei2019primal" style="display: none;">
<pre>@article{lei2019primal,
  title = {Primal-dual block generalized frank-wolfe},
  author = {Lei, Qi and Zhuo, Jiacheng and Caramanis, Constantine and Dhillon, Inderjit S and Dimakis, Alexandros G},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {32},
  pages = {13866--13875},
  year = {2019},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1906.02436.pdf}
}
</pre>
</div>

<div id="alei2019primal" style="display: none;">
<pre>We propose a variant of the Frank-Wolfe algorithm for solving a class of sparse/low-rank optimization problems. Our formulation includes Elastic Net, regularized SVMs and phase retrieval as special cases. The proposed Primal-Dual Block Frank-Wolfe algorithm reduces the per-iteration cost while maintaining linear convergence rate. The per iteration cost of our method depends on the structural complexity of the solution (i.e. sparsity/low-rank) instead of the ambient dimension. We empirically show that our algorithm outperforms the state-of-the-art methods on (multi-class) classification tasks.</pre>
</div>


<script>
function toggleAbstractCClei2019primal(parameter) {
    var x= document.getElementById('alei2019primal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexlei2019primal(parameter) {
    var x= document.getElementById('Blei2019primal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractlei2019primal(parameter) {
    var x= document.getElementById('alei2019primal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2019global">Kwon, Jeongyeol, Wei Qian, Constantine Caramanis, Yudong Chen, and Damek Davis. “Global Convergence of the Em Algorithm for Mixtures of Two Component Linear Regression.” In <i>Conference on Learning Theory (COLT)</i>, 2055–2110. PMLR, 2019.</span></div>
<button class="button0" onclick="toggleBibtexkwon2019global()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2019global()">Abstract</button>



    <a href="https://arxiv.org/pdf/1810.05752.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2019global" style="display: none;">
<pre>@inproceedings{kwon2019global,
  title = {Global convergence of the em algorithm for mixtures of two component linear regression},
  author = {Kwon, Jeongyeol and Qian, Wei and Caramanis, Constantine and Chen, Yudong and Davis, Damek},
  booktitle = {Conference on Learning Theory (COLT)},
  pages = {2055--2110},
  year = {2019},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1810.05752.pdf}
}
</pre>
</div>

<div id="akwon2019global" style="display: none;">
<pre>The Expectation-Maximization algorithm is perhaps the most broadly used algorithm for inference of latent variable problems. A theoretical understanding of its performance, however, largely remains lacking. Recent results established that EM enjoys global convergence for Gaussian Mixture Models. For Mixed Linear Regression, however, only local convergence results have been established, and those only for the high SNR regime. We show here that EM converges for mixed linear regression with two components (it is known that it may fail to converge for three or more), and moreover that this convergence holds for random initialization. Our analysis reveals that EM exhibits very different behavior in Mixed Linear Regression from its behavior in Gaussian Mixture Models, and hence our proofs require the development of several new ideas.</pre>
</div>


<script>
function toggleAbstractCCkwon2019global(parameter) {
    var x= document.getElementById('akwon2019global');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2019global(parameter) {
    var x= document.getElementById('Bkwon2019global');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2019global(parameter) {
    var x= document.getElementById('akwon2019global');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2019learning">Hoffmann, Jessica, and Constantine Caramanis. “Learning Graphs from Noisy Epidemic Cascades.” <i>Proceedings of the ACM on Measurement and Analysis of Computing Systems</i> 3, no. 2 (2019): 1–34.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2019learning()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2019learning()">Abstract</button>



    <a href="https://arxiv.org/pdf/1903.02650.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2019learning" style="display: none;">
<pre>@article{hoffmann2019learning,
  title = {Learning graphs from noisy epidemic cascades},
  author = {Hoffmann, Jessica and Caramanis, Constantine},
  journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume = {3},
  number = {2},
  pages = {1--34},
  year = {2019},
  publisher = {ACM New York, NY, USA},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1903.02650.pdf}
}
</pre>
</div>

<div id="ahoffmann2019learning" style="display: none;">
<pre>We consider the problem of learning the weighted edges of a graph by observing the noisy times of infection for multiple epidemic cascades on this graph. Past work has considered this problem when the cascade information, i.e., infection times, are known exactly. Though the noisy setting is well motivated by many epidemic processes (e.g., most human epidemics), to the best of our knowledge, very little is known about when it is solvable. Previous work on the no-noise setting critically uses the ordering information. If noise can reverse this – a node’s reported (noisy) infection time comes after the reported infection time of some node it infected – then we are unable to see how previous results can be extended.
We therefore tackle two versions of the noisy setting: the limited-noise setting, where we know noisy times of infections, and the extreme-noise setting, in which we only know whether or not a node was infected. We provide a polynomial time algorithm for recovering the structure of bidirectional trees in the extreme-noise setting, and show our algorithm almost matches lower bounds established in the no-noise setting, and hence is optimal up to log-factors. We extend our results for general degree-bounded graphs, where again we show that our (poly-time) algorithm can recover the structure of the graph with optimal sample complexity. We also provide the first efficient algorithm to learn the weights of the bidirectional tree in the limited-noise setting. Finally, we give a polynomial time algorithm for learning the weights of general bounded-degree graphs in the limited-noise setting. This algorithm extends to general graphs (at the price of exponential running time), proving the problem is solvable in the general case. All our algorithms work for any noise distribution, without any restriction on the variance.</pre>
</div>


<script>
function toggleAbstractCChoffmann2019learning(parameter) {
    var x= document.getElementById('ahoffmann2019learning');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2019learning(parameter) {
    var x= document.getElementById('Bhoffmann2019learning');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2019learning(parameter) {
    var x= document.getElementById('ahoffmann2019learning');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="katiyar2019robust">Katiyar, Ashish, Jessica Hoffmann, and Constantine Caramanis. “Robust Estimation of Tree Structured Gaussian Graphical Models.” In <i>International Conference on Machine Learning</i>, 3292–3300. PMLR, 2019.</span></div>
<button class="button0" onclick="toggleBibtexkatiyar2019robust()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkatiyar2019robust()">Abstract</button>



    <a href="https://arxiv.org/pdf/1901.08770.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkatiyar2019robust" style="display: none;">
<pre>@inproceedings{katiyar2019robust,
  title = {Robust estimation of tree structured Gaussian graphical models},
  author = {Katiyar, Ashish and Hoffmann, Jessica and Caramanis, Constantine},
  booktitle = {International Conference on Machine Learning},
  pages = {3292--3300},
  year = {2019},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1901.08770.pdf}
}
</pre>
</div>

<div id="akatiyar2019robust" style="display: none;">
<pre>Consider jointly Gaussian random variables whose conditional independence structure is specified by a graphical model. If we observe realizations of the variables, we can compute the covariance matrix, and it is well known that the support of the inverse covariance matrix corresponds to the edges of the graphical model. Instead, suppose we only have noisy observations. If the noise at each node is independent, we can compute the sum of the covariance matrix and an unknown diagonal. The inverse of this sum is (in general) dense. We ask: can the original independence structure be recovered? We address this question for tree structured graphical models. We prove that this problem is unidentifiable, but show that this unidentifiability is limited to a small class of candidate trees. We further present additional constraints under which the problem is identifiable. Finally, we provide an O(n^3) algorithm to find this equivalence class of trees.</pre>
</div>


<script>
function toggleAbstractCCkatiyar2019robust(parameter) {
    var x= document.getElementById('akatiyar2019robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkatiyar2019robust(parameter) {
    var x= document.getElementById('Bkatiyar2019robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkatiyar2019robust(parameter) {
    var x= document.getElementById('akatiyar2019robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2018cost">Hoffmann, Jessica, and Constantine Caramanis. “The Cost of Uncertainty in Curing Epidemics.” <i>Proceedings of the ACM on Measurement and Analysis of Computing Systems</i> 2, no. 2 (2018): 1–33.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2018cost()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2018cost()">Abstract</button>



    <a href="https://arxiv.org/pdf/1711.00167.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2018cost" style="display: none;">
<pre>@article{hoffmann2018cost,
  title = {The Cost of Uncertainty in Curing Epidemics},
  author = {Hoffmann, Jessica and Caramanis, Constantine},
  journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume = {2},
  number = {2},
  pages = {1--33},
  year = {2018},
  publisher = {ACM New York, NY, USA},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1711.00167.pdf}
}
</pre>
</div>

<div id="ahoffmann2018cost" style="display: none;">
<pre>Motivated by the study of controlling (curing) epidemics, we consider the spread of an SI process on a known graph, where we have a limited budget to use to transition infected nodes back to the susceptible state (i.e., to cure nodes). Recent work has demonstrated that under perfect and instantaneous information (which nodes are/are not infected), the budget required for curing a graph precisely depends on a combinatorial property called the CutWidth. We show that this assumption is in fact necessary: even a minor degradation of perfect information, e.g., a diagnostic test that is 99% accurate, drastically alters the landscape. Infections that could previously be cured in sublinear time now may require exponential time, or orderwise larger budget to cure. The crux of the issue comes down to a tension not present in the full information case: if a node is suspected (but not certain) to be infected, do we risk wasting our budget to try to cure an uninfected node, or increase our certainty by longer observation, at the risk that the infection spreads further? Our results present fundamental, algorithm-independent bounds that tradeoff budget required vs. uncertainty.</pre>
</div>


<script>
function toggleAbstractCChoffmann2018cost(parameter) {
    var x= document.getElementById('ahoffmann2018cost');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2018cost(parameter) {
    var x= document.getElementById('Bhoffmann2018cost');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2018cost(parameter) {
    var x= document.getElementById('ahoffmann2018cost');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="sinno2018second">Sinno, Zeina, Constantine Caramanis, and Alan Bovik. “Second Order Natural Scene Statistics Model of Blind Image Quality Assessment.” In <i>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 1238–42. IEEE, 2018.</span></div>
<button class="button0" onclick="toggleBibtexsinno2018second()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractsinno2018second()">Abstract</button>



 


<div id="Bsinno2018second" style="display: none;">
<pre>@inproceedings{sinno2018second,
  title = {Second order natural scene statistics model of blind image quality assessment},
  author = {Sinno, Zeina and Caramanis, Constantine and Bovik, Alan},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {1238--1242},
  year = {2018},
  organization = {IEEE},
  group = {proceedings}
}
</pre>
</div>

<div id="asinno2018second" style="display: none;">
<pre>The univariate statistics of bandpass-filtered images provide powerful features that drive many successful image quality assessment (IQA) algorithms. Bivariate Natural Scene Statistics (NSS), which model the joint statistics of multiple bandpass image samples also provide potentially powerful features to assess the perceptual quality of images, by capturing both image and distortion correlations. Here, we make the first attempt to use bivariate NSS features to build a model of no-reference image quality prediction. We show that our bivariate model outperforms existing state of the art image quality predictors.</pre>
</div>


<script>
function toggleAbstractCCsinno2018second(parameter) {
    var x= document.getElementById('asinno2018second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexsinno2018second(parameter) {
    var x= document.getElementById('Bsinno2018second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractsinno2018second(parameter) {
    var x= document.getElementById('asinno2018second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="li2018statistical">Li, Tianyang, Liu Liu, Anastasios Kyrillidis, and Constantine Caramanis. “Statistical Inference Using SGD.” In <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, Vol. 32, 2018.</span></div>
<button class="button0" onclick="toggleBibtexli2018statistical()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractli2018statistical()">Abstract</button>



    <a href="https://arxiv.org/pdf/1705.07477.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bli2018statistical" style="display: none;">
<pre>@inproceedings{li2018statistical,
  title = {Statistical inference using SGD},
  author = {Li, Tianyang and Liu, Liu and Kyrillidis, Anastasios and Caramanis, Constantine},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  year = {2018},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1705.07477.pdf}
}
</pre>
</div>

<div id="ali2018statistical" style="display: none;">
<pre>We present a novel method for frequentist statistical inference in M-estimation problems, based on stochastic gradient descent (SGD) with a fixed step size: we demonstrate that the average of such SGD sequences can be used for statistical inference, after proper scaling. An intuitive analysis using the Ornstein-Uhlenbeck process suggests that such averages are asymptotically normal. From a practical perspective, our SGD-based inference procedure is a first order method, and is well-suited for large scale problems. To show its merits, we apply it to both synthetic and real datasets, and demonstrate that its accuracy is comparable to classical statistical methods, while requiring potentially far less computation.</pre>
</div>


<script>
function toggleAbstractCCli2018statistical(parameter) {
    var x= document.getElementById('ali2018statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexli2018statistical(parameter) {
    var x= document.getElementById('Bli2018statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractli2018statistical(parameter) {
    var x= document.getElementById('ali2018statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="park2017non">Park, Dohyung, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. “Non-Square Matrix Sensing without Spurious Local Minima via the Burer-Monteiro Approach.” In <i>Artificial Intelligence and Statistics</i>, 65–74. PMLR, 2017.</span></div>
<button class="button0" onclick="toggleBibtexpark2017non()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpark2017non()">Abstract</button>



    <a href="https://arxiv.org/pdf/1609.03240.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bpark2017non" style="display: none;">
<pre>@inproceedings{park2017non,
  title = {Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach},
  author = {Park, Dohyung and Kyrillidis, Anastasios and Carmanis, Constantine and Sanghavi, Sujay},
  booktitle = {Artificial Intelligence and Statistics},
  pages = {65--74},
  year = {2017},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1609.03240.pdf}
}
</pre>
</div>

<div id="apark2017non" style="display: none;">
<pre>We consider the non-square matrix sensing problem, under restricted isometry property (RIP) assumptions. We focus on the non-convex formulation, where any rank-r matrix X ∈\mathbbR^m \times n is represented as UB^⊤, where U ∈\mathbbR^m \times r and V ∈\mathbbR^n \times r. In this paper, we complement recent findings on the non-convex geometry of the analogous PSD setting [5], and show that matrix factorization does not introduce any spurious local minima, under RIP.</pre>
</div>


<script>
function toggleAbstractCCpark2017non(parameter) {
    var x= document.getElementById('apark2017non');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpark2017non(parameter) {
    var x= document.getElementById('Bpark2017non');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpark2017non(parameter) {
    var x= document.getElementById('apark2017non');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>

<h2 id="journal-papers">Journal Papers</h2>

<ol class="bibliography"><li><div class="text-justify"><span id="kyrillidis2018provable">Kyrillidis, Anastasios, Amir Kalev, Dohyung Park, Srinadh Bhojanapalli, Constantine Caramanis, and Sujay Sanghavi. “Provable Compressed Sensing Quantum State Tomography via Non-Convex Methods.” <i>Npj Quantum Information</i> 4, no. 1 (2018): 1–7.</span></div>
<button class="button0" onclick="toggleBibtexkyrillidis2018provable()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkyrillidis2018provable()">Abstract</button>



    <a href="https://arxiv.org/pdf/1711.02524.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkyrillidis2018provable" style="display: none;">
<pre>@article{kyrillidis2018provable,
  title = {Provable compressed sensing quantum state tomography via non-convex methods},
  author = {Kyrillidis, Anastasios and Kalev, Amir and Park, Dohyung and Bhojanapalli, Srinadh and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {npj Quantum Information},
  volume = {4},
  number = {1},
  pages = {1--7},
  year = {2018},
  publisher = {Nature Publishing Group},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1711.02524.pdf}
}
</pre>
</div>

<div id="akyrillidis2018provable" style="display: none;">
<pre>With nowadays steadily growing quantum processors, it is required to develop new quantum tomography tools that are tailored for high-dimensional systems. In this work, we describe such a computational tool, based on recent ideas from non-convex optimization. The algorithm excels in the compressed-sensing-like setting, where only a few data points are measured from a low-rank or highly-pure quantum state of a high-dimensional system. We show that the algorithm can practically be used in quantum tomography problems that are beyond the reach of convex solvers, and, moreover, is faster than other state-of-the-art non-convex approaches. Crucially, we prove that, despite being a non-convex program, under mild conditions, the algorithm is guaranteed to converge to the global minimum of the problem; thus, it constitutes a provable quantum state tomography protocol.</pre>
</div>


<script>
function toggleAbstractCCkyrillidis2018provable(parameter) {
    var x= document.getElementById('akyrillidis2018provable');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkyrillidis2018provable(parameter) {
    var x= document.getElementById('Bkyrillidis2018provable');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkyrillidis2018provable(parameter) {
    var x= document.getElementById('akyrillidis2018provable');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="park2018finding">Park, Dohyung, Anastasios Kyrillidis, Constantine Caramanis, and Sujay Sanghavi. “Finding Low-Rank Solutions via Nonconvex Matrix Factorization, Efficiently and Provably.” <i>SIAM Journal on Imaging Sciences</i> 11, no. 4 (2018): 2165–2204.</span></div>
<button class="button0" onclick="toggleBibtexpark2018finding()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpark2018finding()">Abstract</button>



    <a href="https://arxiv.org/pdf/1606.03168.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bpark2018finding" style="display: none;">
<pre>@article{park2018finding,
  title = {Finding low-rank solutions via nonconvex matrix factorization, efficiently and provably},
  author = {Park, Dohyung and Kyrillidis, Anastasios and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {SIAM Journal on Imaging Sciences},
  volume = {11},
  number = {4},
  pages = {2165--2204},
  year = {2018},
  publisher = {SIAM},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1606.03168.pdf}
}
</pre>
</div>

<div id="apark2018finding" style="display: none;">
<pre>A rank-r matrix X ∈\mathbbR^m \times n can be written as a product UV^⊤, where U ∈\mathbbR^m \times r and V ∈\mathbbR^n \times r. One could exploit this observation in optimization: e.g., consider the minimization of a convex function f(X) over rank-r matrices, where the set of rank-r matrices is modeled via the factorization UV^⊤. Though such parameterization reduces the number of variables, and is more computationally efficient (of particular interest is the case r \lleq minm,n), it comes at a cost: f(UV^⊤) becomes a non-convex function w.r.t. U and V. 
   
   We study such parameterization for optimization of generic convex objectives f, and focus on first-order, gradient descent algorithmic solutions. We propose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient first-order method that operates on the U,V factors. We show that when f is (restricted) smooth, BFGD has local sublinear convergence, and linear convergence when f is both (restricted) smooth and (restricted) strongly convex. For several key applications, we provide simple and efficient initialization schemes that provide approximate solutions good enough for the above convergence results to hold.</pre>
</div>


<script>
function toggleAbstractCCpark2018finding(parameter) {
    var x= document.getElementById('apark2018finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpark2018finding(parameter) {
    var x= document.getElementById('Bpark2018finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpark2018finding(parameter) {
    var x= document.getElementById('apark2018finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>

<h2 id="books-chapters">Books Chapters</h2>

<ol class="bibliography"></ol>

<h2 id="lecture-notes">Lecture Notes</h2>

<ol class="bibliography"></ol>


</div>

    </div>
    
  </body>
</html>
