<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Full list of publications &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">


      

      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item active" href="/publications/">Publications</a>
      <a class="sidebar-nav-item" href="/researchprojects/">Research Projects</a>
      <a class="sidebar-nav-item" href="https://scholar.google.com/citations?user=47YTUrEAAAAJ&hl=en&oi=ao">Google Scholar</a>
      <a class="sidebar-nav-item" href="https://www.youtube.com/channel/UCSv1_NZITsPl-abaCWtRrJg">YouTube</a>

      <span class="sidebar-nav-item">Jekyll Hyde, Currently v2.1.0</span>
    </nav>

    <p>&copy; 2021. All rights reserved.</p> 
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Full list of publications</h1>
  <h2 id="pre-prints">Pre-prints</h2>

<ol class="bibliography"><li><div class="text-justify"><span id="yi2016solving">Yi, X., Caramanis, C., &amp; Sanghavi, S. (2016). Solving a mixture of many random linear equations by tensor decomposition and alternating minimization. <i>ArXiv Preprint ArXiv:1608.05749</i>.</span></div>
<button class="button0" onclick="toggleBibtexyi2016solving()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2016solving()">Abstract</button>



    <a href="https://arxiv.org/pdf/1608.05749.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2016solving" style="display: none;">
<pre>@article{yi2016solving,
  title = {Solving a mixture of many random linear equations by tensor decomposition and alternating minimization},
  author = {Yi, Xinyang and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {arXiv preprint arXiv:1608.05749},
  year = {2016},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/1608.05749.pdf}
}
</pre>
</div>

<div id="ayi2016solving" style="display: none;">
<pre>We consider the problem of solving mixed random linear equations with k components. This is the noiseless setting of mixed linear regression. The goal is to estimate multiple linear models from mixed samples in the case where the labels (which sample corresponds to which model) are not observed. We give a tractable algorithm for the mixed linear equation problem, and show that under some technical conditions, our algorithm is guaranteed to solve the problem exactly with sample complexity linear in the dimension, and polynomial in k, the number of components. Previous approaches have required either exponential dependence on k, or super-linear dependence on the dimension. The proposed algorithm is a combination of tensor decomposition and alternating minimization. Our analysis involves proving that the initialization provided by the tensor method allows alternating minimization, which is equivalent to EM in our setting, to converge to the global optimum at a linear rate.</pre>
</div>


<script>
function toggleAbstractCCyi2016solving(parameter) {
    var x= document.getElementById('ayi2016solving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2016solving(parameter) {
    var x= document.getElementById('Byi2016solving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2016solving(parameter) {
    var x= document.getElementById('ayi2016solving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>

<h2 id="conference-papers">Conference Papers</h2>

<ol class="bibliography"><li><div class="text-justify"><span id="kwon2020minimax">Kwon, J., Ho, N., &amp; Caramanis, C. (2021). On the minimax optimality of the em algorithm for learning two-component mixed linear regression. <i>International Conference on Artificial Intelligence and Statistics</i>.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020minimax()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020minimax()">Abstract</button>



    <a href="arXiv preprint arXiv:2006.02601"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020minimax" style="display: none;">
<pre>@inproceedings{kwon2020minimax,
  title = {On the minimax optimality of the em algorithm for learning two-component mixed linear regression},
  author = {Kwon, Jeongyeol and Ho, Nhat and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {arXiv preprint arXiv:2006.02601}
}
</pre>
</div>

<div id="akwon2020minimax" style="display: none;">
<pre>We study the convergence rates of the EM algorithm for learning two-component mixed linear regression under all regimes of signal-to-noise ratio (SNR). We resolve a long-standing question that many recent results have attempted to tackle: we completely characterize the convergence behavior of EM, and show that the EM algorithm achieves minimax optimal sample complexity under all SNR regimes. In particular, when the SNR is sufficiently large, the EM updates converge to the true parameter θ^* at the standard parametric convergence rate \calo((d/n)^1/2) after \calo(\log(n/d)) iterations. In the regime where the SNR is above \calo((d/n)^1/4) and below some constant, the EM iterates converge to a \calo(\rm SNR^-1 (d/n)^1/2) neighborhood of the true parameter, when the number of iterations is of the order \calo(\rm SNR^-2 \log(n/d)). In the low SNR regime where the SNR is below \calo((d/n)^1/4), we show that EM converges to a \calo((d/n)^1/4) neighborhood of the true parameters, after \calo((n/d)^1/2) iterations. Notably, these results are achieved under mild conditions of either random initialization or an efficiently computable local initialization. By providing tight convergence guarantees of the EM algorithm in middle-to-low SNR regimes, we fill the remaining gap in the literature, and significantly, reveal that in low SNR, EM changes rate, matching the n^-1/4 rate of the MLE, a behavior that previous work had been unable to show. </pre>
</div>


<script>
function toggleAbstractCCkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020minimax(parameter) {
    var x= document.getElementById('Bkwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="basu2020contextual">Basu, S., Papadigenopoulos, O., Caramanis, C., &amp; Shakkottai, S. (2021). Contextual Blocking Bandits. <i>International Conference on Artificial Intelligence and Statistics</i>.</span></div>
<button class="button0" onclick="toggleBibtexbasu2020contextual()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractbasu2020contextual()">Abstract</button>



    <a href="arXiv preprint arXiv:2006.02601"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bbasu2020contextual" style="display: none;">
<pre>@inproceedings{basu2020contextual,
  title = {Contextual Blocking Bandits},
  author = {Basu, Soumya and Papadigenopoulos, Orestis and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {arXiv preprint arXiv:2006.02601}
}
</pre>
</div>

<div id="abasu2020contextual" style="display: none;">
<pre>We study a novel variant of the multi-armed bandit problem, where at each time step, the player observes an independently sampled context that determines the arms’ mean rewards. However, playing an arm blocks it (across all contexts) for a fixed number of future time steps. The above contextual setting captures important scenarios such as recommendation systems or ad placement with diverse users.
This problem has been recently studied \citepDSSX18 in the full-information setting (i.e., assuming knowledge of the mean context-dependent arm rewards), where competitive ratio bounds have been derived.
We focus on the bandit setting, where these means are initially unknown; we propose a UCB-based variant of the full-information algorithm that guarantees a \mathcalO(\log T)-regret w.r.t. an α-optimal strategy in T time steps, matching the Ω(\log(T)) regret lower bound in this setting. Due to the time correlations caused by blocking, existing techniques for upper bounding regret fail. For proving our regret bounds, we introduce the novel concepts of delayed exploitation and opportunistic sub-sampling and combine them with ideas from combinatorial bandits and non-stationary Markov chains coupling.</pre>
</div>


<script>
function toggleAbstractCCbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexbasu2020contextual(parameter) {
    var x= document.getElementById('Bbasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="jalal2020robust">Jalal, A., Liu, L., Dimakis, A. G., &amp; Caramanis, C. (2020). Robust compressed sensing of generative models. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>.</span></div>
<button class="button0" onclick="toggleBibtexjalal2020robust()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractjalal2020robust()">Abstract</button>



    <a href="https://arxiv.org/pdf/2006.09461.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 

    <a href="https://github.com/ajiljalal/csgm-robust-neurips"><input class="button3" type="button" value="code" /></a>


<div id="Bjalal2020robust" style="display: none;">
<pre>@inproceedings{jalal2020robust,
  title = {Robust compressed sensing of generative models},
  author = {Jalal, Ajil and Liu, Liu and Dimakis, Alexandros G and Caramanis, Constantine},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages = {},
  year = {2020},
  organization = {},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2006.09461.pdf},
  code = {https://github.com/ajiljalal/csgm-robust-neurips}
}
</pre>
</div>

<div id="ajalal2020robust" style="display: none;">
<pre>The goal of compressed sensing is to estimate a high dimensional vector from an underdetermined system of noisy linear equations. In analogy to classical compressed sensing, here we assume a generative model as a prior, that is, we assume the  vector is represented by a deep generative model G: \R^k →\R^n. Classical recovery approaches such as empirical risk minimization (ERM) are guaranteed to succeed when the  measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median of Means (MOM). Our algorithm guarantees recovery for heavy tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm enjoys the same sample complexity guarantees as ERM under sub-Gaussian assumptions. Our experiments validate both aspects of our claims: other algorithms are indeed fragile and fail under heavy tailed and/or corrupted data, while our approach exhibits the predicted robustness.</pre>
</div>


<script>
function toggleAbstractCCjalal2020robust(parameter) {
    var x= document.getElementById('ajalal2020robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexjalal2020robust(parameter) {
    var x= document.getElementById('Bjalal2020robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractjalal2020robust(parameter) {
    var x= document.getElementById('ajalal2020robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="tziotis2020second">Tziotis, I., Caramanis, C., &amp; Mokhtari, A. (2020). Second Order Optimality in Decentralized Non-Convex Optimization via Perturbed Gradient Tracking. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>33</i>.</span></div>
<button class="button0" onclick="toggleBibtextziotis2020second()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracttziotis2020second()">Abstract</button>



    <a href="https://proceedings.neurips.cc/paper/2020/file/f1ea154c843f7cf3677db7ce922a2d17-Paper.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Btziotis2020second" style="display: none;">
<pre>@article{tziotis2020second,
  title = {Second Order Optimality in Decentralized Non-Convex Optimization via Perturbed Gradient Tracking},
  author = {Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://proceedings.neurips.cc/paper/2020/file/f1ea154c843f7cf3677db7ce922a2d17-Paper.pdf}
}
</pre>
</div>

<div id="atziotis2020second" style="display: none;">
<pre>In this paper we study the problem of escaping from saddle points and achieving second-order optimality in a decentralized setting where a group of agents collaborate to minimize their aggregate objective function. We provide a non-asymptotic (finite-time) analysis and show that by following the idea of perturbed gradient descent, it is possible to converge to a second-order stationary point in a number of iterations which depends linearly on dimension and polynomially on the accuracy of second-order stationary point. Doing this in a communication-efficient manner requires overcoming several challenges, from identifying (first order) stationary points in a distributed manner, to adapting the perturbed gradient framework without prohibitive communication complexity. Our proposed Perturbed Decentralized Gradient Tracking (PDGT) method consists of two major stages: (i) a gradient based step to find a first-order stationary point and (ii) a perturbed gradient descent step to escape from a first-order stationary point, if it is a saddle point with sufficient curvature. As a side benefit of our result, in the case that all saddle points are non-degenerate (strict), the proposed PDGT method finds a local minimum of the considered decentralized optimization problem in a finite number of iterations.</pre>
</div>


<script>
function toggleAbstractCCtziotis2020second(parameter) {
    var x= document.getElementById('atziotis2020second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtextziotis2020second(parameter) {
    var x= document.getElementById('Btziotis2020second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracttziotis2020second(parameter) {
    var x= document.getElementById('atziotis2020second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="faw2020mix">Faw, M., Sen, R., Shanmugam, K., Caramanis, C., &amp; Shakkottai, S. (2020). Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>33</i>.</span></div>
<button class="button0" onclick="toggleBibtexfaw2020mix()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractfaw2020mix()">Abstract</button>



    <a href="https://arxiv.org/pdf/1907.10154.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bfaw2020mix" style="display: none;">
<pre>@article{faw2020mix,
  title = {Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions},
  author = {Faw, Matthew and Sen, Rajat and Shanmugam, Karthikeyan and Caramanis, Constantine and Shakkottai, Sanjay},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1907.10154.pdf}
}
</pre>
</div>

<div id="afaw2020mix" style="display: none;">
<pre>We consider a covariate shift problem where one has access to several different training datasets for the same learning problem and a small validation set which possibly differs from all the individual training distributions. This covariate shift is caused, in part, due to unobserved features in the datasets. The objective, then, is to find the best mixture distribution over the training datasets (with only observed features) such that training a learning algorithm using this mixture has the best validation performance. Our proposed algorithm, MixNMatch, combines stochastic gradient descent (SGD) with optimistic tree search and model re-use (evolving partially trained models with samples from different mixture distributions) over the space of mixtures, for this task. We prove simple regret guarantees for our algorithm with respect to recovering the optimal mixture, given a total budget of SGD evaluations. Finally, we validate our algorithm on two real-world datasets.</pre>
</div>


<script>
function toggleAbstractCCfaw2020mix(parameter) {
    var x= document.getElementById('afaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexfaw2020mix(parameter) {
    var x= document.getElementById('Bfaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractfaw2020mix(parameter) {
    var x= document.getElementById('afaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2019disentangling">Hoffmann, J., Basu, S., Goel, S., &amp; Caramanis, C. (2020). Disentangling Mixtures of Epidemics on Graphs. <i>International Conference on Machine Learning (ICML)</i>.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2019disentangling()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2019disentangling()">Abstract</button>



    <a href="https://arxiv.org/pdf/1906.06057.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2019disentangling" style="display: none;">
<pre>@article{hoffmann2019disentangling,
  title = {Disentangling Mixtures of Epidemics on Graphs},
  author = {Hoffmann, Jessica and Basu, Soumya and Goel, Surbhi and Caramanis, Constantine},
  journal = {International Conference on Machine Learning (ICML)},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1906.06057.pdf}
}
</pre>
</div>

<div id="ahoffmann2019disentangling" style="display: none;">
<pre>We consider the problem of learning the weighted edges of a balanced mixture of two undirected graphs from epidemic cascades. While mixture models are popular modeling tools, algorithmic development with rigorous guarantees has lagged. Graph mixtures are apparently no exception: until now, very little is known about whether this problem is solvable. To the best of our knowledge, we establish the first necessary and sufficient conditions for this problem to be solvable in polynomial time on edge-separated graphs. When the conditions are met, i.e., when the graphs are connected with at least three edges, we give an efficient algorithm for learning the weights of both graphs with optimal sample complexity (up to log factors). We give complimentary results and provide sample-optimal (up to log factors) algorithms for mixtures of directed graphs of out-degree at least three, for mixture of undirected graphs of unbalanced and/or unknown priors.</pre>
</div>


<script>
function toggleAbstractCChoffmann2019disentangling(parameter) {
    var x= document.getElementById('ahoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2019disentangling(parameter) {
    var x= document.getElementById('Bhoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2019disentangling(parameter) {
    var x= document.getElementById('ahoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2020algorithm">Kwon, J., &amp; Caramanis, C. (2020). EM algorithm is sample-optimal for learning mixtures of well-separated gaussians. <i>The Conference on Learning Theory (COLT)</i>.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020algorithm()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020algorithm()">Abstract</button>



    <a href="https://arxiv.org/pdf/1906.06057.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020algorithm" style="display: none;">
<pre>@article{kwon2020algorithm,
  title = {EM algorithm is sample-optimal for learning mixtures of well-separated gaussians},
  author = {Kwon, Jeongyeol and Caramanis, Constantine},
  journal = {The Conference on Learning Theory (COLT)},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1906.06057.pdf}
}
</pre>
</div>

<div id="akwon2020algorithm" style="display: none;">
<pre></pre>
</div>


<script>
function toggleAbstractCCkwon2020algorithm(parameter) {
    var x= document.getElementById('akwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020algorithm(parameter) {
    var x= document.getElementById('Bkwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020algorithm(parameter) {
    var x= document.getElementById('akwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kocaoglu2020applications">Kocaoglu, M., Shakkottai, S., Dimakis, A. G., Caramanis, C., &amp; Vishwanath, S. (2020). Applications of Common Entropy for Causal Inference. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>33</i>.</span></div>
<button class="button0" onclick="toggleBibtexkocaoglu2020applications()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkocaoglu2020applications()">Abstract</button>



    <a href="https://arxiv.org/pdf/1807.10399.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkocaoglu2020applications" style="display: none;">
<pre>@article{kocaoglu2020applications,
  title = {Applications of Common Entropy for Causal Inference},
  author = {Kocaoglu, Murat and Shakkottai, Sanjay and Dimakis, Alexandros G and Caramanis, Constantine and Vishwanath, Sriram},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1807.10399.pdf}
}
</pre>
</div>

<div id="akocaoglu2020applications" style="display: none;">
<pre>We study the problem of discovering the simplest latent variable that can make two observed discrete variables conditionally independent. The minimum entropy required for such a latent is known as common entropy in information theory. We extend this notion to Renyi common entropy by minimizing the Renyi entropy of the latent variable. To efficiently compute common entropy, we propose an iterative algorithm that can be used to discover the trade-off between the entropy of the latent variable and the conditional mutual information of the observed variables. We show two applications of common entropy in causal inference: First, under the assumption that there are no low-entropy mediators, it can be used to distinguish causation from spurious correlation among almost all joint distributions on simple causal graphs with two observed variables. Second, common entropy can be used to improve constraint-based methods such as PC or FCI algorithms in the small-sample regime, where these methods are known to struggle. We propose a modification to these constraint-based methods to assess if a separating set found by these algorithms is valid using common entropy. We finally evaluate our algorithms on synthetic and real data to establish their performance.</pre>
</div>


<script>
function toggleAbstractCCkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkocaoglu2020applications(parameter) {
    var x= document.getElementById('Bkocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="zhuo2020communication">Zhuo, J., Lei, Q., Dimakis, A., &amp; Caramanis, C. (2020). Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls. <i>International Conference on Artificial Intelligence and Statistics</i>, 1464–1474.</span></div>
<button class="button0" onclick="toggleBibtexzhuo2020communication()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuo2020communication()">Abstract</button>



    <a href="https://arxiv.org/pdf/1910.07703.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bzhuo2020communication" style="display: none;">
<pre>@inproceedings{zhuo2020communication,
  title = {Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls},
  author = {Zhuo, Jiacheng and Lei, Qi and Dimakis, Alex and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {1464--1474},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1910.07703.pdf}
}
</pre>
</div>

<div id="azhuo2020communication" style="display: none;">
<pre>Large-scale machine learning training suffers from two prior challenges, specifically for nuclear-norm constrained problems with distributed systems: the synchronization slowdown due to the straggling workers, and high communication costs. In this work, we propose an asynchronous Stochastic Frank Wolfe (SFW-asyn) method, which, for the first time, solves the two problems simultaneously, while successfully maintaining the same convergence rate as the vanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon EC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number of machines compared to the vanilla SFW.</pre>
</div>


<script>
function toggleAbstractCCzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuo2020communication(parameter) {
    var x= document.getElementById('Bzhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="liu2020high">Liu, L., Shen, Y., Li, T., &amp; Caramanis, C. (2020). High dimensional robust sparse regression. <i>International Conference on Artificial Intelligence and Statistics</i>, 411–421.</span></div>
<button class="button0" onclick="toggleBibtexliu2020high()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractliu2020high()">Abstract</button>



    <a href="https://arxiv.org/pdf/1805.11643.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bliu2020high" style="display: none;">
<pre>@inproceedings{liu2020high,
  title = {High dimensional robust sparse regression},
  author = {Liu, Liu and Shen, Yanyao and Li, Tianyang and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {411--421},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1805.11643.pdf}
}
</pre>
</div>

<div id="aliu2020high" style="display: none;">
<pre>We provide a novel – and to the best of our knowledge, the first – algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity, in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators: when the covariance matrix in sparse regression is identity, our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm which consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance. Also, it is orderwise more efficient computationally than the ellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.</pre>
</div>


<script>
function toggleAbstractCCliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexliu2020high(parameter) {
    var x= document.getElementById('Bliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2020converges">Kwon, J., &amp; Caramanis, C. (2020). EM converges for a mixture of many linear regressions. <i>International Conference on Artificial Intelligence and Statistics</i>, 1727–1736.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020converges()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020converges()">Abstract</button>



    <a href="https://arxiv.org/pdf/1905.12106.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020converges" style="display: none;">
<pre>@inproceedings{kwon2020converges,
  title = {EM converges for a mixture of many linear regressions},
  author = {Kwon, Jeongyeol and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  pages = {1727--1736},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1905.12106.pdf}
}
</pre>
</div>

<div id="akwon2020converges" style="display: none;">
<pre>We study the convergence of the Expectation-Maximization (EM) algorithm for mixtures of linear regressions with an arbitrary number k of components. We show that as long as signal-to-noise ratio (SNR) is \tildeΩ(k), well-initialized EM converges to the true regression parameters. Previous results for k ≥3 have only established local convergence for the noiseless setting, i.e., where SNR is infinitely large. Our results enlarge the scope to the environment with noises, and notably, we establish a statistical error rate that is independent of the norm (or pairwise distance) of the regression parameters. In particular, our results imply exact recovery as σ→0, in contrast to most previous local convergence results for EM, where the statistical error scaled with the norm of parameters. Standard moment-method approaches may be applied to guarantee we are in the region where our local convergence guarantees apply.</pre>
</div>


<script>
function toggleAbstractCCkwon2020converges(parameter) {
    var x= document.getElementById('akwon2020converges');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020converges(parameter) {
    var x= document.getElementById('Bkwon2020converges');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020converges(parameter) {
    var x= document.getElementById('akwon2020converges');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="lei2019primal">Lei, Q., Zhuo, J., Caramanis, C., Dhillon, I. S., &amp; Dimakis, A. G. (2019). Primal-dual block generalized frank-wolfe. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>32</i>, 13866–13875.</span></div>
<button class="button0" onclick="toggleBibtexlei2019primal()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractlei2019primal()">Abstract</button>



    <a href="https://arxiv.org/pdf/1906.02436.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Blei2019primal" style="display: none;">
<pre>@article{lei2019primal,
  title = {Primal-dual block generalized frank-wolfe},
  author = {Lei, Qi and Zhuo, Jiacheng and Caramanis, Constantine and Dhillon, Inderjit S and Dimakis, Alexandros G},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {32},
  pages = {13866--13875},
  year = {2019},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1906.02436.pdf}
}
</pre>
</div>

<div id="alei2019primal" style="display: none;">
<pre>We propose a variant of the Frank-Wolfe algorithm for solving a class of sparse/low-rank optimization problems. Our formulation includes Elastic Net, regularized SVMs and phase retrieval as special cases. The proposed Primal-Dual Block Frank-Wolfe algorithm reduces the per-iteration cost while maintaining linear convergence rate. The per iteration cost of our method depends on the structural complexity of the solution (i.e. sparsity/low-rank) instead of the ambient dimension. We empirically show that our algorithm outperforms the state-of-the-art methods on (multi-class) classification tasks.</pre>
</div>


<script>
function toggleAbstractCClei2019primal(parameter) {
    var x= document.getElementById('alei2019primal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexlei2019primal(parameter) {
    var x= document.getElementById('Blei2019primal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractlei2019primal(parameter) {
    var x= document.getElementById('alei2019primal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2019global">Kwon, J., Qian, W., Caramanis, C., Chen, Y., &amp; Davis, D. (2019). Global convergence of the em algorithm for mixtures of two component linear regression. <i>Conference on Learning Theory (COLT)</i>, 2055–2110.</span></div>
<button class="button0" onclick="toggleBibtexkwon2019global()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2019global()">Abstract</button>



    <a href="https://arxiv.org/pdf/1810.05752.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2019global" style="display: none;">
<pre>@inproceedings{kwon2019global,
  title = {Global convergence of the em algorithm for mixtures of two component linear regression},
  author = {Kwon, Jeongyeol and Qian, Wei and Caramanis, Constantine and Chen, Yudong and Davis, Damek},
  booktitle = {Conference on Learning Theory (COLT)},
  pages = {2055--2110},
  year = {2019},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1810.05752.pdf}
}
</pre>
</div>

<div id="akwon2019global" style="display: none;">
<pre>The Expectation-Maximization algorithm is perhaps the most broadly used algorithm for inference of latent variable problems. A theoretical understanding of its performance, however, largely remains lacking. Recent results established that EM enjoys global convergence for Gaussian Mixture Models. For Mixed Linear Regression, however, only local convergence results have been established, and those only for the high SNR regime. We show here that EM converges for mixed linear regression with two components (it is known that it may fail to converge for three or more), and moreover that this convergence holds for random initialization. Our analysis reveals that EM exhibits very different behavior in Mixed Linear Regression from its behavior in Gaussian Mixture Models, and hence our proofs require the development of several new ideas.</pre>
</div>


<script>
function toggleAbstractCCkwon2019global(parameter) {
    var x= document.getElementById('akwon2019global');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2019global(parameter) {
    var x= document.getElementById('Bkwon2019global');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2019global(parameter) {
    var x= document.getElementById('akwon2019global');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2019learning">Hoffmann, J., &amp; Caramanis, C. (2019). Learning graphs from noisy epidemic cascades. <i>Proceedings of the ACM on Measurement and Analysis of Computing Systems</i>, <i>3</i>(2), 1–34.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2019learning()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2019learning()">Abstract</button>



    <a href="https://arxiv.org/pdf/1903.02650.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2019learning" style="display: none;">
<pre>@article{hoffmann2019learning,
  title = {Learning graphs from noisy epidemic cascades},
  author = {Hoffmann, Jessica and Caramanis, Constantine},
  journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume = {3},
  number = {2},
  pages = {1--34},
  year = {2019},
  publisher = {ACM New York, NY, USA},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1903.02650.pdf}
}
</pre>
</div>

<div id="ahoffmann2019learning" style="display: none;">
<pre>We consider the problem of learning the weighted edges of a graph by observing the noisy times of infection for multiple epidemic cascades on this graph. Past work has considered this problem when the cascade information, i.e., infection times, are known exactly. Though the noisy setting is well motivated by many epidemic processes (e.g., most human epidemics), to the best of our knowledge, very little is known about when it is solvable. Previous work on the no-noise setting critically uses the ordering information. If noise can reverse this – a node’s reported (noisy) infection time comes after the reported infection time of some node it infected – then we are unable to see how previous results can be extended.
We therefore tackle two versions of the noisy setting: the limited-noise setting, where we know noisy times of infections, and the extreme-noise setting, in which we only know whether or not a node was infected. We provide a polynomial time algorithm for recovering the structure of bidirectional trees in the extreme-noise setting, and show our algorithm almost matches lower bounds established in the no-noise setting, and hence is optimal up to log-factors. We extend our results for general degree-bounded graphs, where again we show that our (poly-time) algorithm can recover the structure of the graph with optimal sample complexity. We also provide the first efficient algorithm to learn the weights of the bidirectional tree in the limited-noise setting. Finally, we give a polynomial time algorithm for learning the weights of general bounded-degree graphs in the limited-noise setting. This algorithm extends to general graphs (at the price of exponential running time), proving the problem is solvable in the general case. All our algorithms work for any noise distribution, without any restriction on the variance.</pre>
</div>


<script>
function toggleAbstractCChoffmann2019learning(parameter) {
    var x= document.getElementById('ahoffmann2019learning');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2019learning(parameter) {
    var x= document.getElementById('Bhoffmann2019learning');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2019learning(parameter) {
    var x= document.getElementById('ahoffmann2019learning');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="katiyar2019robust">Katiyar, A., Hoffmann, J., &amp; Caramanis, C. (2019). Robust estimation of tree structured Gaussian graphical models. <i>International Conference on Machine Learning</i>, 3292–3300.</span></div>
<button class="button0" onclick="toggleBibtexkatiyar2019robust()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkatiyar2019robust()">Abstract</button>



    <a href="https://arxiv.org/pdf/1901.08770.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkatiyar2019robust" style="display: none;">
<pre>@inproceedings{katiyar2019robust,
  title = {Robust estimation of tree structured Gaussian graphical models},
  author = {Katiyar, Ashish and Hoffmann, Jessica and Caramanis, Constantine},
  booktitle = {International Conference on Machine Learning},
  pages = {3292--3300},
  year = {2019},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1901.08770.pdf}
}
</pre>
</div>

<div id="akatiyar2019robust" style="display: none;">
<pre>Consider jointly Gaussian random variables whose conditional independence structure is specified by a graphical model. If we observe realizations of the variables, we can compute the covariance matrix, and it is well known that the support of the inverse covariance matrix corresponds to the edges of the graphical model. Instead, suppose we only have noisy observations. If the noise at each node is independent, we can compute the sum of the covariance matrix and an unknown diagonal. The inverse of this sum is (in general) dense. We ask: can the original independence structure be recovered? We address this question for tree structured graphical models. We prove that this problem is unidentifiable, but show that this unidentifiability is limited to a small class of candidate trees. We further present additional constraints under which the problem is identifiable. Finally, we provide an O(n^3) algorithm to find this equivalence class of trees.</pre>
</div>


<script>
function toggleAbstractCCkatiyar2019robust(parameter) {
    var x= document.getElementById('akatiyar2019robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkatiyar2019robust(parameter) {
    var x= document.getElementById('Bkatiyar2019robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkatiyar2019robust(parameter) {
    var x= document.getElementById('akatiyar2019robust');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2018cost">Hoffmann, J., &amp; Caramanis, C. (2018). The Cost of Uncertainty in Curing Epidemics. <i>Proceedings of the ACM on Measurement and Analysis of Computing Systems</i>, <i>2</i>(2), 1–33.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2018cost()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2018cost()">Abstract</button>



    <a href="https://arxiv.org/pdf/1711.00167.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2018cost" style="display: none;">
<pre>@article{hoffmann2018cost,
  title = {The Cost of Uncertainty in Curing Epidemics},
  author = {Hoffmann, Jessica and Caramanis, Constantine},
  journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume = {2},
  number = {2},
  pages = {1--33},
  year = {2018},
  publisher = {ACM New York, NY, USA},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1711.00167.pdf}
}
</pre>
</div>

<div id="ahoffmann2018cost" style="display: none;">
<pre>Motivated by the study of controlling (curing) epidemics, we consider the spread of an SI process on a known graph, where we have a limited budget to use to transition infected nodes back to the susceptible state (i.e., to cure nodes). Recent work has demonstrated that under perfect and instantaneous information (which nodes are/are not infected), the budget required for curing a graph precisely depends on a combinatorial property called the CutWidth. We show that this assumption is in fact necessary: even a minor degradation of perfect information, e.g., a diagnostic test that is 99% accurate, drastically alters the landscape. Infections that could previously be cured in sublinear time now may require exponential time, or orderwise larger budget to cure. The crux of the issue comes down to a tension not present in the full information case: if a node is suspected (but not certain) to be infected, do we risk wasting our budget to try to cure an uninfected node, or increase our certainty by longer observation, at the risk that the infection spreads further? Our results present fundamental, algorithm-independent bounds that tradeoff budget required vs. uncertainty.</pre>
</div>


<script>
function toggleAbstractCChoffmann2018cost(parameter) {
    var x= document.getElementById('ahoffmann2018cost');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2018cost(parameter) {
    var x= document.getElementById('Bhoffmann2018cost');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2018cost(parameter) {
    var x= document.getElementById('ahoffmann2018cost');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="sinno2018second">Sinno, Z., Caramanis, C., &amp; Bovik, A. (2018). Second order natural scene statistics model of blind image quality assessment. <i>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 1238–1242.</span></div>
<button class="button0" onclick="toggleBibtexsinno2018second()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractsinno2018second()">Abstract</button>



 


<div id="Bsinno2018second" style="display: none;">
<pre>@inproceedings{sinno2018second,
  title = {Second order natural scene statistics model of blind image quality assessment},
  author = {Sinno, Zeina and Caramanis, Constantine and Bovik, Alan},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {1238--1242},
  year = {2018},
  organization = {IEEE},
  group = {proceedings}
}
</pre>
</div>

<div id="asinno2018second" style="display: none;">
<pre>The univariate statistics of bandpass-filtered images provide powerful features that drive many successful image quality assessment (IQA) algorithms. Bivariate Natural Scene Statistics (NSS), which model the joint statistics of multiple bandpass image samples also provide potentially powerful features to assess the perceptual quality of images, by capturing both image and distortion correlations. Here, we make the first attempt to use bivariate NSS features to build a model of no-reference image quality prediction. We show that our bivariate model outperforms existing state of the art image quality predictors.</pre>
</div>


<script>
function toggleAbstractCCsinno2018second(parameter) {
    var x= document.getElementById('asinno2018second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexsinno2018second(parameter) {
    var x= document.getElementById('Bsinno2018second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractsinno2018second(parameter) {
    var x= document.getElementById('asinno2018second');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="li2018statistical">Li, T., Liu, L., Kyrillidis, A., &amp; Caramanis, C. (2018). Statistical inference using SGD. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>32</i>(1).</span></div>
<button class="button0" onclick="toggleBibtexli2018statistical()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractli2018statistical()">Abstract</button>



    <a href="https://arxiv.org/pdf/1705.07477.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bli2018statistical" style="display: none;">
<pre>@inproceedings{li2018statistical,
  title = {Statistical inference using SGD},
  author = {Li, Tianyang and Liu, Liu and Kyrillidis, Anastasios and Caramanis, Constantine},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  year = {2018},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1705.07477.pdf}
}
</pre>
</div>

<div id="ali2018statistical" style="display: none;">
<pre>We present a novel method for frequentist statistical inference in M-estimation problems, based on stochastic gradient descent (SGD) with a fixed step size: we demonstrate that the average of such SGD sequences can be used for statistical inference, after proper scaling. An intuitive analysis using the Ornstein-Uhlenbeck process suggests that such averages are asymptotically normal. From a practical perspective, our SGD-based inference procedure is a first order method, and is well-suited for large scale problems. To show its merits, we apply it to both synthetic and real datasets, and demonstrate that its accuracy is comparable to classical statistical methods, while requiring potentially far less computation.</pre>
</div>


<script>
function toggleAbstractCCli2018statistical(parameter) {
    var x= document.getElementById('ali2018statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexli2018statistical(parameter) {
    var x= document.getElementById('Bli2018statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractli2018statistical(parameter) {
    var x= document.getElementById('ali2018statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="park2017non">Park, D., Kyrillidis, A., Carmanis, C., &amp; Sanghavi, S. (2017). Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach. <i>Artificial Intelligence and Statistics</i>, 65–74.</span></div>
<button class="button0" onclick="toggleBibtexpark2017non()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpark2017non()">Abstract</button>



    <a href="https://arxiv.org/pdf/1609.03240.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bpark2017non" style="display: none;">
<pre>@inproceedings{park2017non,
  title = {Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach},
  author = {Park, Dohyung and Kyrillidis, Anastasios and Carmanis, Constantine and Sanghavi, Sujay},
  booktitle = {Artificial Intelligence and Statistics},
  pages = {65--74},
  year = {2017},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1609.03240.pdf}
}
</pre>
</div>

<div id="apark2017non" style="display: none;">
<pre>We consider the non-square matrix sensing problem, under restricted isometry property (RIP) assumptions. We focus on the non-convex formulation, where any rank-r matrix X ∈\mathbbR^m \times n is represented as UB^⊤, where U ∈\mathbbR^m \times r and V ∈\mathbbR^n \times r. In this paper, we complement recent findings on the non-convex geometry of the analogous PSD setting [5], and show that matrix factorization does not introduce any spurious local minima, under RIP.</pre>
</div>


<script>
function toggleAbstractCCpark2017non(parameter) {
    var x= document.getElementById('apark2017non');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpark2017non(parameter) {
    var x= document.getElementById('Bpark2017non');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpark2017non(parameter) {
    var x= document.getElementById('apark2017non');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="li2017minimax">Li, T., Yi, X., Carmanis, C., &amp; Ravikumar, P. (2017). Minimax gaussian classification &amp; clustering. <i>Artificial Intelligence and Statistics</i>, 1–9.</span></div>
<button class="button0" onclick="toggleBibtexli2017minimax()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractli2017minimax()">Abstract</button>



 


<div id="Bli2017minimax" style="display: none;">
<pre>@inproceedings{li2017minimax,
  title = {Minimax gaussian classification \&amp; clustering},
  author = {Li, Tianyang and Yi, Xinyang and Carmanis, Constantine and Ravikumar, Pradeep},
  booktitle = {Artificial Intelligence and Statistics},
  pages = {1--9},
  year = {2017},
  organization = {PMLR},
  group = {proceedings}
}
</pre>
</div>

<div id="ali2017minimax" style="display: none;">
<pre>We present minimax bounds for classification and clustering error in the setting where covariates are drawn from a mixture of two isotropic Gaussian distributions. Here, we define clustering error in a discriminative fashion, demonstrating fundamental connections between classification (supervised) and clustering (unsupervised). For both classification and clustering, our lower bounds show that without enough samples, the best any classifier or clustering rule can do is close to random guessing. For classification, as part of our upper bound analysis, we show that Fisher?s linear discriminant achieves a fast minimax rate ?(1/n) with enough samples n. For clustering, as part of our upper bound analysis, we show that a clustering rule constructed using principal component analysis achieves the minimax rate with enough samples. We also provide lower and upper bounds for the high-dimensional sparse setting where the dimensionality of the covariates p is potentially larger than the number of samples n, but where the difference between the Gaussian means is sparse.</pre>
</div>


<script>
function toggleAbstractCCli2017minimax(parameter) {
    var x= document.getElementById('ali2017minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexli2017minimax(parameter) {
    var x= document.getElementById('Bli2017minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractli2017minimax(parameter) {
    var x= document.getElementById('ali2017minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="wang2016polygp">Wang, Y., Caramanis, C., &amp; Orshansky, M. (2016). PolyGP: Improving GP-based analog optimization through accurate high-order monomials and semidefinite relaxation. <i>2016 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</i>, 1423–1428.</span></div>
<button class="button0" onclick="toggleBibtexwang2016polygp()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractwang2016polygp()">Abstract</button>



 


<div id="Bwang2016polygp" style="display: none;">
<pre>@inproceedings{wang2016polygp,
  title = {PolyGP: Improving GP-based analog optimization through accurate high-order monomials and semidefinite relaxation},
  author = {Wang, Ye and Caramanis, Constantine and Orshansky, Michael},
  booktitle = {2016 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition (DATE)},
  pages = {1423--1428},
  year = {2016},
  organization = {IEEE},
  group = {proceedings}
}
</pre>
</div>

<div id="awang2016polygp" style="display: none;">
<pre>Geometric programming (GP) is popular for use in equation-based optimization of analog circuits thanks to GP-compatible analog performance functions, and its convexity, hence computational tractability. The main challenge in using GP, and thus a roadblock to wider use and adoption, is the mismatch between what GP can accurately fit, and the behavior of many common device/circuit functions. In this paper, we leverage recent tools from sums-of-squares, moment optimization, and semidefinite optimization (SDP), in order to present a novel and powerful extension to address the monomial inaccuracy: fitting device models as higher-order monomials, defined as the exponential functions of polynomials in the logarithmic variables. By the introduction of high-order monomials, the original GP problems become polynomial geometric programming (PolyGP) problems with non-linear and non-convex objective and constraints. Our PolyGP framework allows significant improvements in model accuracy when symbolic performance functions in terms of device models are present. Via SDP-relaxations inspired by polynomial optimization (POP), we can obtain efficient near-optimal global solutions to the resulting PolyGP. Experimental results through established circuits show that compared to GP, we are able to reduce fitting error of device models to 3.5% from 10.5% on average. Hence, the fitting error of performance functions decrease from 12% of GP and 9% of POP, to 3% accordingly. This translates to the ability of identifying superior solution points and the dramatic decrease of constraint violation in contrast to both GP and POP.</pre>
</div>


<script>
function toggleAbstractCCwang2016polygp(parameter) {
    var x= document.getElementById('awang2016polygp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexwang2016polygp(parameter) {
    var x= document.getElementById('Bwang2016polygp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractwang2016polygp(parameter) {
    var x= document.getElementById('awang2016polygp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="yi2019more">Yi, X., Wang, Z., Yang, Z., Caramanis, C., &amp; Liu, H. (2016). More supervision, less computation: statistical-computational tradeoffs in weakly supervised learning. <i>ArXiv Preprint ArXiv:1907.06257</i>.</span></div>
<button class="button0" onclick="toggleBibtexyi2019more()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2019more()">Abstract</button>



    <a href="https://arxiv.org/abs/1907.06257.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2019more" style="display: none;">
<pre>@article{yi2019more,
  title = {More supervision, less computation: statistical-computational tradeoffs in weakly supervised learning},
  author = {Yi, Xinyang and Wang, Zhaoran and Yang, Zhuoran and Caramanis, Constantine and Liu, Han},
  journal = {arXiv preprint arXiv:1907.06257},
  year = {2016},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1907.06257.pdf}
}
</pre>
</div>

<div id="ayi2019more" style="display: none;">
<pre>We consider the weakly supervised binary classification problem where the labels are randomly flipped with probability 1 - α. Although there exist numerous algorithms for this problem, it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision, which is quantified by α. In this paper, we characterize the effect of αby establishing the information-theoretic and computational boundaries, namely, the minimax-optimal statistical accuracy that can be achieved by all algorithms, and polynomial-time algorithms under an oracle computational model. For small α, our result shows a gap between these two boundaries, which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision. Interestingly, we also show that this gap narrows as ? increases. In other words, having more supervision, i.e., more correct labels, not only improves the optimal statistical accuracy as expected, but also enhances the computational efficiency for achieving such accuracy.</pre>
</div>


<script>
function toggleAbstractCCyi2019more(parameter) {
    var x= document.getElementById('ayi2019more');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2019more(parameter) {
    var x= document.getElementById('Byi2019more');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2019more(parameter) {
    var x= document.getElementById('ayi2019more');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="yi2016fast">Yi, X., Park, D., Chen, Y., &amp; Caramanis, C. (2016). Fast algorithms for robust PCA via gradient descent. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>.</span></div>
<button class="button0" onclick="toggleBibtexyi2016fast()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2016fast()">Abstract</button>



    <a href="https://arxiv.org/pdf/1605.07784.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2016fast" style="display: none;">
<pre>@article{yi2016fast,
  title = {Fast algorithms for robust PCA via gradient descent},
  author = {Yi, Xinyang and Park, Dohyung and Chen, Yudong and Caramanis, Constantine},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2016},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1605.07784.pdf}
}
</pre>
</div>

<div id="ayi2016fast" style="display: none;">
<pre>We consider the problem of Robust PCA in the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with r denoting rank and d dimension, we reduce the complexity from O(r^2d^2log(1/ε)) to O(rd^2log(1/ε)) – a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than O(r^4d\log d\log(1/ε)). Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where r is small compared to d, it also allows for near-linear-in-d run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations.</pre>
</div>


<script>
function toggleAbstractCCyi2016fast(parameter) {
    var x= document.getElementById('ayi2016fast');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2016fast(parameter) {
    var x= document.getElementById('Byi2016fast');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2016fast(parameter) {
    var x= document.getElementById('ayi2016fast');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="wang2016exploiting">Wang, Y., Caramanis, C., &amp; Orshansky, M. (2016). Exploiting randomness in sketching for efficient hardware implementation of machine learning applications. <i>Proceedings of the 35th International Conference on Computer-Aided Design</i>, 1–8.</span></div>
<button class="button0" onclick="toggleBibtexwang2016exploiting()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractwang2016exploiting()">Abstract</button>



 


<div id="Bwang2016exploiting" style="display: none;">
<pre>@inproceedings{wang2016exploiting,
  title = {Exploiting randomness in sketching for efficient hardware implementation of machine learning applications},
  author = {Wang, Ye and Caramanis, Constantine and Orshansky, Michael},
  booktitle = {Proceedings of the 35th International Conference on Computer-Aided Design},
  pages = {1--8},
  year = {2016},
  group = {proceedings}
}
</pre>
</div>

<div id="awang2016exploiting" style="display: none;">
<pre>Energy-efficient processing of large matrices for big-data applications using hardware acceleration is an intense area of research. Sketching of large matrices into their lower-dimensional representations is an effective strategy. For the first time, this paper develops a highly energy-efficient hardware implementation of a class of sketching methods based on random projections, known as Johnson Lindenstrauss (JL) transform. Crucially, we show how the randomness inherent in the projection matrix can be exploited to create highly efficient fixed-point arithmetic realizations of several machine-learning applications. Specifically, the transform’s random matrices have two key properties that allow for significant energy gains in hardware implementations. The first is the smoothing property that allows us to drastically reduce operand bit-width in computation of the JL transform itself. The second is the randomizing property that allows bit-width reduction in subsequent machine-learning applications. Further, we identify a random matrix construction method that exploits the special sparsity structure to result in the most hardware-efficient realization and implement the highly optimized transform on an FPGA. Experimental results on (1) the k-nearest neighbor (KNN) classification and (2) the principal component analysis (PCA) show that with the same bit-width the proposed flow utilizing random projection achieves an up to 7x improvement in both latency and energy. Furthermore, by exploiting the smoothing and randomizing properties we are able to use a 1-bit instead of a 4-bit multiplier within KNN, which results in additional 50% and 6% improvement in area and energy respectively. The proposed I/O streaming strategy along with the hardware-efficient JL algorithm identified by us is able to achieve a 50% runtime reduction, a 17% area reduction in the stage of random projection compared to a standard design.</pre>
</div>


<script>
function toggleAbstractCCwang2016exploiting(parameter) {
    var x= document.getElementById('awang2016exploiting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexwang2016exploiting(parameter) {
    var x= document.getElementById('Bwang2016exploiting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractwang2016exploiting(parameter) {
    var x= document.getElementById('awang2016exploiting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="wang2015novel">Wang, Y., Li, M., Yi, X., Song, Z., Orshansky, M., &amp; Caramanis, C. (2015). Novel power grid reduction method based on l1 regularization. <i>Proceedings of the 52nd Annual Design Automation Conference</i>, 1–6.</span></div>
<button class="button0" onclick="toggleBibtexwang2015novel()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractwang2015novel()">Abstract</button>



 


<div id="Bwang2015novel" style="display: none;">
<pre>@inproceedings{wang2015novel,
  title = {Novel power grid reduction method based on l1 regularization},
  author = {Wang, Ye and Li, Meng and Yi, Xinyang and Song, Zhao and Orshansky, Michael and Caramanis, Constantine},
  booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
  pages = {1--6},
  year = {2015},
  group = {proceedings}
}
</pre>
</div>

<div id="awang2015novel" style="display: none;">
<pre>Model order reduction exploiting the spectral properties of the admittance matrix, known as the graph Laplacian, to control the approximation accuracy is a promising new class of approaches to power grid analysis. In this paper we introduce a method that allows a dramatic increase in the resulting graph sparsity and can handle large dense input graphs. The method is based on the observation that the information about the realistic ranges of port currents can be used to significantly improve the resulting graph sparsity. In practice, port currents cannot vary unboundedly and the estimates of peak currents are often available early in the design cycle. However, the existing methods including the sampling-based spectral sparsification approach [11] cannot utilize this information.

We propose a novel framework of graph Sparsification by L1 regularization on Laplacians (SparseLL) to exploit the available range information to achieve a higher degree of sparsity and better approximation quality. By formulating the power grid reduction as a sparsity-inducing optimization problem, we leverage the recent progress in stochastic approximation and develop a stochastic gradient descent algorithm as an efficient solution.

Using established benchmarks for experiments, we demonstrate that SparseLL can achieve an up to 10X edge sparsity improvement compared to the spectral sparsification approach assuming the full range of currents, with an up to 10X accuracy improvement. The running time of our algorithm also scales quite favorably due to the low complexity and fast convergence, which leads us to believe that our algorithm is highly suitable for large-scale dense problems.</pre>
</div>


<script>
function toggleAbstractCCwang2015novel(parameter) {
    var x= document.getElementById('awang2015novel');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexwang2015novel(parameter) {
    var x= document.getElementById('Bwang2015novel');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractwang2015novel(parameter) {
    var x= document.getElementById('awang2015novel');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="milling2015local">Milling, C., Caramanis, C., Mannor, S., &amp; Shakkottai, S. (2015). Local detection of infections in heterogeneous networks. <i>2015 IEEE Conference on Computer Communications (INFOCOM)</i>, 1517–1525.</span></div>
<button class="button0" onclick="toggleBibtexmilling2015local()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractmilling2015local()">Abstract</button>



 


<div id="Bmilling2015local" style="display: none;">
<pre>@inproceedings{milling2015local,
  title = {Local detection of infections in heterogeneous networks},
  author = {Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay},
  booktitle = {2015 IEEE Conference on Computer Communications (INFOCOM)},
  pages = {1517--1525},
  year = {2015},
  organization = {IEEE},
  group = {proceedings}
}
</pre>
</div>

<div id="amilling2015local" style="display: none;">
<pre>In many networks the operator is faced with nodes that report a potentially important phenomenon such as failures, illnesses, and viruses. The operator is faced with the question: Is it spreading over the network, or simply occurring at random? We seek to answer this question from highly noisy and incomplete data, where at a single point in time we are given a possibly very noisy subset of the infected population (including false positives and negatives). While previous work has focused on uniform spreading rates for the infection, heterogeneous graphs with unequal edge weights are more faithful models of reality. Critically, the network structure may not be fully known and modeling epidemic spread on unknown graphs relies on non-homogeneous edge (spreading) weights. Such heterogeneous graphs pose considerable challenges, requiring both algorithmic and analytical development. We develop an algorithm that can distinguish between a spreading phenomenon and a randomly occurring phenomenon while using only local information and not knowing the complete network topology and the weights. Further, we show that this algorithm can succeed even in the presence of noise, false positives and unknown graph edges.</pre>
</div>


<script>
function toggleAbstractCCmilling2015local(parameter) {
    var x= document.getElementById('amilling2015local');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexmilling2015local(parameter) {
    var x= document.getElementById('Bmilling2015local');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractmilling2015local(parameter) {
    var x= document.getElementById('amilling2015local');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="meirom2015localized">Meirom, E. A., Milling, C., Caramanis, C., Mannor, S., Shakkottai, S., &amp; Orda, A. (2015). Localized epidemic detection in networks with overwhelming noise. <i>ACM SIGMETRICS Performance Evaluation Review</i>, <i>43</i>(1), 441–442.</span></div>
<button class="button0" onclick="toggleBibtexmeirom2015localized()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractmeirom2015localized()">Abstract</button>



    <a href="https://arxiv.org/abs/1402.1263.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bmeirom2015localized" style="display: none;">
<pre>@article{meirom2015localized,
  title = {Localized epidemic detection in networks with overwhelming noise},
  author = {Meirom, Eli A and Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay and Orda, Ariel},
  journal = {ACM SIGMETRICS Performance Evaluation Review},
  volume = {43},
  number = {1},
  pages = {441--442},
  year = {2015},
  publisher = {ACM New York, NY, USA},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1402.1263.pdf}
}
</pre>
</div>

<div id="ameirom2015localized" style="display: none;">
<pre>We consider the problem of detecting an epidemic in a population where individual diagnoses are extremely noisy. The motivation for this problem is the plethora of examples (influenza strains in humans, or computer viruses in smartphones, etc.) where reliable diagnoses are scarce, but noisy data plentiful. In flu/phone-viruses, exceedingly few infected people/phones are professionally diagnosed (only a small fraction go to a doctor) but less reliable secondary signatures (e.g., people staying home, or greater-than-typical upload activity) are more readily available. These secondary data are often plagued by unreliability: many people with the flu do not stay home, and many people that stay home do not have the flu. This paper identifies the precise regime where knowledge of the contact network enables finding the needle in the haystack: we provide a distributed, efficient and robust algorithm that can correctly identify the existence of a spreading epidemic from highly unreliable local data. Our algorithm requires only local-neighbor knowledge of this graph, and in a broad array of settings that we describe, succeeds even when false negatives and false positives make up an overwhelming fraction of the data available. Our results show it succeeds in the presence of partial information about the contact network, and also when there is not a single "patient zero", but rather many (hundreds, in our examples) of initial patient-zeroes, spread across the graph.</pre>
</div>


<script>
function toggleAbstractCCmeirom2015localized(parameter) {
    var x= document.getElementById('ameirom2015localized');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexmeirom2015localized(parameter) {
    var x= document.getElementById('Bmeirom2015localized');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractmeirom2015localized(parameter) {
    var x= document.getElementById('ameirom2015localized');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="yi2015optimal">Yi, X., Wang, Z., Caramanis, C., &amp; Liu, H. (2015). Optimal linear estimation under unknown nonlinear transform. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>28</i>, 1549.</span></div>
<button class="button0" onclick="toggleBibtexyi2015optimal()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2015optimal()">Abstract</button>



    <a href="https://arxiv.org/abs/1505.03257.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2015optimal" style="display: none;">
<pre>@article{yi2015optimal,
  title = {Optimal linear estimation under unknown nonlinear transform},
  author = {Yi, Xinyang and Wang, Zhaoran and Caramanis, Constantine and Liu, Han},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {28},
  pages = {1549},
  year = {2015},
  publisher = {NIH Public Access},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1505.03257.pdf}
}
</pre>
</div>

<div id="ayi2015optimal" style="display: none;">
<pre>Linear regression studies the problem of estimating a model parameter β^* ∈\mathbbR^p, from n observations (y_i,x_)_i=1^n from linear model y_i = ⟨x_i,β^* ⟩+ \epsilon_i. We consider a significant generalization in which the relationship between ⟨x_i,β^* ⟩and y_i is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. This model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. We propose a novel spectral-based estimation procedure and show that we can recover β^*$ in settings (i.e., classes of link function f) where previous algorithms fail. In general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between y_i and ⟨x_i ,β^* ⟩. We also consider the high dimensional setting where β^* is sparse ,and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p?n. For a broad class of link functions between ⟨x_i ,β^* ⟩and y_i, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.</pre>
</div>


<script>
function toggleAbstractCCyi2015optimal(parameter) {
    var x= document.getElementById('ayi2015optimal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2015optimal(parameter) {
    var x= document.getElementById('Byi2015optimal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2015optimal(parameter) {
    var x= document.getElementById('ayi2015optimal');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="yi2015regularized">Yi, X., &amp; Caramanis, C. (2015). Regularized em algorithms: A unified framework and statistical guarantees. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>28</i>, 1549.</span></div>
<button class="button0" onclick="toggleBibtexyi2015regularized()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2015regularized()">Abstract</button>



    <a href="https://arxiv.org/abs/1511.08551.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2015regularized" style="display: none;">
<pre>@article{yi2015regularized,
  title = {Regularized em algorithms: A unified framework and statistical guarantees},
  author = {Yi, Xinyang and Caramanis, Constantine},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {28},
  pages = {1549},
  year = {2015},
  publisher = {NIH Public Access},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1511.08551.pdf}
}
</pre>
</div>

<div id="ayi2015regularized" style="display: none;">
<pre>Latent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M-step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M-step using the state-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.</pre>
</div>


<script>
function toggleAbstractCCyi2015regularized(parameter) {
    var x= document.getElementById('ayi2015regularized');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2015regularized(parameter) {
    var x= document.getElementById('Byi2015regularized');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2015regularized(parameter) {
    var x= document.getElementById('ayi2015regularized');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="yi2015binary">Yi, X., Caramanis, C., &amp; Price, E. (2015). Binary embedding: Fundamental limits and fast algorithm. <i>International Conference on Machine Learning (ICML)</i>, 2162–2170.</span></div>
<button class="button0" onclick="toggleBibtexyi2015binary()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2015binary()">Abstract</button>



    <a href="https://arxiv.org/abs/1502.05746.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2015binary" style="display: none;">
<pre>@inproceedings{yi2015binary,
  title = {Binary embedding: Fundamental limits and fast algorithm},
  author = {Yi, Xinyang and Caramanis, Constantine and Price, Eric},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = {2162--2170},
  year = {2015},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1502.05746.pdf}
}
</pre>
</div>

<div id="ayi2015binary" style="display: none;">
<pre>Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary N distinct points in \mathbbS^p-1, our goal is to encode each point using m-dimensional binary strings such that we can reconstruct their geodesic distance up to δuniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time O(mp). We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires m = Ω(\log N / δ^2) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) [DELETED, see comment]; (3) we also provide an analytic result about embedding a general set of points K ⊆\mathbbS^p-1 with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets.</pre>
</div>


<script>
function toggleAbstractCCyi2015binary(parameter) {
    var x= document.getElementById('ayi2015binary');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2015binary(parameter) {
    var x= document.getElementById('Byi2015binary');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2015binary(parameter) {
    var x= document.getElementById('ayi2015binary');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="park2014greedy">Park, D., Caramanis, C., &amp; Sanghavi, S. (2014). Greedy subspace clustering. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>27</i>, 2753–2761. https://proceedings.neurips.cc/paper/2014/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf</span></div>
<button class="button0" onclick="toggleBibtexpark2014greedy()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpark2014greedy()">Abstract</button>



    <a href="https://arxiv.org/abs/1410.8864.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bpark2014greedy" style="display: none;">
<pre>@article{park2014greedy,
  title = {Greedy subspace clustering},
  author = {Park, Dohyung and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {27},
  pages = {2753--2761},
  year = {2014},
  publisher = {NIH Public Access},
  url = {https://proceedings.neurips.cc/paper/2014/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1410.8864.pdf}
}
</pre>
</div>

<div id="apark2014greedy" style="display: none;">
<pre>We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity between subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.</pre>
</div>


<script>
function toggleAbstractCCpark2014greedy(parameter) {
    var x= document.getElementById('apark2014greedy');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpark2014greedy(parameter) {
    var x= document.getElementById('Bpark2014greedy');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpark2014greedy(parameter) {
    var x= document.getElementById('apark2014greedy');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="papailiopoulos2014finding">Papailiopoulos, D., Mitliagkas, I., Dimakis, A., &amp; Caramanis, C. (2014). Finding Dense Subgraphs via Low-Rank Bilinear Optimization. In E. P. Xing &amp; T. Jebara (Eds.), <i>Proceedings of the 31st International Conference on Machine Learning (ICML)</i> (Vol. 32, Number 2, pp. 1890–1898). PMLR.</span></div>
<button class="button0" onclick="toggleBibtexpapailiopoulos2014finding()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpapailiopoulos2014finding()">Abstract</button>



 


<div id="Bpapailiopoulos2014finding" style="display: none;">
<pre>@inproceedings{papailiopoulos2014finding,
  title = {Finding Dense Subgraphs via Low-Rank Bilinear Optimization},
  author = {Papailiopoulos, Dimitris and Mitliagkas, Ioannis and Dimakis, Alexandros and Caramanis, Constantine},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
  pages = {1890--1898},
  year = {2014},
  editor = {Xing, Eric P. and Jebara, Tony},
  volume = {32},
  number = {2},
  publisher = {PMLR},
  group = {proceedings}
}
</pre>
</div>

<div id="apapailiopoulos2014finding" style="display: none;">
<pre>Given a graph, the Densest k-Subgraph (DkS) problem asks for the subgraph on k vertices that contains the largest number of edges. In this work, we develop a novel algorithm for DkS that searches a low-dimensional space for provably good solutions. We obtain provable performance bounds that depend on the graph spectrum. One of our results is that if there exists a k-subgraph that contains a constant fraction of all the edges, we can approximate DkS within a factor arbitrarily close to two in polynomial time. Our algorithm runs in nearly linear time, under spectral assumptions satisfied by most graphs found in applications. Moreover, it is highly scalable and parallelizable. We demonstrate this by implementing it in MapReduce and executing numerous experiments on massive real-world graphs that have up to billions of edges. We empirically show that our algorithm can find subgraphs of significantly higher density compared to the previous state of the art.</pre>
</div>


<script>
function toggleAbstractCCpapailiopoulos2014finding(parameter) {
    var x= document.getElementById('apapailiopoulos2014finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpapailiopoulos2014finding(parameter) {
    var x= document.getElementById('Bpapailiopoulos2014finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpapailiopoulos2014finding(parameter) {
    var x= document.getElementById('apapailiopoulos2014finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="yi2014alternating">Yi, X., Caramanis, C., &amp; Sanghavi, S. (2014). Alternating minimization for mixed linear regression. <i>International Conference on Machine Learning</i>, 613–621.</span></div>
<button class="button0" onclick="toggleBibtexyi2014alternating()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractyi2014alternating()">Abstract</button>



    <a href="https://arxiv.org/abs/1310.3745.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Byi2014alternating" style="display: none;">
<pre>@inproceedings{yi2014alternating,
  title = {Alternating minimization for mixed linear regression},
  author = {Yi, Xinyang and Caramanis, Constantine and Sanghavi, Sujay},
  booktitle = {International Conference on Machine Learning},
  pages = {613--621},
  year = {2014},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1310.3745.pdf}
}
</pre>
</div>

<div id="ayi2014alternating" style="display: none;">
<pre>Mixed linear regression involves the recovery of two (or more) unknown vectors from unlabeled linear measurements; that is, where each sample comes from exactly one of the vectors, but we do not know which one. It is a classic problem, and the natural and empirically most popular approach to its solution has been the EM algorithm. As in other settings, this is prone to bad local minima; however, each iteration is very fast (alternating between guessing labels, and solving with those labels).
In this paper we provide a new initialization procedure for EM, based on finding the leading two eigenvectors of an appropriate matrix. We then show that with this, a re-sampled version of the EM algorithm provably converges to the correct vectors, under natural assumptions on the sampling distribution, and with nearly optimal (unimprovable) sample complexity. This provides not only the first characterization of EM’s performance, but also much lower sample complexity as compared to both standard (randomly initialized) EM, and other methods for this problem.</pre>
</div>


<script>
function toggleAbstractCCyi2014alternating(parameter) {
    var x= document.getElementById('ayi2014alternating');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexyi2014alternating(parameter) {
    var x= document.getElementById('Byi2014alternating');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractyi2014alternating(parameter) {
    var x= document.getElementById('ayi2014alternating');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="chen2014convex">Chen, Y., Yi, X., &amp; Caramanis, C. (2014). A convex formulation for mixed regression with two components: Minimax optimal rates. <i>Conference on Learning Theory (COLT)</i>, 560–604.</span></div>
<button class="button0" onclick="toggleBibtexchen2014convex()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractchen2014convex()">Abstract</button>



    <a href="https://arxiv.org/abs/1312.7006"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bchen2014convex" style="display: none;">
<pre>@inproceedings{chen2014convex,
  title = {A convex formulation for mixed regression with two components: Minimax optimal rates},
  author = {Chen, Yudong and Yi, Xinyang and Caramanis, Constantine},
  booktitle = {Conference on Learning Theory (COLT)},
  pages = {560--604},
  year = {2014},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/abs/1312.7006}
}
</pre>
</div>

<div id="achen2014convex" style="display: none;">
<pre>We consider the mixed regression problem with two components, under adversarial and stochastic noise. We give a convex optimization formulation that provably recovers the true solution, and provide upper bounds on the recovery errors for both arbitrary noise and stochastic noise settings. We also give matching minimax lower bounds (up to log factors), showing that under certain assumptions, our algorithm is information-theoretically optimal. Our results represent the first tractable algorithm guaranteeing successful recovery with tight bounds on recovery errors and sample complexity.</pre>
</div>


<script>
function toggleAbstractCCchen2014convex(parameter) {
    var x= document.getElementById('achen2014convex');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexchen2014convex(parameter) {
    var x= document.getElementById('Bchen2014convex');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractchen2014convex(parameter) {
    var x= document.getElementById('achen2014convex');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="wang2014enabling">Wang, Y., Orshansky, M., &amp; Caramanis, C. (2014). Enabling efficient analog synthesis by coupling sparse regression and polynomial optimization. <i>Proceedings of the 51st Annual Design Automation Conference</i>, 1–6.</span></div>
<button class="button0" onclick="toggleBibtexwang2014enabling()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractwang2014enabling()">Abstract</button>



 


<div id="Bwang2014enabling" style="display: none;">
<pre>@inproceedings{wang2014enabling,
  title = {Enabling efficient analog synthesis by coupling sparse regression and polynomial optimization},
  author = {Wang, Ye and Orshansky, Michael and Caramanis, Constantine},
  booktitle = {Proceedings of the 51st Annual Design Automation Conference},
  pages = {1--6},
  year = {2014},
  group = {proceedings}
}
</pre>
</div>

<div id="awang2014enabling" style="display: none;">
<pre>The challenge of equation-based analog synthesis comes from its dual nature: functions producing good least-square fits to SPICE-generated data are non-convex, hence not amenable to efficient optimization. In this paper, we leverage recent progress on Semidefinite Programming (SDP) relaxations of polynomial (non-convex) optimization. Using a general polynomial allows for much more accurate fitting of SPICE data compared to the more restricted functional forms. Recent SDP techniques for convex relaxations of polynomial optimizations are powerful but alone still insufficient: even for small problems, the resulting relaxations are prohibitively high dimensional.
  
We harness these new polynomial tools and realize their promise by introducing a novel regression technique that fits non-convex polynomials with a special sparsity structure. We show that the coupled sparse fitting and optimization (CSFO) flow that we introduce allows us to find accurate high-order polynomials while keeping the resulting optimization tractable.


Using established circuits for optimization experiments, we demonstrate that by handling higher-order polynomials we reduce fitting error to 3.6% from 10%, on average. This translates into a dramatic increase in the rate of constraint satisfaction: for a 1% violation threshold, the success rate is increased from 0% to 78%.
</pre>
</div>


<script>
function toggleAbstractCCwang2014enabling(parameter) {
    var x= document.getElementById('awang2014enabling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexwang2014enabling(parameter) {
    var x= document.getElementById('Bwang2014enabling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractwang2014enabling(parameter) {
    var x= document.getElementById('awang2014enabling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="mitliagkas2013memory">Mitliagkas, I., Caramanis, C., &amp; Jain, P. (2013). Memory Limited, Streaming PCA. <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, <i>26</i>. https://papers.nips.cc/paper/2013/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf</span></div>
<button class="button0" onclick="toggleBibtexmitliagkas2013memory()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractmitliagkas2013memory()">Abstract</button>



    <a href="https://arxiv.org/1307.0032.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bmitliagkas2013memory" style="display: none;">
<pre>@inproceedings{mitliagkas2013memory,
  title = {Memory Limited, Streaming PCA},
  author = {Mitliagkas, Ioannis and Caramanis, Constantine and Jain, Prateek},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {26},
  year = {2013},
  publisher = {NIH Public Access},
  url = {https://papers.nips.cc/paper/2013/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf},
  group = {proceedings},
  arxiv = {https://arxiv.org/1307.0032.pdf}
}
</pre>
</div>

<div id="amitliagkas2013memory" style="display: none;">
<pre>We consider streaming, one-pass principal component analysis (PCA), in the high-dimensional regime, with limited memory. Here, p-dimensional samples are presented sequentially, and the goal is to produce the k-dimensional subspace that best approximates these points. Standard algorithms require O(p^2) memory; meanwhile no algorithm can do better than O(kp) memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the \em spiked covariance model, where p-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, n, scales proportionally with the dimension, p. Yet, all algorithms that provably achieve this, have memory complexity O(p^2). Meanwhile, algorithms with memory-complexity O(kp) do not have provable bounds on sample complexity comparable to p. We present an algorithm that achieves both: it uses O(kp) memory (meaning storage of any kind) and is able to compute the k-dimensional spike with O(p\logp) sample-complexity – the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.</pre>
</div>


<script>
function toggleAbstractCCmitliagkas2013memory(parameter) {
    var x= document.getElementById('amitliagkas2013memory');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexmitliagkas2013memory(parameter) {
    var x= document.getElementById('Bmitliagkas2013memory');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractmitliagkas2013memory(parameter) {
    var x= document.getElementById('amitliagkas2013memory');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>

<h2 id="journal-papers">Journal Papers</h2>

<ol class="bibliography"><li><div class="text-justify"><span id="kyrillidis2018provable">Kyrillidis, A., Kalev, A., Park, D., Bhojanapalli, S., Caramanis, C., &amp; Sanghavi, S. (2018). Provable compressed sensing quantum state tomography via non-convex methods. <i>Npj Quantum Information</i>, <i>4</i>(1), 1–7.</span></div>
<button class="button0" onclick="toggleBibtexkyrillidis2018provable()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkyrillidis2018provable()">Abstract</button>



    <a href="https://arxiv.org/pdf/1711.02524.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkyrillidis2018provable" style="display: none;">
<pre>@article{kyrillidis2018provable,
  title = {Provable compressed sensing quantum state tomography via non-convex methods},
  author = {Kyrillidis, Anastasios and Kalev, Amir and Park, Dohyung and Bhojanapalli, Srinadh and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {npj Quantum Information},
  volume = {4},
  number = {1},
  pages = {1--7},
  year = {2018},
  publisher = {Nature Publishing Group},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1711.02524.pdf}
}
</pre>
</div>

<div id="akyrillidis2018provable" style="display: none;">
<pre>With nowadays steadily growing quantum processors, it is required to develop new quantum tomography tools that are tailored for high-dimensional systems. In this work, we describe such a computational tool, based on recent ideas from non-convex optimization. The algorithm excels in the compressed-sensing-like setting, where only a few data points are measured from a low-rank or highly-pure quantum state of a high-dimensional system. We show that the algorithm can practically be used in quantum tomography problems that are beyond the reach of convex solvers, and, moreover, is faster than other state-of-the-art non-convex approaches. Crucially, we prove that, despite being a non-convex program, under mild conditions, the algorithm is guaranteed to converge to the global minimum of the problem; thus, it constitutes a provable quantum state tomography protocol.</pre>
</div>


<script>
function toggleAbstractCCkyrillidis2018provable(parameter) {
    var x= document.getElementById('akyrillidis2018provable');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkyrillidis2018provable(parameter) {
    var x= document.getElementById('Bkyrillidis2018provable');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkyrillidis2018provable(parameter) {
    var x= document.getElementById('akyrillidis2018provable');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="park2018finding">Park, D., Kyrillidis, A., Caramanis, C., &amp; Sanghavi, S. (2018). Finding low-rank solutions via nonconvex matrix factorization, efficiently and provably. <i>SIAM Journal on Imaging Sciences</i>, <i>11</i>(4), 2165–2204.</span></div>
<button class="button0" onclick="toggleBibtexpark2018finding()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpark2018finding()">Abstract</button>



    <a href="https://arxiv.org/pdf/1606.03168.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bpark2018finding" style="display: none;">
<pre>@article{park2018finding,
  title = {Finding low-rank solutions via nonconvex matrix factorization, efficiently and provably},
  author = {Park, Dohyung and Kyrillidis, Anastasios and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {SIAM Journal on Imaging Sciences},
  volume = {11},
  number = {4},
  pages = {2165--2204},
  year = {2018},
  publisher = {SIAM},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1606.03168.pdf}
}
</pre>
</div>

<div id="apark2018finding" style="display: none;">
<pre>A rank-r matrix X ∈\mathbbR^m \times n can be written as a product UV^⊤, where U ∈\mathbbR^m \times r and V ∈\mathbbR^n \times r. One could exploit this observation in optimization: e.g., consider the minimization of a convex function f(X) over rank-r matrices, where the set of rank-r matrices is modeled via the factorization UV^⊤. Though such parameterization reduces the number of variables, and is more computationally efficient (of particular interest is the case r \lleq minm,n), it comes at a cost: f(UV^⊤) becomes a non-convex function w.r.t. U and V. 
   
   We study such parameterization for optimization of generic convex objectives f, and focus on first-order, gradient descent algorithmic solutions. We propose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient first-order method that operates on the U,V factors. We show that when f is (restricted) smooth, BFGD has local sublinear convergence, and linear convergence when f is both (restricted) smooth and (restricted) strongly convex. For several key applications, we provide simple and efficient initialization schemes that provide approximate solutions good enough for the above convergence results to hold.</pre>
</div>


<script>
function toggleAbstractCCpark2018finding(parameter) {
    var x= document.getElementById('apark2018finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpark2018finding(parameter) {
    var x= document.getElementById('Bpark2018finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpark2018finding(parameter) {
    var x= document.getElementById('apark2018finding');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="sinno2018towards">Sinno, Z., Caramanis, C., &amp; Bovik, A. C. (2018). Towards a closed form second-order natural scene statistics model. <i>IEEE Transactions on Image Processing</i>, <i>27</i>(7), 3194–3209.</span></div>
<button class="button0" onclick="toggleBibtexsinno2018towards()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractsinno2018towards()">Abstract</button>



 


<div id="Bsinno2018towards" style="display: none;">
<pre>@article{sinno2018towards,
  title = {Towards a closed form second-order natural scene statistics model},
  author = {Sinno, Zeina and Caramanis, Constantine and Bovik, Alan C},
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {7},
  pages = {3194--3209},
  year = {2018},
  publisher = {IEEE},
  group = {journal}
}
</pre>
</div>

<div id="asinno2018towards" style="display: none;">
<pre>Previous work on natural scene statistics (NSS)-based image models has focused primarily on characterizing the univariate bandpass statistics of single pixels. These models have proven to be powerful tools driving a variety of computer vision and image/video processing applications, including depth estimation, image quality assessment, and image denoising, among others. Multivariate NSS models descriptive of the joint distributions of spatially separated bandpass image samples have, however, received relatively little attention. Here, we develop a closed form bivariate spatial correlation model of bandpass and normalized image samples that completes an existing 2D joint generalized Gaussian distribution model of adjacent bandpass pixels. Our model is built using a set of diverse, high-quality naturalistic photographs, and as a control, we study the model properties on white noise. We also study the way the model fits are affected when the images are modified by common distortions.</pre>
</div>


<script>
function toggleAbstractCCsinno2018towards(parameter) {
    var x= document.getElementById('asinno2018towards');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexsinno2018towards(parameter) {
    var x= document.getElementById('Bsinno2018towards');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractsinno2018towards(parameter) {
    var x= document.getElementById('asinno2018towards');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="chen2017convex">Chen, Y., Yi, X., &amp; Caramanis, C. (2017). Convex and nonconvex formulations for mixed regression with two components: Minimax optimal rates. <i>IEEE Transactions on Information Theory</i>, <i>64</i>(3), 1738–1766.</span></div>
<button class="button0" onclick="toggleBibtexchen2017convex()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractchen2017convex()">Abstract</button>



    <a href="https://arxiv.org/pdf/1312.7006.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bchen2017convex" style="display: none;">
<pre>@article{chen2017convex,
  title = {Convex and nonconvex formulations for mixed regression with two components: Minimax optimal rates},
  author = {Chen, Yudong and Yi, Xinyang and Caramanis, Constantine},
  journal = {IEEE Transactions on Information Theory},
  volume = {64},
  number = {3},
  pages = {1738--1766},
  year = {2017},
  publisher = {IEEE},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1312.7006.pdf}
}
</pre>
</div>

<div id="achen2017convex" style="display: none;">
<pre>We consider the mixed regression problem with two components, under adversarial and stochastic noise. We give a convex optimization formulation that provably recovers the true solution, and provide upper bounds on the recovery errors for both arbitrary noise and stochastic noise settings. We also give matching minimax lower bounds (up to log factors), showing that under certain assumptions, our algorithm is information-theoretically optimal. Our results represent the first tractable algorithm guaranteeing successful recovery with tight bounds on recovery errors and sample complexity.</pre>
</div>


<script>
function toggleAbstractCCchen2017convex(parameter) {
    var x= document.getElementById('achen2017convex');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexchen2017convex(parameter) {
    var x= document.getElementById('Bchen2017convex');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractchen2017convex(parameter) {
    var x= document.getElementById('achen2017convex');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="meirom2017detecting">Meirom, E. A., Caramanis, C., Mannor, S., Orda, A., &amp; Shakkottai, S. (2017). Detecting Cascades from Weak Signatures. <i>IEEE Transactions on Network Science and Engineering</i>, <i>5</i>(4), 313–325.</span></div>
<button class="button0" onclick="toggleBibtexmeirom2017detecting()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractmeirom2017detecting()">Abstract</button>



 


<div id="Bmeirom2017detecting" style="display: none;">
<pre>@article{meirom2017detecting,
  title = {Detecting Cascades from Weak Signatures},
  author = {Meirom, Eli A and Caramanis, Constantine and Mannor, Shie and Orda, Ariel and Shakkottai, Sanjay},
  journal = {IEEE Transactions on Network Science and Engineering},
  volume = {5},
  number = {4},
  pages = {313--325},
  year = {2017},
  publisher = {IEEE},
  group = {journal}
}
</pre>
</div>

<div id="ameirom2017detecting" style="display: none;">
<pre>Inspired by cyber-security applications, we consider the problem of detecting an infection process in a network when the indication that any particular node is infected is extremely noisy. Instead of waiting for a single node to provide sufficient evidence that it is indeed infected, we take advantage of the graph structure to detect cascades of weak indications of failures. We view the detection problem as a hypothesis testing problem, devise a new inference algorithm, and analyze its false positive and false negative errors in the high noise regime. Extensive simulations show that our algorithm is able to obtain low errors in the high noise regime by taking advantage of cascading topology analysis.</pre>
</div>


<script>
function toggleAbstractCCmeirom2017detecting(parameter) {
    var x= document.getElementById('ameirom2017detecting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexmeirom2017detecting(parameter) {
    var x= document.getElementById('Bmeirom2017detecting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractmeirom2017detecting(parameter) {
    var x= document.getElementById('ameirom2017detecting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="xu2016statistical">Xu, H., Caramanis, C., &amp; Mannor, S. (2016). Statistical optimization in high dimensions. <i>Operations Research</i>, <i>64</i>(4), 958–979.</span></div>
<button class="button0" onclick="toggleBibtexxu2016statistical()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractxu2016statistical()">Abstract</button>



 


<div id="Bxu2016statistical" style="display: none;">
<pre>@article{xu2016statistical,
  title = {Statistical optimization in high dimensions},
  author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  journal = {Operations research},
  volume = {64},
  number = {4},
  pages = {958--979},
  year = {2016},
  publisher = {INFORMS},
  group = {journal}
}
</pre>
</div>

<div id="axu2016statistical" style="display: none;">
<pre>We consider optimization problems whose parameters are known only approximately, based on noisy samples. In large-scale applications, the number of samples one can collect is typically of the same order of (or even less than) the dimensionality of the problem. This so-called high-dimensional statistical regime has been the object of intense recent research in machine learning and statistics, primarily due to phenomena inherent to this regime, such as the fact that the noise one sees here often dwarfs the magnitude of the signal itself. While relevant in numerous important operations research and engineering optimization applications, this setup falls far outside the traditional scope of robust and stochastic optimization. We propose three algorithms to address this setting, combining ideas from statistics, machine learning, and robust optimization. Our algorithms are motivated by three natural optimization objectives: minimizing the number of grossly violated constraints; maximizing the number of exactly satisfied constraints; and, finally, developing algorithms whose running time scales with the intrinsic dimension of a problem, as opposed to its observed dimension?a mismatch that, as we discuss in detail, can be dire in settings where constraints are meant to describe preferences of behaviors. The key ingredients of our algorithms are dimensionality reduction techniques from machine learning, robust optimization, and concentration of measure tools from statistics.</pre>
</div>


<script>
function toggleAbstractCCxu2016statistical(parameter) {
    var x= document.getElementById('axu2016statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexxu2016statistical(parameter) {
    var x= document.getElementById('Bxu2016statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractxu2016statistical(parameter) {
    var x= document.getElementById('axu2016statistical');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="ye2016user">Ye, Q., Bursalioglu, O. Y., Papadopoulos, H. C., Caramanis, C., &amp; Andrews, J. G. (2016). User association and interference management in massive MIMO HetNets. <i>IEEE Transactions on Communications</i>, <i>64</i>(5), 2049–2065.</span></div>
<button class="button0" onclick="toggleBibtexye2016user()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractye2016user()">Abstract</button>



    <a href="https://arxiv.org/pdf/1509.07594.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bye2016user" style="display: none;">
<pre>@article{ye2016user,
  title = {User association and interference management in massive MIMO HetNets},
  author = {Ye, Qiaoyang and Bursalioglu, Ozgun Yilmaz and Papadopoulos, Haralabos C and Caramanis, Constantine and Andrews, Jeffrey G},
  journal = {IEEE Transactions on Communications},
  volume = {64},
  number = {5},
  pages = {2049--2065},
  year = {2016},
  publisher = {IEEE},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1509.07594.pdf}
}
</pre>
</div>

<div id="aye2016user" style="display: none;">
<pre>Two key traits of 5G cellular networks are much higher base station (BS) densities - especially in the case of low-power BSs - and the use of massive MIMO at these BSs. This paper explores how massive MIMO can be used to jointly maximize the offloading gains and minimize the interference challenges arising from adding small cells. We consider two interference management approaches: joint transmission (JT) with local precoding, where users are served simultaneously by multiple BSs without requiring channel state information exchanges among cooperating BSs, and resource blanking, where some macro BS resources are left blank to reduce the interference in the small cell downlink. A key advantage offered by massive MIMO is channel hardening, which enables to predict instantaneous rates a priori. This allows us to develop a unified framework, where resource allocation is cast as a network utility maximization (NUM) problem, and to demonstrate large gains in cell-edge rates based on the NUM solution. We propose an efficient dual subgradient based algorithm, which converges towards the NUM solution. A scheduling scheme is also proposed to approach the NUM solution. Simulations illustrate more than 2x rate gain for 10th percentile users vs. an optimal association without interference management.</pre>
</div>


<script>
function toggleAbstractCCye2016user(parameter) {
    var x= document.getElementById('aye2016user');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexye2016user(parameter) {
    var x= document.getElementById('Bye2016user');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractye2016user(parameter) {
    var x= document.getElementById('aye2016user');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="chen2015matrix">Chen, Y., Xu, H., Caramanis, C., &amp; Sanghavi, S. (2015). Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs. <i>IEEE Transactions on Information Theory</i>, <i>62</i>(1), 503–526.</span></div>
<button class="button0" onclick="toggleBibtexchen2015matrix()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractchen2015matrix()">Abstract</button>



    <a href="https://arxiv.org/pdf/1102.2254.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bchen2015matrix" style="display: none;">
<pre>@article{chen2015matrix,
  title = {Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs},
  author = {Chen, Yudong and Xu, Huan and Caramanis, Constantine and Sanghavi, Sujay},
  journal = {IEEE Transactions on Information Theory},
  volume = {62},
  number = {1},
  pages = {503--526},
  year = {2015},
  publisher = {IEEE},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1102.2254.pdf}
}
</pre>
</div>

<div id="achen2015matrix" style="display: none;">
<pre>This paper considers the problem of matrix completion when some number of the columns are completely and arbitrarily corrupted, potentially by a malicious adversary. It is well-known that standard algorithms for matrix completion can return arbitrarily poor results, if even a single column is corrupted. One direct application comes from robust collaborative filtering. Here, some number of users are so-called manipulators who try to skew the predictions of the algorithm by calibrating their inputs to the system. In this paper, we develop an efficient algorithm for this problem based on a combination of a trimming procedure and a convex program that minimizes the nuclear norm and the l_1,2 norm. Our theoretical results show that given a vanishing fraction of observed entries, it is nevertheless possible to complete the underlying matrix even when the number of corrupted columns grows. Significantly, our results hold without any assumptions on the locations or values of the observed entries of the manipulated columns. Moreover, we show by an information-theoretic argument that our guarantees are nearly optimal in terms of the fraction of sampled entries on the authentic columns, the fraction of corrupted columns, and the rank of the underlying matrix. Our results therefore sharply characterize the tradeoffs between sample, robustness and rank in matrix completion.</pre>
</div>


<script>
function toggleAbstractCCchen2015matrix(parameter) {
    var x= document.getElementById('achen2015matrix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexchen2015matrix(parameter) {
    var x= document.getElementById('Bchen2015matrix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractchen2015matrix(parameter) {
    var x= document.getElementById('achen2015matrix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="gopalan2015wireless">Gopalan, A., Caramanis, C., &amp; Shakkottai, S. (2015). Wireless scheduling with partial channel state information: large deviations and optimality. <i>Queueing Systems</i>, <i>80</i>(4), 293–340.</span></div>
<button class="button0" onclick="toggleBibtexgopalan2015wireless()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractgopalan2015wireless()">Abstract</button>



    <a href="https://arxiv.org/pdf/1405.6307.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bgopalan2015wireless" style="display: none;">
<pre>@article{gopalan2015wireless,
  title = {Wireless scheduling with partial channel state information: large deviations and optimality},
  author = {Gopalan, Aditya and Caramanis, Constantine and Shakkottai, Sanjay},
  journal = {Queueing Systems},
  volume = {80},
  number = {4},
  pages = {293--340},
  year = {2015},
  publisher = {Springer},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1405.6307.pdf}
}
</pre>
</div>

<div id="agopalan2015wireless" style="display: none;">
<pre>We consider a server serving a time-slotted queued system of multiple packet-based flows, with exogenous packet arrivals and time-varying service rates. At each time, the server can observe instantaneous service rates for only a subset of flows (from within a fixed collection of observable subsets) before scheduling a flow in the subset for service. We are interested in queue-length aware scheduling to keep the queues short, and develop scheduling algorithms that use only partial service rate information from subsets of channels to minimize the likelihood of queue overflow in the system. Specifically, we present a new joint subset-sampling and scheduling algorithm called Max-Exp that uses only the current queue lengths to pick a subset of flows, and subsequently schedules a flow using the Exponential rule. When the collection of observable subsets is disjoint, we show that Max-Exp achieves the best exponential decay rate, among all scheduling algorithms using partial information, of the tail of the longest queue in the system. Towards this, we employ novel analytical techniques for studying the performance of scheduling algorithms using partial state, which may be of independent interest. These include new sample-path large deviations results for processes obtained by non-random, predictable sampling of sequences of independent and identically distributed random variables. A consequence of these results is that scheduling with partial state information yields a rate function significantly different from scheduling with full channel information. In the special case when the observable subsets are singleton flows, i.e., when there is effectively no a priori channel-state information, Max-Exp reduces to simply serving the flow with the longest queue; thus, our results show that to always serve the longest queue in the absence of any channel-state information is large-deviations optimal.</pre>
</div>


<script>
function toggleAbstractCCgopalan2015wireless(parameter) {
    var x= document.getElementById('agopalan2015wireless');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexgopalan2015wireless(parameter) {
    var x= document.getElementById('Bgopalan2015wireless');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractgopalan2015wireless(parameter) {
    var x= document.getElementById('agopalan2015wireless');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="milling2015distinguishing">Milling, C., Caramanis, C., Mannor, S., &amp; Shakkottai, S. (2015). Distinguishing infections on different graph topologies. <i>IEEE Transactions on Information Theory</i>, <i>61</i>(6), 3100–3120.</span></div>
<button class="button0" onclick="toggleBibtexmilling2015distinguishing()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractmilling2015distinguishing()">Abstract</button>



    <a href="https://arxiv.org/pdf/1309.6545.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bmilling2015distinguishing" style="display: none;">
<pre>@article{milling2015distinguishing,
  title = {Distinguishing infections on different graph topologies},
  author = {Milling, Chris and Caramanis, Constantine and Mannor, Shie and Shakkottai, Sanjay},
  journal = {IEEE Transactions on Information Theory},
  volume = {61},
  number = {6},
  pages = {3100--3120},
  year = {2015},
  publisher = {IEEE},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/1309.6545.pdf}
}
</pre>
</div>

<div id="amilling2015distinguishing" style="display: none;">
<pre>The history of infections and epidemics holds famous examples where understanding, containing and ultimately treating an outbreak began with understanding its mode of spread. Influenza, HIV and most computer viruses, spread person to person, device to device, through contact networks; Cholera, Cancer, and seasonal allergies, on the other hand, do not. In this paper we study two fundamental questions of detection: first, given a snapshot view of a (perhaps vanishingly small) fraction of those infected, under what conditions is an epidemic spreading via contact (e.g., Influenza), distinguishable from a "random illness" operating independently of any contact network (e.g., seasonal allergies); second, if we do have an epidemic, under what conditions is it possible to determine which network of interactions is the main cause of the spread – the causative network – without any knowledge of the epidemic, other than the identity of a minuscule subsample of infected nodes?
The core, therefore, of this paper, is to obtain an understanding of the diagnostic power of network information. We derive sufficient conditions networks must satisfy for these problems to be identifiable, and produce efficient, highly scalable algorithms that solve these problems. We show that the identifiability condition we give is fairly mild, and in particular, is satisfied by two common graph topologies: the grid, and the Erdos-Renyi graphs.</pre>
</div>


<script>
function toggleAbstractCCmilling2015distinguishing(parameter) {
    var x= document.getElementById('amilling2015distinguishing');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexmilling2015distinguishing(parameter) {
    var x= document.getElementById('Bmilling2015distinguishing');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractmilling2015distinguishing(parameter) {
    var x= document.getElementById('amilling2015distinguishing');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>

<h2 id="books-chapters">Books Chapters</h2>

<ol class="bibliography"></ol>

<h2 id="lecture-notes">Lecture Notes</h2>

<ol class="bibliography"></ol>


</div>

    </div>
    
  </body>
</html>
