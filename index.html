<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
       &middot; Constantine Caramanis
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h2>
        Constantine Caramanis
      </h2>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">


      

      <a class="sidebar-nav-item active" href="/">Home</a>
      <a class="sidebar-nav-item" href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item" href="/publications/">Publications</a>
      <a class="sidebar-nav-item" href="/researchgroup/">Research Group</a>
      <a class="sidebar-nav-item" href="/researchprojects/">Research Projects</a>
      <a class="sidebar-nav-item" href="https://scholar.google.com/citations?user=47YTUrEAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a>
      <a class="sidebar-nav-item" href="https://www.youtube.com/@constantine.caramanis" target="_blank">YouTube</a>
      <a class="sidebar-nav-item" href="https://ml.utexas.edu/ifml" target="_blank">IFML</a>
      <a class="sidebar-nav-item" href="https://wncg.org" target="_blank">WNCG</a>
      <a class="sidebar-nav-item" href="https://archimedesai.gr/en/" target="_blank">Archimedes AI</a>




      <!--<span class="sidebar-nav-item">Jekyll Hyde, Currently v2.1.0</span>-->
    </nav>

    <p>&copy; 2025. All rights reserved.</p> 
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title"></h1>
  <table class="imgtable"><tr><td>
<img src="images/Caramanis_ECE_2018_06.jpg" alt="Constantine Caramanis Photo" width="150px" />&nbsp;</td>
<td align="left"><ul>
<p style="font-family: 'Garamond';font-size:16px"><b>Constantine Caramanis</b></p>
<p style="font-family: 'Garamond';font-size:16px">Professor, Dept. of <a href="http://www.ece.utexas.edu/" target="_blank">Electrical 
and Computer Engineering</a></p>
<p style="font-family: 'Garamond';font-size:16px">Chandra Family Endowed Distinguished Professorship in Electrical and Computer Engineering</p>
<p style="font-family: 'Garamond';font-size:16px">Member of the <a href="http://www.cs.utexas.edu/" target="_blank">Computer Science</a> Graduate 
Studies Committee </p>
<p style="font-family: 'Garamond';font-size:16px">Office: 2501 Speedway, EER Building Room 6.820</p>
<p style="font-family: 'Garamond';font-size:16px"><tt>e-mail: constantine <a href="at">at</a> utexas.edu</tt></p>
 <br />
</ul>
</td></tr></table>

<hr />

<p>
I am a Professor in the ECE department of The University of Texas at Austin. I received a PhD in EECS from The Massachusetts Institute of Technology, in the Laboratory for Information and Decision Systems (LIDS), and an AB in Mathematics from Harvard University. I received the NSF CAREER award in 2011, and I am an IEEE Fellow. 
</p>
<p><br /></p>

<p>My current research interests focus on decision-making in large-scale complex systems, with a focus on learning and computation. Specifically, I am interested in robust and adaptable optimization, high dimensional statistics and machine learning, and applications to large-scale networks, including social networks, wireless networks, transportation networks, and energy networks. I have also worked on applications of machine learning and optimization to computer-aided design.
</p>
<p><br /></p>
<p> I am affiliated with the NSF <a href="https://ml.utexas.edu/ifml" target="_blank">Institute for Foundations of Machine Learning,</a> and the <a href="https://ml.utexas.edu/" target="_blank"> Machine Learning Lab.</a></p>
<p> I am affiliated with the <a href="https://archimedesai.gr/en/" target="_blank">Archimedes Research Center</a> in Athens.</p>

<hr />

<h3 id="teaching">Teaching</h3>

<ul>
  <li>Fall 2025: Convex Optimization</li>
  <li>Spring 2025: Data Science Lab</li>
</ul>

<p>I have also created three classes which I have made available online.</p>

<ul>
  <li>
    <p><a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc" target="_blank"> Optimization Algorithms</a></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORcTRFMVF3aUgyYlHsxfhNL" target="_blank">Combinatorial Optimization</a></p>
  </li>
  <li>
    <p><a href="https://caramanis.github.io/MachineLearningClass/" target="_blank">Introduction to Machine Learning (in Greek)</a></p>
  </li>
</ul>

<hr />

<h3 id="research-group">Research Group</h3>

<h4 id="current-group">Current Group</h4>

<ul>
  <li>
    <p><a href="https://alexia-atsidakou.netlify.app/" target="_blank">Alexia Atsidakou (ECE)</a> (co-advised with Sujay Sanghavi)</p>
  </li>
  <li>
    <p><a href="https://caramanis.github.io/" target="_blank">Alexandros Kouridakis (CS)</a></p>
  </li>
  <li>
    <p><a href="https://lntk.github.io//" target="_blank">Khang Le (ECE) </a> (co-advised with Nhat Ho)</p>
  </li>
  <li>
    <p><a href="https://liturout.github.io/" target="_blank">Litu Rout (ECE) </a> (co-advised with Sanjay Shakkottai)</p>
  </li>
  <li>
    <p><a href="https://nikostsikouras.github.io/" target="_blank">Nikos Tsikouras (NKUA) </a> (co-advised with Christos Tzamos)</p>
  </li>
  <li>
    <p><a href="https://apostolistsorvantzis.github.io/" target="_blank"> Apostolis Tsorvantzis (NTUA) </a> (co-advised with Dimitris Fotakis and Chara Podimata)</p>
  </li>
</ul>

<h4 id="group-alumni">Group Alumni</h4>

<ul>
  <li>
    <p><a href="https://www.linkedin.com/in/eirini-asteri-a015a9174/" target="_blank">Eirini Asteri</a> (co-advised with Alex Dimakis): Google</p>
  </li>
  <li>
    <p><a href="https://pages.cs.wisc.edu/%7Eyudongchen" target="_blank">Yudong Chen</a>: Associate Professor at Cornell ORIE –&gt; Associate Professor at Wisconsin-Madison CS.</p>
  </li>
  <li>
    <p><a href="https://matthewfaw.github.io/" target="_blank">Matthew Faw (ECE)</a> (co-advised with Sanjay Shakkottai): Post doc at Georgia Tech.</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/douglasfearing" target="_blank">Doug Fearing</a> (with C. Barnhart, MIT): Assistant Professor, UT Austin B. School –&gt; Dodgers –&gt; Zelus Analytics</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/aminkhalek/" target="_blank">Amin Abdel-Khalek</a> (co-advised with Robert Heath): Apple</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/harish-ganapathy-0a940414/" target="_blank">Harish Ganapathy</a>: Google Brain</p>
  </li>
  <li>
    <p><a href="https://ece.iisc.ac.in/~aditya/" target="_blank">Aditya Gopalan</a> (with Sanjay Shakkottai): Associate Professor, Indian Institute of Science</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/melissa-hall-924a44b6/" target="_blank">Melissa Hall</a>: Facebook</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/jehoffmann/?originalSubdomain=fr" target="_blank">Jessica Hoffmann</a>: Google</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/kiyeon-jeon/?originalSubdomain=kr" target="_blank">Kiyeon Jeon</a>: IBM</p>
  </li>
  <li>
    <p>Ken’ichi Kamada: Visiting Scientist from Yokogawa Co.</p>
  </li>
  <li>
    <p><a href="https://ashishkatiyar13.github.io/" target="_blank">Ashish Katiyar </a>: Amazon</p>
  </li>
  <li>
    <p><a href="https://kwonchungli.github.io/" target="_blank">Jeongyeol Kwon (ECE) </a>: Post doc at Madison</p>
  </li>
  <li>
    <p><a href="http://akyrillidis.github.io/about/" target="_blank">Anastasios Kyrillidis</a> (w/ Sujay Sanghavi &amp; Alex Dimakis): Assistant Professor, Rice CS</p>
  </li>
  <li>
    <p><a href="http://li-tianyang.com/research/" target="_blank">Tianyang Li</a>: Two Sigma</p>
  </li>
  <li>
    <p><a href="https://liuliuforph.github.io/" target="_blank">Liu Liu</a>: Tencent</p>
  </li>
  <li>
    <p><a href="http://mitliagkas.github.io/" target="_blank">Ioannis Mitliagkas</a> (co-advised with Sriram Vishwanath): Associate Professor, U. Montreal</p>
  </li>
  <li>
    <p><a href="https://web.ma.utexas.edu/users/jneeman/" target="_blank">Joe Neeman</a> (co-advised with Sujay Sanghavi): Assistant Professor, UT Austin, Math</p>
  </li>
  <li>
    <p><a href="http://dhpark22.github.io/" target="_blank"> Dohyung Park</a> (co-advised with Sujay Sanghavi): Facebook</p>
  </li>
  <li>
    <p><a href="https://scholar.google.com/citations?user=86YS2vQAAAAJ&amp;hl=el" target="_blank">Orestis Papadigenopoulos (CS) </a></p>
  </li>
  <li>
    <p><a href="https://sites.google.com/site/pslsri93/home" target="_blank">Srilakshmi Pattabiraman</a>: UIUC Ph.D.</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/sanika-phanse-245429145/" target="_blank">Sanika Phanse</a>: MongoDB</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/zrinka-puljiz-133a593" target="_blank">Zrinka Puljiz</a> (co-advised with Sanjay Shakkottai): Google</p>
  </li>
  <li>
    <p>Ashish Singh (with Michael Orshansky): Terra Technology</p>
  </li>
  <li>
    <p><a href="https://sites.gatech.edu/huan-xu/" target="_blank">Huan Xu</a> (with David Morton): Assistant Professor, Georgia Tech, ISyE Dept. –&gt; Alibaba</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/ye-wang-301b8648" target="_blank">Wang Ye</a> (co-advised with Michael Orshansky): Cadence</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/qiaoyang-ye-92073548" target="_blank">Qiaoyang Ye</a> (co-advised with Jeff Andrews): Samsung Research America</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/xinyang-yi-21818845" target="_blank">Xinyang Yi</a>: Google Brain</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/sungho-yun-4821131a" target="_blank">Sungho Yun</a>: Apple.</p>
  </li>
  <li>
    <p><a href="https://www.linkedin.com/in/jiacheng-zhuo-b8b021100/" target="_blank">Jiacheng Zhuo</a>: Algorithm Developer at Hudson River Trading.</p>
  </li>
</ul>

<hr />
<h3 id="the-last-ten-publications-chronologically">The last ten publications, chronologically…</h3>
<ol class="bibliography"><li><div class="text-justify"><span id="rout2025anchored">Rout, Litu, Constantine Caramanis, and Sanjay Shakkottai. “Anchored Diffusion Language Model.” <i>Preprint</i>, 2025.</span></div>
<button class="button0" onclick="toggleBibtexrout2025anchored()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractrout2025anchored()">Abstract</button>



    <a href="https://arxiv.org/pdf/2505.18456"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Brout2025anchored" style="display: none;">
<pre>@article{rout2025anchored,
  title = {Anchored Diffusion Language Model},
  author = {Rout, Litu and Caramanis, Constantine and Shakkottai, Sanjay},
  journal = {preprint},
  year = {2025},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/2505.18456}
}
</pre>
</div>

<div id="arout2025anchored" style="display: none;">
<pre>Diffusion Language Models (DLMs) promise parallel generation and bidirectional context, yet they underperform autoregressive (AR) models in both likelihood modeling and generated text quality. We identify that this performance gap arises when important tokens (e.g., key words or low-frequency words that anchor a sentence) are masked early in the forward process, limiting contextual information for accurate reconstruction. To address this, we introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage framework that first predicts distributions over important tokens via an anchor network, and then predicts the likelihoods of missing tokens conditioned on the anchored predictions. ADLM significantly improves test perplexity on LM1B and OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap with strong AR baselines. It also achieves state-of-the-art performance in zero-shot generalization across seven benchmarks and surpasses AR models in MAUVE score, which marks the first time a DLM generates better human-like text than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower Bound (ANELBO) objective and show that anchoring improves sample complexity and likelihood modeling. Beyond diffusion, anchoring boosts performance in AR models and enhances reasoning in math and logic tasks, outperforming existing chain-of-thought approaches.</pre>
</div>


<script>
function toggleAbstractCCrout2025anchored(parameter) {
    var x= document.getElementById('arout2025anchored');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexrout2025anchored(parameter) {
    var x= document.getElementById('Brout2025anchored');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractrout2025anchored(parameter) {
    var x= document.getElementById('arout2025anchored');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="raoofinfilling">Raoof, Negin, Litu Rout, Giannis Daras, Sujay Sanghavi, Constantine Caramanis, Sanjay Shakkottai, and Alex Dimakis. “Infilling Score: A Pretraining Data Detection Algorithm for Large Language Models.” <i>Proceedings of the International Conference on Learning Representations (ICLR)</i>, 2025.</span></div>
<button class="button0" onclick="toggleBibtexraoofinfilling()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractraoofinfilling()">Abstract</button>



    <a href="https://openreview.net/pdf?id=9QPH1YQCMn"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Braoofinfilling" style="display: none;">
<pre>@article{raoofinfilling,
  title = {Infilling Score: A Pretraining Data Detection Algorithm for Large Language Models},
  author = {Raoof, Negin and Rout, Litu and Daras, Giannis and Sanghavi, Sujay and Caramanis, Constantine and Shakkottai, Sanjay and Dimakis, Alex},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2025},
  group = {proceedings},
  arxiv = {https://openreview.net/pdf?id=9QPH1YQCMn}
}
</pre>
</div>

<div id="araoofinfilling" style="display: none;">
<pre>In pretraining data detection, the goal is to detect whether a given sentence is in the dataset used for training a Large Language Model LLM). Recent methods (such as Min-K % and Min-K%++) reveal that most training corpora are likely contaminated with both sensitive content and evaluation benchmarks, leading to inflated test set performance. These methods sometimes fail to detect samples from the pretraining data, primarily because they depend on statistics composed of causal token likelihoods. We introduce Infilling Score, a new test-statistic based on non-causal token likelihoods. Infilling Score can be computed for autoregressive models without re-training using Bayes rule. A naive application of Bayes rule scales linearly with the vocabulary size. However, we propose a ratio test-statistic whose computation is invariant to vocabulary size. Empirically, our method achieves a significant accuracy gain over state-of-the-art methods including Min-K%, and Min-K%++ on the WikiMIA benchmark across seven models with different parameter sizes. Further, we achieve higher AUC compared to reference-free methods on the challenging MIMIR benchmark. Finally, we create a benchmark dataset consisting of recent data sources published after the release of Llama-3; this benchmark provides a statistical baseline to indicate potential corpora used for Llama-3 training.</pre>
</div>


<script>
function toggleAbstractCCraoofinfilling(parameter) {
    var x= document.getElementById('araoofinfilling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexraoofinfilling(parameter) {
    var x= document.getElementById('Braoofinfilling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractraoofinfilling(parameter) {
    var x= document.getElementById('araoofinfilling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="rout2024rb">Rout, Litu, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. “RB-Modulation: Training-Free Personalization of Diffusion Models Using Stochastic Optimal Control.” <i>Proceedings of the International Conference on Learning Representations (ICLR)</i>, 2025.</span></div>
<button class="button0" onclick="toggleBibtexrout2024rb()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractrout2024rb()">Abstract</button>



    <a href="https://arxiv.org/pdf/2405.17401"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Brout2024rb" style="display: none;">
<pre>@article{rout2024rb,
  title = {RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control},
  author = {Rout, Litu and Chen, Yujia and Ruiz, Nataniel and Kumar, Abhishek and Caramanis, Constantine and Shakkottai, Sanjay and Chu, Wen-Sheng},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2025},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2405.17401}
}
</pre>
</div>

<div id="arout2024rb" style="display: none;">
<pre>We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. 
RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our test-time optimization framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets. See project page (https://rb-modulation.github.io/) for code and further details.</pre>
</div>


<script>
function toggleAbstractCCrout2024rb(parameter) {
    var x= document.getElementById('arout2024rb');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexrout2024rb(parameter) {
    var x= document.getElementById('Brout2024rb');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractrout2024rb(parameter) {
    var x= document.getElementById('arout2024rb');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="rout2024semantic">Rout, Litu, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. “Semantic Image Inversion and Editing Using Rectified Stochastic Differential Equations.” <i>Proceedings of the International Conference on Learning Representations (ICLR)</i>, 2025.</span></div>
<button class="button0" onclick="toggleBibtexrout2024semantic()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractrout2024semantic()">Abstract</button>



    <a href="https://arxiv.org/pdf/2410.10792"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Brout2024semantic" style="display: none;">
<pre>@article{rout2024semantic,
  title = {Semantic image inversion and editing using rectified stochastic differential equations},
  author = {Rout, Litu and Chen, Yujia and Ruiz, Nataniel and Caramanis, Constantine and Shakkottai, Sanjay and Chu, Wen-Sheng},
  journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
  arxiv = {https://arxiv.org/pdf/2410.10792},
  year = {2025}
}
</pre>
</div>

<div id="arout2024semantic" style="display: none;">
<pre>Generative models transform random noise into images, while their inversion aims to reconstruct structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of real images using stochastic equivalents of rectified flow models (e.g., Flux). While Diffusion Models (DMs) dominate the field of generative modeling for images, their inversion suffers from faithfulness and editability challenges due to nonlinear drift and diffusion. Existing DM inversion methods require costly training of additional parameters or test-time optimization of latent variables. Rectified Flows (RFs) offer a promising alternative to DMs, yet their inversion remains underexplored. 
We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator, and prove that the resulting vector field is equivalent to a rectified stochastic differential equation. 
We further extend our framework to design a stochastic sampler for Flux.
Our method achieves state-of-the-art performance in zero-shot inversion and editing, surpassing prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference. See our project page https://rf-inversion.github.io/ for code and demo.</pre>
</div>


<script>
function toggleAbstractCCrout2024semantic(parameter) {
    var x= document.getElementById('arout2024semantic');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexrout2024semantic(parameter) {
    var x= document.getElementById('Brout2024semantic');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractrout2024semantic(parameter) {
    var x= document.getElementById('arout2024semantic');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="rout2024secondorder">Rout, Litu, Yujia Chen, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. “Beyond First-Order Tweedie: Solving Inverse Problems Using Latent Diffusion.” <i>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2024.</span></div>
<button class="button0" onclick="toggleBibtexrout2024secondorder()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractrout2024secondorder()">Abstract</button>



    <a href="https://arxiv.org/pdf/2312.00852.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Brout2024secondorder" style="display: none;">
<pre>@article{rout2024secondorder,
  title = {Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion},
  author = {Rout, Litu and Chen, Yujia and Kumar, Abhishek and Caramanis, Constantine and Shakkottai, Sanjay and Chu, Wen-Sheng},
  journal = {Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2312.00852.pdf}
}
</pre>
</div>

<div id="arout2024secondorder" style="display: none;">
<pre>Sampling from the posterior distribution poses a major computational challenge in solving inverse problems using latent diffusion models. Common methods rely on Tweedie’s first-order moments, which are known to induce a quality-limiting bias. Existing second-order approximations are impractical due to prohibitive computational costs, making standard reverse diffusion processes intractable for posterior sampling. This paper introduces Second-order Tweedie sampler from Surrogate Loss (STSL), a novel sampler that offers efficiency comparable to first-order Tweedie with a tractable reverse process using second-order approximation. Our theoretical results reveal that the second-order approximation is lower bounded by our surrogate loss that only requires O(1) compute using the trace of the Hessian, and by the lower bound we derive a new drift term to make the reverse process tractable. Our method surpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural function evaluations, respectively, while notably enhancing sampling quality on FFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to text-guided image editing and addresses residual distortions present from corrupted images in leading text-guided image editing methods. To our best knowledge, this is the first work to offer an efficient second-order approximation in solving inverse problems using latent diffusion and editing real-world images with corruptions.</pre>
</div>


<script>
function toggleAbstractCCrout2024secondorder(parameter) {
    var x= document.getElementById('arout2024secondorder');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexrout2024secondorder(parameter) {
    var x= document.getElementById('Brout2024secondorder');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractrout2024secondorder(parameter) {
    var x= document.getElementById('arout2024secondorder');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="zhuoSlowMatrix2024">Zhuo, Jiacheng, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. “On the Computational and Statistical Complexity of over-Parameterized Matrix Sensing.” <i>Journal of Machine Learning Research</i>, 2024.</span></div>
<button class="button0" onclick="toggleBibtexzhuoSlowMatrix2024()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuoSlowMatrix2024()">Abstract</button>



    <a href="https://arxiv.org/pdf/2102.02756.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="BzhuoSlowMatrix2024" style="display: none;">
<pre>@article{zhuoSlowMatrix2024,
  title = {On the computational and statistical complexity of over-parameterized matrix sensing},
  author = {Zhuo, Jiacheng and Kwon, Jeongyeol and Ho, Nhat and Caramanis, Constantine},
  journal = {Journal of Machine Learning Research},
  year = {2024},
  group = {journal},
  arxiv = {https://arxiv.org/pdf/2102.02756.pdf}
}
</pre>
</div>

<div id="azhuoSlowMatrix2024" style="display: none;">
<pre>We consider solving the low rank matrix sensing problem with Factorized Gradient Descend (FGD) method when the true rank is unknown and over-specified, which we refer to as over-parameterized matrix sensing.
If the ground truth signal X^* ∈\mathbbR^d*d is of rank r, but we try to recover it using FF^⊤where F ∈\mathbbR^d*k and k&gt;r, the existing statistical analysis falls short, due to a flat local curvature of the loss function around the global maxima. By decomposing the factorized matrix \fitMat into separate column spaces to capture the effect of extra ranks, we show that ||F_t F_t - F||_F^2 converges to a statistical error of \tilde\mathcalO \parenthk d σ^2/n after \tilde\mathcalO(\frac\sigma_rσ\sqrt\fracnd) number of iterations where \fitMat_t is the output of FGD after t iterations, σ^2 is the variance of the observation noise, \sigma_r is the r-th largest eigenvalue of \trueMat, and n is the number of sample. Our results, therefore, offer a comprehensive picture of the statistical and computational complexity of FGD for the over-parameterized matrix sensing problem.</pre>
</div>


<script>
function toggleAbstractCCzhuoSlowMatrix2024(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2024');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuoSlowMatrix2024(parameter) {
    var x= document.getElementById('BzhuoSlowMatrix2024');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuoSlowMatrix2024(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2024');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="atsidakou24apandora">Atsidakou, Alexia, Constantine Caramanis, Evangelia Gergatsouli, Orestis Papadigenopoulos, and Christos Tzamos. “Contextual Pandora’s Box.” <i>Association for the Advancement of Artificial Intelligence (AAAI)</i>, 2024.</span></div>
<button class="button0" onclick="toggleBibtexatsidakou24apandora()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractatsidakou24apandora()">Abstract</button>



    <a href="https://arxiv.org/pdf/2205.13114.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Batsidakou24apandora" style="display: none;">
<pre>@article{atsidakou24apandora,
  title = {Contextual Pandora's Box},
  author = {Atsidakou, Alexia and Caramanis, Constantine and Gergatsouli, Evangelia and Papadigenopoulos, Orestis and Tzamos, Christos},
  journal = {Association for the Advancement of Artificial Intelligence (AAAI)},
  year = {2024},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2205.13114.pdf}
}
</pre>
</div>

<div id="aatsidakou24apandora" style="display: none;">
<pre>Pandora’s Box is a fundamental stochastic optimization problem, where the decision-maker must find a good alternative while minimizing the search cost of exploring the value of each alternative. In the original formulation, it is assumed that accurate priors are given for the values of all the alternatives, while
recent work studies the online variant of Pandora’s Box where priors are originally unknown. 
In this work, we extend Pandora’s Box to the online setting, while incorporating context. 
At every round, we are presented with a number of alternatives each having a context, an exploration cost and an unknown value drawn from an unknown prior distribution that may change at every round. Our main result is a no-regret algorithm that performs comparably well to the optimal algorithm which knows all prior distributions exactly. Our algorithm works even in the bandit setting where the algorithm never learns the values of the alternatives that were not explored. 
The key technique that enables our result is novel a modification of the realizability condition in contextual bandits that connects a context to the reservation value of the corresponding distribution rather than its mean.</pre>
</div>


<script>
function toggleAbstractCCatsidakou24apandora(parameter) {
    var x= document.getElementById('aatsidakou24apandora');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexatsidakou24apandora(parameter) {
    var x= document.getElementById('Batsidakou24apandora');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractatsidakou24apandora(parameter) {
    var x= document.getElementById('aatsidakou24apandora');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon24lmdp">Kwon, Jeongyeol, Yonathan Efroni, Shie Mannor, and Constantine Caramanis. “RL in Latent MDPs Is Tractable: Online Guarantees via Off-Policy Evaluation.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2024.</span></div>
<button class="button0" onclick="toggleBibtexkwon24lmdp()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon24lmdp()">Abstract</button>



    <a href="https://arxiv.org/pdf/2406.01389"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon24lmdp" style="display: none;">
<pre>@article{kwon24lmdp,
  title = {RL in Latent MDPs is Tractable: Online Guarantees via Off-Policy Evaluation},
  author = {Kwon, Jeongyeol and Efroni, Yonathan and Mannor, Shie and Caramanis, Constantine},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2024},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2406.01389}
}
</pre>
</div>

<div id="akwon24lmdp" style="display: none;">
<pre>In many real-world decision problems there is partially observed, hidden or latent information that remains fixed throughout an interaction. Such decision problems can be modeled as Latent Markov Decision Processes (LMDPs), where a latent variable is selected at the beginning of an interaction and is not disclosed to the agent. In the last decade, there has been significant progress in solving LMDPs under different structural assumptions. However, for general LMDPs, there is no known learning algorithm that provably matches the existing lower bound (Kwon et al., 2021). We introduce the first sample-efficient algorithm for LMDPs without any additional structural assumptions. Our result builds off a new perspective on the role of off-policy evaluation guarantees and coverage coefficients in LMDPs, a perspective, that has been overlooked in the context of exploration in partially observed environments. Specifically, we establish a novel off-policy evaluation lemma and introduce a new coverage coefficient for LMDPs. Then, we show how these can be used to derive near-optimal guarantees of an optimistic exploration algorithm. These results, we believe, can be valuable for a wide range of interactive learning problems beyond LMDPs, and especially, for partially observed environments.</pre>
</div>


<script>
function toggleAbstractCCkwon24lmdp(parameter) {
    var x= document.getElementById('akwon24lmdp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon24lmdp(parameter) {
    var x= document.getElementById('Bkwon24lmdp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon24lmdp(parameter) {
    var x= document.getElementById('akwon24lmdp');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="tsikouras2024optimization">Tsikouras, Nikos, Constantine Caramanis, and Christos Tzamos. “Optimization Can Learn Johnson Lindenstrauss Embeddings.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2024.</span></div>
<button class="button0" onclick="toggleBibtextsikouras2024optimization()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracttsikouras2024optimization()">Abstract</button>



    <a href="arXiv preprint arXiv:2412.07242"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Btsikouras2024optimization" style="display: none;">
<pre>@article{tsikouras2024optimization,
  title = {Optimization Can Learn Johnson Lindenstrauss Embeddings},
  author = {Tsikouras, Nikos and Caramanis, Constantine and Tzamos, Christos},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  group = {proceedings},
  arxiv = {arXiv preprint arXiv:2412.07242},
  year = {2024}
}
</pre>
</div>

<div id="atsikouras2024optimization" style="display: none;">
<pre>Embeddings play a pivotal role across various disciplines, offering compact representations of complex data structures. Randomized methods like Johnson-Lindenstrauss (JL) provide state-of-the-art and essentially unimprovable theoretical guarantees for achieving such representations. These guarantees are worst-case and in particular, neither the analysis, nor the algorithm, takes into account any potential structural information of the data. The natural question is: must we randomize? Could we instead use an optimization-based approach, working directly with the data? A first answer is no: as we show, the distance-preserving objective of JL has a non-convex landscape over the space of projection matrices, with many bad stationary points. But this is not the final answer.
We present a novel method motivated by diffusion models, that circumvents this fundamental challenge: rather than performing optimization directly over the space of projection matrices, we use optimization over the larger space of random solution samplers, gradually reducing the variance of the sampler. We show that by moving through this larger space, our objective converges to a deterministic (zero variance) solution, avoiding bad stationary points.
This method can also be seen as an optimization-based derandomization approach and is an idea and method that we believe can be applied to many other problems.</pre>
</div>


<script>
function toggleAbstractCCtsikouras2024optimization(parameter) {
    var x= document.getElementById('atsikouras2024optimization');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtextsikouras2024optimization(parameter) {
    var x= document.getElementById('Btsikouras2024optimization');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracttsikouras2024optimization(parameter) {
    var x= document.getElementById('atsikouras2024optimization');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2024prospective">Kwon, Jeongyeol, Yonathan Efroni, Shie Mannor, and Constantine Caramanis. “Prospective Side Information for Latent MDPs.” In <i>International Conference on Machine Learning (ICML)</i>. PMLR, 2024.</span></div>
<button class="button0" onclick="toggleBibtexkwon2024prospective()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2024prospective()">Abstract</button>



    <a href="https://arxiv.org/pdf/2310.07596"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2024prospective" style="display: none;">
<pre>@inproceedings{kwon2024prospective,
  title = {Prospective side information for latent {MDP}s},
  author = {Kwon, Jeongyeol and Efroni, Yonathan and Mannor, Shie and Caramanis, Constantine},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = {},
  year = {2024},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2310.07596}
}
</pre>
</div>

<div id="akwon2024prospective" style="display: none;">
<pre>In many interactive decision-making settings, there is latent and unobserved information that remains fixed. Consider, for example, a dialogue system, where complete information about a user, such as the user’s preferences, is not given. In such an environment, the latent information remains fixed throughout each episode, since the identity of the user does not change during an interaction. This type of environment can be modeled as a Latent Markov Decision Process (LMDP), a special instance of Partially Observed Markov Decision Processes (POMDPs). Previous work established exponential lower bounds in the number of latent contexts for the LMDP class. This puts forward a question: under which natural assumptions a near-optimal policy of an LMDP can be efficiently learned? In this work, we study the class of LMDPs with prospective side information, when an agent receives additional, weakly revealing, information on the latent context at the beginning of each episode. We show that, surprisingly, this problem is not captured by contemporary settings and algorithms designed for partially observed environments. We then establish that any sample efficient algorithm must suffer at least Ω(K^2/3)-regret, as opposed to standard Ω(\sqrtK) lower bounds, and design an algorithm with a matching upper bound. </pre>
</div>


<script>
function toggleAbstractCCkwon2024prospective(parameter) {
    var x= document.getElementById('akwon2024prospective');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2024prospective(parameter) {
    var x= document.getElementById('Bkwon2024prospective');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2024prospective(parameter) {
    var x= document.getElementById('akwon2024prospective');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>


</div>

    </div>
    
  </body>
</html>
