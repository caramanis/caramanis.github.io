<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
       &middot; Constantine Caramanis
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h2>
        Constantine Caramanis
      </h2>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">


      

      <a class="sidebar-nav-item active" href="/">Home</a>
      <a class="sidebar-nav-item" href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item" href="/publications/">Publications</a>
      <a class="sidebar-nav-item" href="/researchgroup/">Research Group</a>
      <a class="sidebar-nav-item" href="/researchprojects/">Research Projects</a>
      <a class="sidebar-nav-item" href="https://scholar.google.com/citations?user=47YTUrEAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a>
      <a class="sidebar-nav-item" href="https://www.youtube.com/channel/UCSv1_NZITsPl-abaCWtRrJg" target="_blank">YouTube</a>
      <a class="sidebar-nav-item" href="https://ml.utexas.edu/ifml" target="_blank">IFML</a>
      <a class="sidebar-nav-item" href="https://wncg.org" target="_blank">WNCG</a>




      <!--<span class="sidebar-nav-item">Jekyll Hyde, Currently v2.1.0</span>-->
    </nav>

    <p>&copy; 2021. All rights reserved.</p> 
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title"></h1>
  <table class="imgtable"><tr><td>
<img src="images/Caramanis_ECE_2018_06.jpg" alt="Constantine Caramanis Photo" width="150px" />&nbsp;</td>
<td align="left"><ul>
<p style="font-family: 'Garamond';font-size:16px"><b>Constantine Caramanis</b></p>
<p style="font-family: 'Garamond';font-size:16px">Professor, Dept. of <a href="http://www.ece.utexas.edu/" target="_blank">Electrical 
and Computer Engineering</a></p>
<p style="font-family: 'Garamond';font-size:16px">William H. Hartwig Endowed Faculty Fellowship in Engineering</p>
<p style="font-family: 'Garamond';font-size:16px">Director of the <a href="http://wncg.org/" target="_blank">Wireless Networking and Communications 
Group</a></p>
<p style="font-family: 'Garamond';font-size:16px">Member of the <a href="http://www.cs.utexas.edu/" target="_blank">Computer Science</a> Graduate 
Studies Committee </p>
<p style="font-family: 'Garamond';font-size:16px">Office: 2501 Speedway, EER Building Room 6.820</p>
<p style="font-family: 'Garamond';font-size:16px"><tt>e-mail: constantine <a href="at">at</a> utexas.edu</tt></p>
 <br />
</ul>
</td></tr></table>

<hr />

<p>
I am a Professor in the ECE department of The University of Texas at Austin. I received a PhD in EECS from The Massachusetts Institute of Technology, in the Laboratory for Information and Decision Systems (LIDS), and an AB in Mathematics from Harvard University. I received the NSF CAREER award in 2011.
</p>
<p><br /></p>

<p>My current research interests focus on decision-making in large-scale complex systems, with a focus on learning and computation. Specifically, I am interested in robust and adaptable optimization, high dimensional statistics and machine learning, and applications to large-scale networks, including social networks, wireless networks, transportation networks, and energy networks. I have also worked on applications of machine learning and optimization to computer-aided design.
</p>
<p><br /></p>
<p> I am affiliated with the NSF <a href="https://ml.utexas.edu/ifml" target="_blank">Institute for Foundations of Machine Learning.</a></p>

<hr />

<h3 id="teaching">Teaching</h3>

<ul>
  <li>
    <p>Spring 2021: Data Science Lab</p>
  </li>
  <li>
    <p>Fall 2020: Combinatorial Optimization</p>
  </li>
  <li>
    <p>Spring 2020: Large Scale Optimization II</p>
  </li>
</ul>

<p>I have also created two classes which I have made available online.</p>

<ul>
  <li>
    <p><a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc" target="_blank"> Optimization Algorithms</a></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORcTRFMVF3aUgyYlHsxfhNL" target="_blank">Combinatorial Optimization</a></p>
  </li>
</ul>

<hr />

<h3 id="research-group">Research Group</h3>

<h4 id="current-group">Current Group</h4>

<ul>
  <li>
    <p>Alexia Atsidakou (ECE) (co-advised with Sujay Sanghavi)</p>
  </li>
  <li>
    <p><a href="https://matthewfaw.github.io/" target="_blank">Matthew Faw (ECE)</a> (co-advised with Sanjay Shakkottai)</p>
  </li>
  <li>
    <p><a href="https://www.cs.utexas.edu/~hoffmann/" target="_blank">Jessica Hoffmann</a> – post doctoral scholar at <a href="https://ml.utexas.edu/ifml" target="_blank">IFML</a></p>
  </li>
  <li>
    <p><a href="https://ashishkatiyar13.github.io/" target="_blank">Ashish Katiyar (ECE) </a></p>
  </li>
  <li>
    <p><a href="https://kwonchungli.github.io/" target="_blank">Jeong Yeol Kwon (ECE) </a></p>
  </li>
  <li>
    <p><a href="https://liuliuforph.github.io/" target="_blank">Liu Liu (ECE) </a></p>
  </li>
  <li>
    <p><a href="https://www.cs.utexas.edu/~papadig/" target="_blank">Orestis Papadigenopoulos (CS) </a></p>
  </li>
  <li>
    <p>Sanika Phanse (ECE)</p>
  </li>
  <li>
    <p><a href="http://www.cs.utexas.edu/~jzhuo/" target="_blank">Jiacheng Zhuo (CS)</a></p>
  </li>
</ul>

<h4 id="group-alumni">Group Alumni</h4>

<ul>
  <li>
    <p>Eirini Asteri (co-advised with Alex Dimakis): Google</p>
  </li>
  <li>
    <p><a href="https://people.orie.cornell.edu/yudong.chen/" target="_blank">Yudong Chen</a>: Assistant Professor at Cornell ORIE</p>
  </li>
  <li>
    <p>Doug Fearing (with C. Barnhart, MIT): Assistant Professor, UT Austin B. School –&gt; Dodgers –&gt; Zelus Analytics</p>
  </li>
  <li>
    <p>Amin Abdel-Khalek (co-advised with Robert Heath): Freescale</p>
  </li>
  <li>
    <p>Harish Ganapathy: Google</p>
  </li>
  <li>
    <p><a href="https://ece.iisc.ac.in/~aditya/" target="_blank">Aditya Gopalan</a> (with Sanjay Shakkottai): Assistant Professor, Indian Institute of Science</p>
  </li>
  <li>
    <p>Melissa Hall: Facebook</p>
  </li>
  <li>
    <p><a href="https://www.cs.utexas.edu/~hoffmann/" target="_blank">Jessica Hoffmann</a> – post doctoral scholar at <a href="https://ml.utexas.edu/ifml" target="_blank">IFML</a></p>
  </li>
  <li>
    <p>Kiyeon Jeon</p>
  </li>
  <li>
    <p>Ken’ichi Kamada: Visiting Scientist from Yokogawa Co.</p>
  </li>
  <li>
    <p><a href="http://akyrillidis.github.io/about/" target="_blank">Anastasios Kyrillidis</a> (w/ Sujay Sanghavi &amp; Alex Dimakis): Assistant Professor, Rice CS</p>
  </li>
  <li>
    <p><a href="http://li-tianyang.com/research/" target="_blank">Tianyang Li</a>: Two Sigma</p>
  </li>
  <li>
    <p><a href="http://mitliagkas.github.io/" target="_blank">Ioannis Mitliagkas</a> (with Sriram Vishwanath): Assistant Professor, U. Montreal</p>
  </li>
  <li>
    <p>Zrinka Puljiz (co-advised with Sanjay Shakkottai): Google</p>
  </li>
  <li>
    <p>Ashish Singh (with Michael Orshansky): Terra Technology</p>
  </li>
  <li>
    <p><a href="https://web.ma.utexas.edu/users/jneeman/" target="_blank">Joe Neeman</a> (with Sujay Sanghavi): Assistant Professor, UT Austin, Math</p>
  </li>
  <li>
    <p><a href="http://dhpark22.github.io/" target="_blank"> Dohyung Park</a> (co-advised with Sujay Sanghavi): Facebook</p>
  </li>
  <li>
    <p>Srilakshmi Pattabiraman</p>
  </li>
  <li>
    <p>Wang Ye (co-advised with Michael Orshansky)</p>
  </li>
  <li>
    <p><a href="https://sites.gatech.edu/huan-xu/" target="_blank">Huan Xu</a> (with David Morton): Assistant Professor, Georgia Tech, ISyE Dept. –&gt; Alibaba.</p>
  </li>
  <li>
    <p>Qiaoyang Ye (with Jeff Andrews): Intel</p>
  </li>
  <li>
    <p>Xinyang Yi: Google</p>
  </li>
  <li>
    <p>Sungho Yun: ASSIA inc.</p>
  </li>
</ul>

<hr />
<h3 id="the-last-ten-publications-chronologically">The last ten publications, chronologically…</h3>
<ol class="bibliography"><li><div class="text-justify"><span id="basu2020contextual">Basu, Soumya, Orestis Papadigenopoulos, Constantine Caramanis, and Sanjay Shakkottai. “Contextual Blocking Bandits.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>. PMLR, 2021.</span></div>
<button class="button0" onclick="toggleBibtexbasu2020contextual()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractbasu2020contextual()">Abstract</button>



    <a href="https://arxiv.org/pdf/2003.03426.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bbasu2020contextual" style="display: none;">
<pre>@inproceedings{basu2020contextual,
  title = {Contextual Blocking Bandits},
  author = {Basu, Soumya and Papadigenopoulos, Orestis and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2003.03426.pdf}
}
</pre>
</div>

<div id="abasu2020contextual" style="display: none;">
<pre>We study a novel variant of the multi-armed bandit problem, where at each time step, the player observes an independently sampled context that determines the arms’ mean rewards. However, playing an arm blocks it (across all contexts) for a fixed number of future time steps. The above contextual setting captures important scenarios such as recommendation systems or ad placement with diverse users.
This problem has been recently studied \citepDSSX18 in the full-information setting (i.e., assuming knowledge of the mean context-dependent arm rewards), where competitive ratio bounds have been derived.
We focus on the bandit setting, where these means are initially unknown; we propose a UCB-based variant of the full-information algorithm that guarantees a \mathcalO(\log T)-regret w.r.t. an α-optimal strategy in T time steps, matching the Ω(\log(T)) regret lower bound in this setting. Due to the time correlations caused by blocking, existing techniques for upper bounding regret fail. For proving our regret bounds, we introduce the novel concepts of delayed exploitation and opportunistic sub-sampling and combine them with ideas from combinatorial bandits and non-stationary Markov chains coupling.</pre>
</div>


<script>
function toggleAbstractCCbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexbasu2020contextual(parameter) {
    var x= document.getElementById('Bbasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2020minimax">Kwon, Jeongyeol, Nhat Ho, and Constantine Caramanis. “On the Minimax Optimality of the Em Algorithm for Learning Two-Component Mixed Linear Regression.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>. PMLR, 2021.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020minimax()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020minimax()">Abstract</button>



    <a href="https://arxiv.org/pdf/2006.02601.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020minimax" style="display: none;">
<pre>@inproceedings{kwon2020minimax,
  title = {On the minimax optimality of the em algorithm for learning two-component mixed linear regression},
  author = {Kwon, Jeongyeol and Ho, Nhat and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2006.02601.pdf}
}
</pre>
</div>

<div id="akwon2020minimax" style="display: none;">
<pre>We study the convergence rates of the EM algorithm for learning two-component mixed linear regression under all regimes of signal-to-noise ratio (SNR). We resolve a long-standing question that many recent results have attempted to tackle: we completely characterize the convergence behavior of EM, and show that the EM algorithm achieves minimax optimal sample complexity under all SNR regimes. In particular, when the SNR is sufficiently large, the EM updates converge to the true parameter θ^* at the standard parametric convergence rate \calo((d/n)^1/2) after \calo(\log(n/d)) iterations. In the regime where the SNR is above \calo((d/n)^1/4) and below some constant, the EM iterates converge to a \calo(\rm SNR^-1 (d/n)^1/2) neighborhood of the true parameter, when the number of iterations is of the order \calo(\rm SNR^-2 \log(n/d)). In the low SNR regime where the SNR is below \calo((d/n)^1/4), we show that EM converges to a \calo((d/n)^1/4) neighborhood of the true parameters, after \calo((n/d)^1/2) iterations. Notably, these results are achieved under mild conditions of either random initialization or an efficiently computable local initialization. By providing tight convergence guarantees of the EM algorithm in middle-to-low SNR regimes, we fill the remaining gap in the literature, and significantly, reveal that in low SNR, EM changes rate, matching the n^-1/4 rate of the MLE, a behavior that previous work had been unable to show. </pre>
</div>


<script>
function toggleAbstractCCkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020minimax(parameter) {
    var x= document.getElementById('Bkwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="papadig2021recurrent">Papadigenopoulos, Orestis, and Constantine Caramanis. “Recurrent Submodular Welfare and Matroid Blocking Bandits.” <i>ArXiv Preprint ArXiv:2102.00321</i>, 2021.</span></div>
<button class="button0" onclick="toggleBibtexpapadig2021recurrent()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpapadig2021recurrent()">Abstract</button>



    <a href="https://arxiv.org/pdf/2102.00321.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bpapadig2021recurrent" style="display: none;">
<pre>@article{papadig2021recurrent,
  title = {Recurrent Submodular Welfare and Matroid Blocking Bandits},
  author = {Papadigenopoulos, Orestis and Caramanis, Constantine},
  journal = {arXiv preprint arXiv:2102.00321},
  year = {2021},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/2102.00321.pdf}
}
</pre>
</div>

<div id="apapadig2021recurrent" style="display: none;">
<pre>A recent line of research focuses on the study of the stochastic multi-armed bandits problem (MAB), in the case where temporal correlations of specific structure are imposed between the player’s actions and the reward distributions of the arms (Kleinberg and Immorlica [FOCS18], Basu et al. [NIPS19]). As opposed to the standard MAB setting, where the optimal solution in hindsight can be trivially characterized, these correlations lead to (sub-)optimal solutions that exhibit interesting dynamical patterns – a phenomenon that yields new challenges both from an algorithmic as well as a learning perspective. In this work, we extend the above direction to a combinatorial bandit setting and study a variant of stochastic MAB, where arms are subject to matroid constraints and each arm becomes unavailable (blocked) for a fixed number of rounds after each play. A natural common generalization of the state-of-the-art for blocking bandits, and that for matroid bandits, yields a (1−1/e)-approximation for partition matroids, yet it only guarantees a 1/2-approximation for general matroids. In this paper we develop new algorithmic ideas that allow us to obtain a polynomial-time (1−1/e)-approximation algorithm (asymptotically and in expectation) for any matroid, and thus allow us to control the (1−1/e)-approximate regret. A key ingredient is the technique of correlated (interleaved) scheduling. Along the way, we discover an interesting connection to a variant of Submodular Welfare Maximization, for which we provide (asymptotically) matching upper and lower approximability bounds.</pre>
</div>


<script>
function toggleAbstractCCpapadig2021recurrent(parameter) {
    var x= document.getElementById('apapadig2021recurrent');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpapadig2021recurrent(parameter) {
    var x= document.getElementById('Bpapadig2021recurrent');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpapadig2021recurrent(parameter) {
    var x= document.getElementById('apapadig2021recurrent');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="zhuoSlowMatrix2021">Zhuo, Jiacheng, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. “On the Computational and Statistical Complexity of over-Parameterized Matrix Sensing.” <i>Preprint</i>, 2021.</span></div>
<button class="button0" onclick="toggleBibtexzhuoSlowMatrix2021()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuoSlowMatrix2021()">Abstract</button>



    <a href="https://www.cs.utexas.edu/ jzhuo/over-parametrized.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="BzhuoSlowMatrix2021" style="display: none;">
<pre>@article{zhuoSlowMatrix2021,
  title = {On the computational and statistical complexity of over-parameterized matrix sensing},
  author = {Zhuo, Jiacheng and Kwon, Jeongyeol and Ho, Nhat and Caramanis, Constantine},
  journal = {preprint},
  year = {2021},
  group = {misc},
  arxiv = {https://www.cs.utexas.edu/~jzhuo/over-parametrized.pdf}
}
</pre>
</div>

<div id="azhuoSlowMatrix2021" style="display: none;">
<pre>We consider solving the low rank matrix sensing problem with Factorized Gradient Descend (FGD) method when the true rank is unknown and over-specified, which we refer to as over-parameterized matrix sensing.
If the ground truth signal X^* ∈\mathbbR^d*d is of rank r, but we try to recover it using FF^⊤where F ∈\mathbbR^d*k and k&gt;r, the existing statistical analysis falls short, due to a flat local curvature of the loss function around the global maxima. By decomposing the factorized matrix \fitMat into separate column spaces to capture the effect of extra ranks, we show that ||F_t F_t - F||_F^2 converges to a statistical error of \tilde\mathcalO \parenthk d σ^2/n after \tilde\mathcalO(\frac\sigma_rσ\sqrt\fracnd) number of iterations where \fitMat_t is the output of FGD after t iterations, σ^2 is the variance of the observation noise, \sigma_r is the r-th largest eigenvalue of \trueMat, and n is the number of sample. Our results, therefore, offer a comprehensive picture of the statistical and computational complexity of FGD for the over-parameterized matrix sensing problem.</pre>
</div>


<script>
function toggleAbstractCCzhuoSlowMatrix2021(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuoSlowMatrix2021(parameter) {
    var x= document.getElementById('BzhuoSlowMatrix2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuoSlowMatrix2021(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="faw2020mix">Faw, Matthew, Rajat Sen, Karthikeyan Shanmugam, Constantine Caramanis, and Sanjay Shakkottai. “Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i> 33 (2020).</span></div>
<button class="button0" onclick="toggleBibtexfaw2020mix()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractfaw2020mix()">Abstract</button>



    <a href="https://arxiv.org/pdf/1907.10154.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bfaw2020mix" style="display: none;">
<pre>@article{faw2020mix,
  title = {Mix and Match: An Optimistic Tree-Search Approach for Learning Models from Mixture Distributions},
  author = {Faw, Matthew and Sen, Rajat and Shanmugam, Karthikeyan and Caramanis, Constantine and Shakkottai, Sanjay},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1907.10154.pdf}
}
</pre>
</div>

<div id="afaw2020mix" style="display: none;">
<pre>We consider a covariate shift problem where one has access to several different training datasets for the same learning problem and a small validation set which possibly differs from all the individual training distributions. This covariate shift is caused, in part, due to unobserved features in the datasets. The objective, then, is to find the best mixture distribution over the training datasets (with only observed features) such that training a learning algorithm using this mixture has the best validation performance. Our proposed algorithm, MixNMatch, combines stochastic gradient descent (SGD) with optimistic tree search and model re-use (evolving partially trained models with samples from different mixture distributions) over the space of mixtures, for this task. We prove simple regret guarantees for our algorithm with respect to recovering the optimal mixture, given a total budget of SGD evaluations. Finally, we validate our algorithm on two real-world datasets.</pre>
</div>


<script>
function toggleAbstractCCfaw2020mix(parameter) {
    var x= document.getElementById('afaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexfaw2020mix(parameter) {
    var x= document.getElementById('Bfaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractfaw2020mix(parameter) {
    var x= document.getElementById('afaw2020mix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2019disentangling">Hoffmann, Jessica, Soumya Basu, Surbhi Goel, and Constantine Caramanis. “Disentangling Mixtures of Epidemics on Graphs.” <i>International Conference on Machine Learning (ICML)</i>, 2020.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2019disentangling()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2019disentangling()">Abstract</button>



    <a href="https://arxiv.org/pdf/1906.06057.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2019disentangling" style="display: none;">
<pre>@article{hoffmann2019disentangling,
  title = {Disentangling Mixtures of Epidemics on Graphs},
  author = {Hoffmann, Jessica and Basu, Soumya and Goel, Surbhi and Caramanis, Constantine},
  journal = {International Conference on Machine Learning (ICML)},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1906.06057.pdf}
}
</pre>
</div>

<div id="ahoffmann2019disentangling" style="display: none;">
<pre>We consider the problem of learning the weighted edges of a balanced mixture of two undirected graphs from epidemic cascades. While mixture models are popular modeling tools, algorithmic development with rigorous guarantees has lagged. Graph mixtures are apparently no exception: until now, very little is known about whether this problem is solvable. To the best of our knowledge, we establish the first necessary and sufficient conditions for this problem to be solvable in polynomial time on edge-separated graphs. When the conditions are met, i.e., when the graphs are connected with at least three edges, we give an efficient algorithm for learning the weights of both graphs with optimal sample complexity (up to log factors). We give complimentary results and provide sample-optimal (up to log factors) algorithms for mixtures of directed graphs of out-degree at least three, for mixture of undirected graphs of unbalanced and/or unknown priors.</pre>
</div>


<script>
function toggleAbstractCChoffmann2019disentangling(parameter) {
    var x= document.getElementById('ahoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2019disentangling(parameter) {
    var x= document.getElementById('Bhoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2019disentangling(parameter) {
    var x= document.getElementById('ahoffmann2019disentangling');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2020algorithm">Kwon, Jeongyeol, and Constantine Caramanis. “The EM Algorithm Gives Sample-Optimality for Learning Mixtures of Well-Separated Gaussians.” <i>The Conference on Learning Theory (COLT)</i>, 2020.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020algorithm()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020algorithm()">Abstract</button>



    <a href="https://arxiv.org/pdf/2002.00329.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020algorithm" style="display: none;">
<pre>@article{kwon2020algorithm,
  title = {The EM algorithm gives sample-optimality for learning mixtures of well-separated gaussians},
  author = {Kwon, Jeongyeol and Caramanis, Constantine},
  journal = {The Conference on Learning Theory (COLT)},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2002.00329.pdf}
}
</pre>
</div>

<div id="akwon2020algorithm" style="display: none;">
<pre>We consider the problem of spherical Gaussian Mixture models with k?3 components when the components are well separated. A fundamental previous result established that separation of Ω(\sqrt \log k) is necessary and sufficient for identifiability of the parameters with polynomial sample complexity (Regev and Vijayaraghavan, 2017). In the same context, we show that O(kd/ε^2) samples suffice for any ??1/k, closing the gap from polynomial to linear, and thus giving the first optimal sample upper bound for the parameter estimation of well-separated Gaussian mixtures. We accomplish this by proving a new result for the Expectation-Maximization (EM) algorithm: we show that EM converges locally, under separation Ω(\sqrt \log k ). The previous best-known guarantee required Ω(\sqrt k) separation (Yan, et al., 2017). Unlike prior work, our results do not assume or use prior knowledge of the (potentially different) mixing weights or variances of the Gaussian components. Furthermore, our results show that the finite-sample error of EM does not depend on non-universal quantities such as pairwise distances between means of Gaussian components.</pre>
</div>


<script>
function toggleAbstractCCkwon2020algorithm(parameter) {
    var x= document.getElementById('akwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020algorithm(parameter) {
    var x= document.getElementById('Bkwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020algorithm(parameter) {
    var x= document.getElementById('akwon2020algorithm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kocaoglu2020applications">Kocaoglu, Murat, Sanjay Shakkottai, Alexandros G Dimakis, Constantine Caramanis, and Sriram Vishwanath. “Applications of Common Entropy for Causal Inference.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i> 33 (2020).</span></div>
<button class="button0" onclick="toggleBibtexkocaoglu2020applications()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkocaoglu2020applications()">Abstract</button>



    <a href="https://arxiv.org/pdf/1807.10399.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkocaoglu2020applications" style="display: none;">
<pre>@article{kocaoglu2020applications,
  title = {Applications of Common Entropy for Causal Inference},
  author = {Kocaoglu, Murat and Shakkottai, Sanjay and Dimakis, Alexandros G and Caramanis, Constantine and Vishwanath, Sriram},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1807.10399.pdf}
}
</pre>
</div>

<div id="akocaoglu2020applications" style="display: none;">
<pre>We study the problem of discovering the simplest latent variable that can make two observed discrete variables conditionally independent. The minimum entropy required for such a latent is known as common entropy in information theory. We extend this notion to Renyi common entropy by minimizing the Renyi entropy of the latent variable. To efficiently compute common entropy, we propose an iterative algorithm that can be used to discover the trade-off between the entropy of the latent variable and the conditional mutual information of the observed variables. We show two applications of common entropy in causal inference: First, under the assumption that there are no low-entropy mediators, it can be used to distinguish causation from spurious correlation among almost all joint distributions on simple causal graphs with two observed variables. Second, common entropy can be used to improve constraint-based methods such as PC or FCI algorithms in the small-sample regime, where these methods are known to struggle. We propose a modification to these constraint-based methods to assess if a separating set found by these algorithms is valid using common entropy. We finally evaluate our algorithms on synthetic and real data to establish their performance.</pre>
</div>


<script>
function toggleAbstractCCkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkocaoglu2020applications(parameter) {
    var x= document.getElementById('Bkocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="liu2020high">Liu, Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. “High Dimensional Robust Sparse Regression.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 411–21. PMLR, 2020.</span></div>
<button class="button0" onclick="toggleBibtexliu2020high()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractliu2020high()">Abstract</button>



    <a href="https://arxiv.org/pdf/1805.11643.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bliu2020high" style="display: none;">
<pre>@inproceedings{liu2020high,
  title = {High dimensional robust sparse regression},
  author = {Liu, Liu and Shen, Yanyao and Li, Tianyang and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {411--421},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1805.11643.pdf}
}
</pre>
</div>

<div id="aliu2020high" style="display: none;">
<pre>We provide a novel – and to the best of our knowledge, the first – algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity, in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators: when the covariance matrix in sparse regression is identity, our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm which consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance. Also, it is orderwise more efficient computationally than the ellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.</pre>
</div>


<script>
function toggleAbstractCCliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexliu2020high(parameter) {
    var x= document.getElementById('Bliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="zhuo2020communication">Zhuo, Jiacheng, Qi Lei, Alex Dimakis, and Constantine Caramanis. “Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-Norm Balls.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 1464–74. PMLR, 2020.</span></div>
<button class="button0" onclick="toggleBibtexzhuo2020communication()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuo2020communication()">Abstract</button>



    <a href="https://arxiv.org/pdf/1910.07703.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bzhuo2020communication" style="display: none;">
<pre>@inproceedings{zhuo2020communication,
  title = {Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls},
  author = {Zhuo, Jiacheng and Lei, Qi and Dimakis, Alex and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {1464--1474},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1910.07703.pdf}
}
</pre>
</div>

<div id="azhuo2020communication" style="display: none;">
<pre>Large-scale machine learning training suffers from two prior challenges, specifically for nuclear-norm constrained problems with distributed systems: the synchronization slowdown due to the straggling workers, and high communication costs. In this work, we propose an asynchronous Stochastic Frank Wolfe (SFW-asyn) method, which, for the first time, solves the two problems simultaneously, while successfully maintaining the same convergence rate as the vanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon EC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number of machines compared to the vanilla SFW.</pre>
</div>


<script>
function toggleAbstractCCzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuo2020communication(parameter) {
    var x= document.getElementById('Bzhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>


</div>

    </div>
    
  </body>
</html>
