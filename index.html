<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
       &middot; Constantine Caramanis
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h2>
        Constantine Caramanis
      </h2>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">


      

      <a class="sidebar-nav-item active" href="/">Home</a>
      <a class="sidebar-nav-item" href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item" href="/publications/">Publications</a>
      <a class="sidebar-nav-item" href="/researchgroup/">Research Group</a>
      <a class="sidebar-nav-item" href="/researchprojects/">Research Projects</a>
      <a class="sidebar-nav-item" href="https://scholar.google.com/citations?user=47YTUrEAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a>
      <a class="sidebar-nav-item" href="https://www.youtube.com/channel/UCSv1_NZITsPl-abaCWtRrJg" target="_blank">YouTube</a>
      <a class="sidebar-nav-item" href="https://ml.utexas.edu/ifml" target="_blank">IFML</a>
      <a class="sidebar-nav-item" href="https://wncg.org" target="_blank">WNCG</a>
      <a class="sidebar-nav-item" href="https://www.utmlds.club/" target="_blank">MLDS</a>





      <!--<span class="sidebar-nav-item">Jekyll Hyde, Currently v2.1.0</span>-->
    </nav>

    <p>&copy; 2021. All rights reserved.</p> 
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title"></h1>
  <table class="imgtable"><tr><td>
<img src="images/Caramanis_ECE_2018_06.jpg" alt="Constantine Caramanis Photo" width="150px" />&nbsp;</td>
<td align="left"><ul>
<p style="font-family: 'Garamond';font-size:16px"><b>Constantine Caramanis</b></p>
<p style="font-family: 'Garamond';font-size:16px">Professor, Dept. of <a href="http://www.ece.utexas.edu/" target="_blank">Electrical 
and Computer Engineering</a></p>
<p style="font-family: 'Garamond';font-size:16px">William H. Hartwig Endowed Faculty Fellowship in Engineering</p>
<p style="font-family: 'Garamond';font-size:16px">Director of the <a href="http://wncg.org/" target="_blank">Wireless Networking and Communications 
Group</a></p>
<p style="font-family: 'Garamond';font-size:16px">Member of the <a href="http://www.cs.utexas.edu/" target="_blank">Computer Science</a> Graduate 
Studies Committee </p>
<p style="font-family: 'Garamond';font-size:16px">Office: 2501 Speedway, EER Building Room 6.820</p>
<p style="font-family: 'Garamond';font-size:16px"><tt>e-mail: constantine <a href="at">at</a> utexas.edu</tt></p>
 <br />
</ul>
</td></tr></table>

<hr />

<p>
I am a Professor in the ECE department of The University of Texas at Austin. I received a PhD in EECS from The Massachusetts Institute of Technology, in the Laboratory for Information and Decision Systems (LIDS), and an AB in Mathematics from Harvard University. I received the NSF CAREER award in 2011.
</p>
<p><br /></p>

<p>My current research interests focus on decision-making in large-scale complex systems, with a focus on learning and computation. Specifically, I am interested in robust and adaptable optimization, high dimensional statistics and machine learning, and applications to large-scale networks, including social networks, wireless networks, transportation networks, and energy networks. I have also worked on applications of machine learning and optimization to computer-aided design.
</p>
<p><br /></p>
<p> I am affiliated with the NSF <a href="https://ml.utexas.edu/ifml" target="_blank">Institute for Foundations of Machine Learning.</a></p>

<hr />

<h3 id="teaching">Teaching</h3>

<ul>
  <li>
    <p>Spring 2021: Data Science Lab</p>
  </li>
  <li>
    <p>Fall 2020: Combinatorial Optimization</p>
  </li>
  <li>
    <p>Spring 2020: Large Scale Optimization II</p>
  </li>
</ul>

<p>I have also created two classes which I have made available online.</p>

<ul>
  <li>
    <p><a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc" target="_blank"> Optimization Algorithms</a></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORcTRFMVF3aUgyYlHsxfhNL" target="_blank">Combinatorial Optimization</a></p>
  </li>
</ul>

<hr />

<h3 id="research-group">Research Group</h3>

<h4 id="current-group">Current Group</h4>

<ul>
  <li>
    <p>Alexia Atsidakou (ECE) (co-advised with Sujay Sanghavi)</p>
  </li>
  <li>
    <p><a href="https://matthewfaw.github.io/" target="_blank">Matthew Faw (ECE)</a> (co-advised with Sanjay Shakkottai)</p>
  </li>
  <li>
    <p><a href="https://www.cs.utexas.edu/~hoffmann/" target="_blank">Jessica Hoffmann</a> – post doctoral scholar at <a href="https://ml.utexas.edu/ifml" target="_blank">IFML</a></p>
  </li>
  <li>
    <p><a href="https://ashishkatiyar13.github.io/" target="_blank">Ashish Katiyar (ECE) </a></p>
  </li>
  <li>
    <p><a href="https://kwonchungli.github.io/" target="_blank">Jeong Yeol Kwon (ECE) </a></p>
  </li>
  <li>
    <p><a href="https://liuliuforph.github.io/" target="_blank">Liu Liu (ECE) </a></p>
  </li>
  <li>
    <p><a href="https://www.cs.utexas.edu/~papadig/" target="_blank">Orestis Papadigenopoulos (CS) </a></p>
  </li>
  <li>
    <p>Sanika Phanse (ECE)</p>
  </li>
  <li>
    <p><a href="http://www.cs.utexas.edu/~jzhuo/" target="_blank">Jiacheng Zhuo (CS)</a></p>
  </li>
</ul>

<h4 id="group-alumni">Group Alumni</h4>

<ul>
  <li>
    <p>Eirini Asteri (co-advised with Alex Dimakis): Google</p>
  </li>
  <li>
    <p><a href="https://people.orie.cornell.edu/yudong.chen/" target="_blank">Yudong Chen</a>: Assistant Professor at Cornell ORIE</p>
  </li>
  <li>
    <p>Doug Fearing (with C. Barnhart, MIT): Assistant Professor, UT Austin B. School –&gt; Dodgers –&gt; Zelus Analytics</p>
  </li>
  <li>
    <p>Amin Abdel-Khalek (co-advised with Robert Heath): Freescale</p>
  </li>
  <li>
    <p>Harish Ganapathy: Google</p>
  </li>
  <li>
    <p><a href="https://ece.iisc.ac.in/~aditya/" target="_blank">Aditya Gopalan</a> (with Sanjay Shakkottai): Assistant Professor, Indian Institute of Science</p>
  </li>
  <li>
    <p>Melissa Hall: Facebook</p>
  </li>
  <li>
    <p><a href="https://www.cs.utexas.edu/~hoffmann/" target="_blank">Jessica Hoffmann</a> – post doctoral scholar at <a href="https://ml.utexas.edu/ifml" target="_blank">IFML</a></p>
  </li>
  <li>
    <p>Kiyeon Jeon</p>
  </li>
  <li>
    <p>Ken’ichi Kamada: Visiting Scientist from Yokogawa Co.</p>
  </li>
  <li>
    <p><a href="http://akyrillidis.github.io/about/" target="_blank">Anastasios Kyrillidis</a> (w/ Sujay Sanghavi &amp; Alex Dimakis): Assistant Professor, Rice CS</p>
  </li>
  <li>
    <p><a href="http://li-tianyang.com/research/" target="_blank">Tianyang Li</a>: Two Sigma</p>
  </li>
  <li>
    <p><a href="http://mitliagkas.github.io/" target="_blank">Ioannis Mitliagkas</a> (with Sriram Vishwanath): Assistant Professor, U. Montreal</p>
  </li>
  <li>
    <p>Zrinka Puljiz (co-advised with Sanjay Shakkottai): Google</p>
  </li>
  <li>
    <p>Ashish Singh (with Michael Orshansky): Terra Technology</p>
  </li>
  <li>
    <p><a href="https://web.ma.utexas.edu/users/jneeman/" target="_blank">Joe Neeman</a> (with Sujay Sanghavi): Assistant Professor, UT Austin, Math</p>
  </li>
  <li>
    <p><a href="http://dhpark22.github.io/" target="_blank"> Dohyung Park</a> (co-advised with Sujay Sanghavi): Facebook</p>
  </li>
  <li>
    <p>Srilakshmi Pattabiraman</p>
  </li>
  <li>
    <p>Wang Ye (co-advised with Michael Orshansky)</p>
  </li>
  <li>
    <p><a href="https://sites.gatech.edu/huan-xu/" target="_blank">Huan Xu</a> (with David Morton): Assistant Professor, Georgia Tech, ISyE Dept. –&gt; Alibaba.</p>
  </li>
  <li>
    <p>Qiaoyang Ye (with Jeff Andrews): Intel</p>
  </li>
  <li>
    <p>Xinyang Yi: Google</p>
  </li>
  <li>
    <p>Sungho Yun: ASSIA inc.</p>
  </li>
</ul>

<hr />
<h3 id="the-last-ten-publications-chronologically">The last ten publications, chronologically…</h3>
<ol class="bibliography"><li><div class="text-justify"><span id="zhuoSlowMatrix2022">Zhuo, Jiacheng, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. “On the Computational and Statistical Complexity of over-Parameterized Matrix Sensing.” <i>Preprint</i>, 2021.</span></div>
<button class="button0" onclick="toggleBibtexzhuoSlowMatrix2022()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuoSlowMatrix2022()">Abstract</button>



    <a href="https://arxiv.org/pdf/2102.02756.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="BzhuoSlowMatrix2022" style="display: none;">
<pre>@article{zhuoSlowMatrix2022,
  title = {On the computational and statistical complexity of over-parameterized matrix sensing},
  author = {Zhuo, Jiacheng and Kwon, Jeongyeol and Ho, Nhat and Caramanis, Constantine},
  journal = {preprint},
  year = {2021},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/2102.02756.pdf}
}
</pre>
</div>

<div id="azhuoSlowMatrix2022" style="display: none;">
<pre>We consider solving the low rank matrix sensing problem with Factorized Gradient Descend (FGD) method when the true rank is unknown and over-specified, which we refer to as over-parameterized matrix sensing.
If the ground truth signal X^* ∈\mathbbR^d*d is of rank r, but we try to recover it using FF^⊤where F ∈\mathbbR^d*k and k&gt;r, the existing statistical analysis falls short, due to a flat local curvature of the loss function around the global maxima. By decomposing the factorized matrix \fitMat into separate column spaces to capture the effect of extra ranks, we show that ||F_t F_t - F||_F^2 converges to a statistical error of \tilde\mathcalO \parenthk d σ^2/n after \tilde\mathcalO(\frac\sigma_rσ\sqrt\fracnd) number of iterations where \fitMat_t is the output of FGD after t iterations, σ^2 is the variance of the observation noise, \sigma_r is the r-th largest eigenvalue of \trueMat, and n is the number of sample. Our results, therefore, offer a comprehensive picture of the statistical and computational complexity of FGD for the over-parameterized matrix sensing problem.</pre>
</div>


<script>
function toggleAbstractCCzhuoSlowMatrix2022(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2022');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuoSlowMatrix2022(parameter) {
    var x= document.getElementById('BzhuoSlowMatrix2022');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuoSlowMatrix2022(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2022');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="zhuoSlowMatrix2021">Kwon, Jeongyeol, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. “RL for Latent MDPs: Regret Guarantees and a Lower Bound.” <i>Preprint</i>, 2021.</span></div>
<button class="button0" onclick="toggleBibtexzhuoSlowMatrix2021()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuoSlowMatrix2021()">Abstract</button>



    <a href="https://arxiv.org/pdf/2102.04939.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="BzhuoSlowMatrix2021" style="display: none;">
<pre>@article{zhuoSlowMatrix2021,
  title = {RL for Latent MDPs: Regret Guarantees and a Lower Bound},
  author = {Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  journal = {preprint},
  year = {2021},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/2102.04939.pdf}
}
</pre>
</div>

<div id="azhuoSlowMatrix2021" style="display: none;">
<pre>In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of M possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least ?((SA)M) episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, \it i.e., providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., Boots et al.) and a reachability assumption, we show that the need for initialization can be removed.</pre>
</div>


<script>
function toggleAbstractCCzhuoSlowMatrix2021(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuoSlowMatrix2021(parameter) {
    var x= document.getElementById('BzhuoSlowMatrix2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuoSlowMatrix2021(parameter) {
    var x= document.getElementById('azhuoSlowMatrix2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="katiyarRobustMRF2021">Katiyar, Ashish, Soumya Basu, Vatsal Shah, and Constantine Caramanis. “Robust Estimation of Tree Structured Markov Random Fields.” <i>Preprint</i>, 2021.</span></div>
<button class="button0" onclick="toggleBibtexkatiyarRobustMRF2021()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkatiyarRobustMRF2021()">Abstract</button>



    <a href="https://arxiv.org/pdf/2102.08554.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="BkatiyarRobustMRF2021" style="display: none;">
<pre>@article{katiyarRobustMRF2021,
  title = {Robust Estimation of Tree Structured Markov Random Fields},
  author = {Katiyar, Ashish and Basu, Soumya and Shah, Vatsal and Caramanis, Constantine},
  journal = {preprint},
  year = {2021},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/2102.08554.pdf}
}
</pre>
</div>

<div id="akatiyarRobustMRF2021" style="display: none;">
<pre>We study the problem of learning tree-structured Markov random fields (MRF) on discrete random variables with common support when the observations are corrupted by unknown noise. As the presence of noise in the observations obfuscates the original tree structure, the extent of recoverability of the tree-structured MRFs under noisy observations is brought into question.
We show that in a general noise model, the underlying tree structure can be recovered only up to an equivalence class where each of the leaf nodes is indistinguishable from its parent and siblings, forming a leaf cluster. As the indistinguishability arises due to contrived noise models, we study the natural k-ary symmetric channel noise model where the value of each node is changed to a uniform value in the support with an unequal and unknown probability. Here, the answer becomes much more nuanced. We show that with a support size of 2, and the binary symmetric channel noise model, the leaf clusters remain indistinguishable. From support size 3 and up, the recoverability of a leaf cluster is dictated by the joint probability mass function of the nodes within it. We provide a precise characterization of recoverability by deriving a necessary and sufficient condition for the recoverability of a leaf cluster. We provide an algorithm that recovers the tree if this condition is satisfied, and recovers the tree up to the leaf clusters failing this condition.</pre>
</div>


<script>
function toggleAbstractCCkatiyarRobustMRF2021(parameter) {
    var x= document.getElementById('akatiyarRobustMRF2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkatiyarRobustMRF2021(parameter) {
    var x= document.getElementById('BkatiyarRobustMRF2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkatiyarRobustMRF2021(parameter) {
    var x= document.getElementById('akatiyarRobustMRF2021');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="hoffmann2020quarantines">Hoffmann, Jessica, Matt Jordan, and Constantine Caramanis. “Quarantines as a Targeted Immunization Strategy.” <i>ArXiv Preprint ArXiv:2008.08262</i>, 2021.</span></div>
<button class="button0" onclick="toggleBibtexhoffmann2020quarantines()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstracthoffmann2020quarantines()">Abstract</button>



    <a href="https://arxiv.org/pdf/2008.08262.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bhoffmann2020quarantines" style="display: none;">
<pre>@article{hoffmann2020quarantines,
  title = {Quarantines as a Targeted Immunization Strategy},
  author = {Hoffmann, Jessica and Jordan, Matt and Caramanis, Constantine},
  journal = {arXiv preprint arXiv:2008.08262},
  year = {2021},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/2008.08262.pdf}
}
</pre>
</div>

<div id="ahoffmann2020quarantines" style="display: none;">
<pre>In the context of the recent COVID-19 outbreak, quarantine has been used to "flatten the curve" and slow the spread of the disease. In this paper, we show that this is not the only benefit of quarantine for the mitigation of an SIR epidemic spreading on a graph. Indeed, human contact networks exhibit a powerlaw structure, which means immunizing nodes at random is extremely ineffective at slowing the epidemic, while immunizing high-degree nodes can efficiently guarantee herd immunity. We theoretically prove that if quarantines are declared at the right moment, high-degree nodes are disproportionately in the Removed state, which is a form of targeted immunization. Even if quarantines are declared too early, subsequent waves of infection spread slower than the first waves. This leads us to propose an opening and closing strategy aiming at immunizing the graph while infecting the minimum number of individuals, guaranteeing the population is now robust to future infections. To the best of our knowledge, this is the only strategy that guarantees herd immunity without requiring vaccines. We extensively verify our results on simulated and real-life networks.</pre>
</div>


<script>
function toggleAbstractCChoffmann2020quarantines(parameter) {
    var x= document.getElementById('ahoffmann2020quarantines');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexhoffmann2020quarantines(parameter) {
    var x= document.getElementById('Bhoffmann2020quarantines');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstracthoffmann2020quarantines(parameter) {
    var x= document.getElementById('ahoffmann2020quarantines');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kwon2020minimax">Kwon, Jeongyeol, Nhat Ho, and Constantine Caramanis. “On the Minimax Optimality of the Em Algorithm for Learning Two-Component Mixed Linear Regression.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>. PMLR, 2021.</span></div>
<button class="button0" onclick="toggleBibtexkwon2020minimax()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkwon2020minimax()">Abstract</button>



    <a href="https://arxiv.org/pdf/2006.02601.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkwon2020minimax" style="display: none;">
<pre>@inproceedings{kwon2020minimax,
  title = {On the minimax optimality of the em algorithm for learning two-component mixed linear regression},
  author = {Kwon, Jeongyeol and Ho, Nhat and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2006.02601.pdf}
}
</pre>
</div>

<div id="akwon2020minimax" style="display: none;">
<pre>We study the convergence rates of the EM algorithm for learning two-component mixed linear regression under all regimes of signal-to-noise ratio (SNR). We resolve a long-standing question that many recent results have attempted to tackle: we completely characterize the convergence behavior of EM, and show that the EM algorithm achieves minimax optimal sample complexity under all SNR regimes. In particular, when the SNR is sufficiently large, the EM updates converge to the true parameter θ^* at the standard parametric convergence rate \calo((d/n)^1/2) after \calo(\log(n/d)) iterations. In the regime where the SNR is above \calo((d/n)^1/4) and below some constant, the EM iterates converge to a \calo(\rm SNR^-1 (d/n)^1/2) neighborhood of the true parameter, when the number of iterations is of the order \calo(\rm SNR^-2 \log(n/d)). In the low SNR regime where the SNR is below \calo((d/n)^1/4), we show that EM converges to a \calo((d/n)^1/4) neighborhood of the true parameters, after \calo((n/d)^1/2) iterations. Notably, these results are achieved under mild conditions of either random initialization or an efficiently computable local initialization. By providing tight convergence guarantees of the EM algorithm in middle-to-low SNR regimes, we fill the remaining gap in the literature, and significantly, reveal that in low SNR, EM changes rate, matching the n^-1/4 rate of the MLE, a behavior that previous work had been unable to show. </pre>
</div>


<script>
function toggleAbstractCCkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkwon2020minimax(parameter) {
    var x= document.getElementById('Bkwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkwon2020minimax(parameter) {
    var x= document.getElementById('akwon2020minimax');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="basu2020contextual">Basu, Soumya, Orestis Papadigenopoulos, Constantine Caramanis, and Sanjay Shakkottai. “Contextual Blocking Bandits.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>. PMLR, 2021.</span></div>
<button class="button0" onclick="toggleBibtexbasu2020contextual()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractbasu2020contextual()">Abstract</button>



    <a href="https://arxiv.org/pdf/2003.03426.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bbasu2020contextual" style="display: none;">
<pre>@inproceedings{basu2020contextual,
  title = {Contextual Blocking Bandits},
  author = {Basu, Soumya and Papadigenopoulos, Orestis and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {},
  year = {2021},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/2003.03426.pdf}
}
</pre>
</div>

<div id="abasu2020contextual" style="display: none;">
<pre>We study a novel variant of the multi-armed bandit problem, where at each time step, the player observes an independently sampled context that determines the arms’ mean rewards. However, playing an arm blocks it (across all contexts) for a fixed number of future time steps. The above contextual setting captures important scenarios such as recommendation systems or ad placement with diverse users.
This problem has been recently studied \citepDSSX18 in the full-information setting (i.e., assuming knowledge of the mean context-dependent arm rewards), where competitive ratio bounds have been derived.
We focus on the bandit setting, where these means are initially unknown; we propose a UCB-based variant of the full-information algorithm that guarantees a \mathcalO(\log T)-regret w.r.t. an α-optimal strategy in T time steps, matching the Ω(\log(T)) regret lower bound in this setting. Due to the time correlations caused by blocking, existing techniques for upper bounding regret fail. For proving our regret bounds, we introduce the novel concepts of delayed exploitation and opportunistic sub-sampling and combine them with ideas from combinatorial bandits and non-stationary Markov chains coupling.</pre>
</div>


<script>
function toggleAbstractCCbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexbasu2020contextual(parameter) {
    var x= document.getElementById('Bbasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractbasu2020contextual(parameter) {
    var x= document.getElementById('abasu2020contextual');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="papadig2021recurrent">Papadigenopoulos, Orestis, and Constantine Caramanis. “Recurrent Submodular Welfare and Matroid Blocking Bandits.” <i>ArXiv Preprint ArXiv:2102.00321</i>, 2021.</span></div>
<button class="button0" onclick="toggleBibtexpapadig2021recurrent()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractpapadig2021recurrent()">Abstract</button>



    <a href="https://arxiv.org/pdf/2102.00321.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bpapadig2021recurrent" style="display: none;">
<pre>@article{papadig2021recurrent,
  title = {Recurrent Submodular Welfare and Matroid Blocking Bandits},
  author = {Papadigenopoulos, Orestis and Caramanis, Constantine},
  journal = {arXiv preprint arXiv:2102.00321},
  year = {2021},
  group = {misc},
  arxiv = {https://arxiv.org/pdf/2102.00321.pdf}
}
</pre>
</div>

<div id="apapadig2021recurrent" style="display: none;">
<pre>A recent line of research focuses on the study of the stochastic multi-armed bandits problem (MAB), in the case where temporal correlations of specific structure are imposed between the player’s actions and the reward distributions of the arms (Kleinberg and Immorlica [FOCS18], Basu et al. [NIPS19]). As opposed to the standard MAB setting, where the optimal solution in hindsight can be trivially characterized, these correlations lead to (sub-)optimal solutions that exhibit interesting dynamical patterns – a phenomenon that yields new challenges both from an algorithmic as well as a learning perspective. In this work, we extend the above direction to a combinatorial bandit setting and study a variant of stochastic MAB, where arms are subject to matroid constraints and each arm becomes unavailable (blocked) for a fixed number of rounds after each play. A natural common generalization of the state-of-the-art for blocking bandits, and that for matroid bandits, yields a (1−1/e)-approximation for partition matroids, yet it only guarantees a 1/2-approximation for general matroids. In this paper we develop new algorithmic ideas that allow us to obtain a polynomial-time (1−1/e)-approximation algorithm (asymptotically and in expectation) for any matroid, and thus allow us to control the (1−1/e)-approximate regret. A key ingredient is the technique of correlated (interleaved) scheduling. Along the way, we discover an interesting connection to a variant of Submodular Welfare Maximization, for which we provide (asymptotically) matching upper and lower approximability bounds.</pre>
</div>


<script>
function toggleAbstractCCpapadig2021recurrent(parameter) {
    var x= document.getElementById('apapadig2021recurrent');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexpapadig2021recurrent(parameter) {
    var x= document.getElementById('Bpapadig2021recurrent');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractpapadig2021recurrent(parameter) {
    var x= document.getElementById('apapadig2021recurrent');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="kocaoglu2020applications">Kocaoglu, Murat, Sanjay Shakkottai, Alexandros G Dimakis, Constantine Caramanis, and Sriram Vishwanath. “Applications of Common Entropy for Causal Inference.” <i>Advances in Neural Information Processing Systems (NeurIPS)</i> 33 (2020).</span></div>
<button class="button0" onclick="toggleBibtexkocaoglu2020applications()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractkocaoglu2020applications()">Abstract</button>



    <a href="https://arxiv.org/pdf/1807.10399.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bkocaoglu2020applications" style="display: none;">
<pre>@article{kocaoglu2020applications,
  title = {Applications of Common Entropy for Causal Inference},
  author = {Kocaoglu, Murat and Shakkottai, Sanjay and Dimakis, Alexandros G and Caramanis, Constantine and Vishwanath, Sriram},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume = {33},
  year = {2020},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1807.10399.pdf}
}
</pre>
</div>

<div id="akocaoglu2020applications" style="display: none;">
<pre>We study the problem of discovering the simplest latent variable that can make two observed discrete variables conditionally independent. The minimum entropy required for such a latent is known as common entropy in information theory. We extend this notion to Renyi common entropy by minimizing the Renyi entropy of the latent variable. To efficiently compute common entropy, we propose an iterative algorithm that can be used to discover the trade-off between the entropy of the latent variable and the conditional mutual information of the observed variables. We show two applications of common entropy in causal inference: First, under the assumption that there are no low-entropy mediators, it can be used to distinguish causation from spurious correlation among almost all joint distributions on simple causal graphs with two observed variables. Second, common entropy can be used to improve constraint-based methods such as PC or FCI algorithms in the small-sample regime, where these methods are known to struggle. We propose a modification to these constraint-based methods to assess if a separating set found by these algorithms is valid using common entropy. We finally evaluate our algorithms on synthetic and real data to establish their performance.</pre>
</div>


<script>
function toggleAbstractCCkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexkocaoglu2020applications(parameter) {
    var x= document.getElementById('Bkocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractkocaoglu2020applications(parameter) {
    var x= document.getElementById('akocaoglu2020applications');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="zhuo2020communication">Zhuo, Jiacheng, Qi Lei, Alex Dimakis, and Constantine Caramanis. “Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-Norm Balls.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 1464–74. PMLR, 2020.</span></div>
<button class="button0" onclick="toggleBibtexzhuo2020communication()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractzhuo2020communication()">Abstract</button>



    <a href="https://arxiv.org/pdf/1910.07703.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bzhuo2020communication" style="display: none;">
<pre>@inproceedings{zhuo2020communication,
  title = {Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls},
  author = {Zhuo, Jiacheng and Lei, Qi and Dimakis, Alex and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {1464--1474},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1910.07703.pdf}
}
</pre>
</div>

<div id="azhuo2020communication" style="display: none;">
<pre>Large-scale machine learning training suffers from two prior challenges, specifically for nuclear-norm constrained problems with distributed systems: the synchronization slowdown due to the straggling workers, and high communication costs. In this work, we propose an asynchronous Stochastic Frank Wolfe (SFW-asyn) method, which, for the first time, solves the two problems simultaneously, while successfully maintaining the same convergence rate as the vanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon EC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number of machines compared to the vanilla SFW.</pre>
</div>


<script>
function toggleAbstractCCzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexzhuo2020communication(parameter) {
    var x= document.getElementById('Bzhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractzhuo2020communication(parameter) {
    var x= document.getElementById('azhuo2020communication');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><div class="text-justify"><span id="liu2020high">Liu, Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. “High Dimensional Robust Sparse Regression.” In <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 411–21. PMLR, 2020.</span></div>
<button class="button0" onclick="toggleBibtexliu2020high()">Bibtex</button>
 

<button class="button0" onclick="toggleAbstractliu2020high()">Abstract</button>



    <a href="https://arxiv.org/pdf/1805.11643.pdf"><input class="button4" type="button" value="ArXiv" /></a>

 


<div id="Bliu2020high" style="display: none;">
<pre>@inproceedings{liu2020high,
  title = {High dimensional robust sparse regression},
  author = {Liu, Liu and Shen, Yanyao and Li, Tianyang and Caramanis, Constantine},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {411--421},
  year = {2020},
  organization = {PMLR},
  group = {proceedings},
  arxiv = {https://arxiv.org/pdf/1805.11643.pdf}
}
</pre>
</div>

<div id="aliu2020high" style="display: none;">
<pre>We provide a novel – and to the best of our knowledge, the first – algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity, in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators: when the covariance matrix in sparse regression is identity, our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm which consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance. Also, it is orderwise more efficient computationally than the ellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.</pre>
</div>


<script>
function toggleAbstractCCliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleBibtexliu2020high(parameter) {
    var x= document.getElementById('Bliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<script>
function toggleAbstractliu2020high(parameter) {
    var x= document.getElementById('aliu2020high');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>


</div>

    </div>
    
  </body>
</html>
