<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Teaching &middot; Constantine Caramanis
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h2>
        Constantine Caramanis
      </h2>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">


      

      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item active" href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item" href="/publications/">Publications</a>
      <a class="sidebar-nav-item" href="/researchgroup/">Research Group</a>
      <a class="sidebar-nav-item" href="/researchprojects/">Research Projects</a>
      <a class="sidebar-nav-item" href="https://scholar.google.com/citations?user=47YTUrEAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a>
      <a class="sidebar-nav-item" href="https://www.youtube.com/@constantine.caramanis" target="_blank">YouTube</a>
      <a class="sidebar-nav-item" href="https://ml.utexas.edu/ifml" target="_blank">IFML</a>
      <a class="sidebar-nav-item" href="https://wncg.org" target="_blank">WNCG</a>
      <a class="sidebar-nav-item" href="https://archimedesai.gr/en/" target="_blank">Archimedes AI</a>




      <!--<span class="sidebar-nav-item">Jekyll Hyde, Currently v2.1.0</span>-->
    </nav>

    <p>&copy; 2025. All rights reserved.</p> 
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Teaching</h1>
  <h2 id="i-teach-classes-in-ece-computer-science-and-mccombs-school-of-business">I teach classes in ECE, Computer Science, and McCombs School of Business.</h2>

<h2 id="undergraduate-teaching-in-ece">Undergraduate Teaching in ECE</h2>

<h3 id="data-science-lab">Data Science Lab:</h3>

<p>Most recently taught</p>
<ul>
  <li>Spring 2025</li>
  <li>Spring 2024</li>
</ul>

<p><strong>Course Description</strong>: The emerging field of data analytics and, more broadly data science, is transforming engineering, healthcare, scientific discovery and many industries ranging from Agriculture to Telecommunications. In this class we discuss how to use data to build models to perform prediction and inference. Topics: Predictive modeling. Regression and Classification. Data cleaning and preprocessing. Feature engineering. Unsupervised methods. Principal Component Analysis. Data clustering. Model selection and feature selection. Entropy and Information theory. We spend quite some time on Neural Networks and Deep Learning. Time permitting, we also cover Machine learning for signals and time-series data. 
<br /><br />
The main feature of this class, as the name suggests, is a hands-on approach. We spend significant time working with real and large scale data sets, playing on Kaggle, etc. Though there are labs (homeworks), a midterm and a Kaggle competition, the main deliverable for this class is a group final project. 
<br /><br />
Data Science Lab vs Data Science Principles: Necessarily, this class has significant overlap with the Data Science Principles course, as we have worked hard to ensure that they can be taken in any order. Both classes cover things like regression, classification, decision trees, bagging and boosting. This class will focus more on hands-on issues and challenges, whereas the principles course is more focused on analysis and derivations. Also, this course will have a significantly more pronounced focus on Neural Nets. 
<br /><br />
Some of the amazing final projects students have done in the past can be found <a href="/finalproject/">here</a>.</p>

<p>For undergraduates interested in ML and possibly ML research, I encourage you to check out the Machine Learning and Data Science Club (MLDS) at UT.</p>

<hr />

<h2 id="graduate-teaching-in-ece">Graduate Teaching in ECE</h2>

<h3 id="optimization-i">Optimization I</h3>

<p>Most recently taught</p>
<ul>
  <li>Fall 2024</li>
</ul>

<h3 id="optimization-ii-algorithms-for-large-scale-convex-optimization">Optimization II: Algorithms for Large Scale Convex Optimization</h3>

<p>Most recently taught</p>
<ul>
  <li>Spring 2020</li>
  <li>Spring 2019</li>
</ul>

<p><strong>Course Description</strong>: This is intended to be a second course in optimization, pitched to advanced graduate students who have already taken a first course (covering topics like duality, formulations, etc.) and are interested in using optimization in their research. The course focuses on the details of algorithms and their analysis, for many different problems in convex optimization. The main topics covered are as follows. Note that lecture duration/emphasis is not evenly divided for each of these topics:
<br /><br /></p>
<ol>
  <li>Convex Sets, functions, basic definitions. Optimality conditions for constrained possibly non-differentiable convex problems.</li>
  <li>Gradient and Subgradient descent. Convergence rates for convex functions, for convex and smooth functions, for convex, smooth and strongly convex functions.</li>
  <li>Oracle Lower Bounds and Accelerated Methods</li>
  <li>Proximal Gradient. ISTA and FISTA.</li>
  <li>Mirror Descent</li>
  <li>Frank Wolfe and Conditional Gradient</li>
  <li>Stochastic methods. SVRG.</li>
  <li>Newton and Quasi-Newton Methods</li>
  <li>Interior Point Methods</li>
  <li>Legendre-Fenchel Duality</li>
  <li>Dual Decomposition Algorithms: Proximal Point Algorithm, Prox Grad in the Dual, Augmented Lagrangian Method</li>
  <li>Monotone Operators, Contractive Operators, Non-Expansive and Firmly Non-Expansive Operators.</li>
  <li>Operator Splitting, Douglas-Rachford and ADMM</li>
</ol>

<p>You can find a full record of this class <a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc" target="_blank"> at this YouTube link.</a></p>

<h3 id="combinatorial-optimization">Combinatorial Optimization</h3>

<p>Most recently taught</p>
<ul>
  <li>Fall 2020</li>
</ul>

<p><strong>Course Description</strong>: This course is intended to be an advanced graduate course for students that have significant mathematical maturity and also interest in optimization. Though we go through LP duality from the beginning, prior exposure to LP and Convex Optimization would be greatly beneficial. 
<br /><br />
The focus is on some classical problems in polyhedral combinatorial optimization, as well as basic results in linear programming, such as the basic geometry of polytopes, LP duality, the primal dual framework, and the ellipsoid algorithm. After that, we focus on Matching, Matroids and Submodular optimization, with a variety of other problems and results thrown in along the way. We conclude with a series of lectures on extension complexity and lower bounds coming from communication complexity.
<br /><br />
The course is intended to be self-contained, and there is no required textbook. The material for the course is drawn from many sources. But primary among these are the following four textbooks: 
<br /><br /></p>
<ol>
  <li>Combinatorial Optimization, by Papadimitriou and Steiglitz,</li>
  <li>Geometric Algorithms and Combinatorial Optimization, by Grotschel, Lovasz and Schrijver,</li>
  <li>Theory of Linear and Integer Programming, by Schrijver,</li>
  <li>Combinatorial Optimization, by Schrijver.</li>
</ol>

<p>You can find a full record of this class <a href="https://www.youtube.com/playlist?list=PLXsmhnDvpjORcTRFMVF3aUgyYlHsxfhNL" target="_blank"> at this YouTube link.</a></p>

<hr />

<h2 id="graduate-teaching-in-cs">Graduate Teaching in CS</h2>

<p>I teach two courses in the online MS program in CS and AI. These are aligned with the courses Optimization I and Optimization II described above.</p>

<hr />

<h2 id="teaching-in-mccombs-business-data-science-and-introduction-to-deep-learning">Teaching in McCombs: Business Data Science and Introduction to Deep Learning</h2>

<p>I teach two courses as part of the Masterâ€™s in IT and Management program (MSITM) at McCombs Business School.</p>

<ul>
  <li>Business data science</li>
</ul>

<p>This class teaches fundamental concepts in machine learning and data science, with a few towards applications in business, blending intuition, applicability and business impact, and technical rigor.</p>

<ul>
  <li>Introduction to Deep Learning</li>
</ul>

<p>This class builds on the first semester. We focus on the modern tools and applications of machine learning, developing a working knowledge of neural networks using PyTorch, including convolutional neural networks and transformers. We also spend considerable time discussing foundation models, their use, and their potential impact.</p>


</div>

    </div>
    
  </body>
</html>
